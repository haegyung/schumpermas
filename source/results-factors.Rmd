---
title: "Results: Baseline"
author: "Maximilian Held"
output: pdf_document
library: held_library.bib
---

# Data

## Administration

<!--```{r administration, child='keyneson/keyneson.wiki/q-administration.md'}```-->
<!-- TODO MH: paste condition of instruction? -->
<!-- TODO MH include data-gathering here, too -->

<!-- COMMENT MH
consider jonathan haidt, joshua greene on deep pragmatism and slow/fast morality
-->


## Data Gathering

The first CiviCon Citizen Conference on taxation produced a wealth of data, including:

- 2x 18x Q-Sorts (before, after)
- 2x 18x Item Feedback (before, after)
- 17x Socio-Economic & Demographic data
- 15x General written feedback
- 315x Notes, Posters, Illustrations
- 1601x Photos
- 100hrs Audio
- 80hrs Video (1080p)

<!-- TODO MCH: make this into a table with availability and status of processing -->


## Data Import

All citizens as well as the researcher and the two moderators completed two Q sorts each, one at the beginning and one at the end of the conference.


## Software for Open Science and Reproducible Research

Because scientific computing can do a great deal to harm or promote reproducible research and open access, some words on the choice of software will be in order.
<!-- TODO MCH: find some reproducible quote? what is it -->

Good software for the following q methodological analyses should fulfill the following criteria:

- *Reproducibility.*
  It should easy document and reproduce all the steps undertaken from raw data to the final factor interpretation.
  This will be especially important during the factor extraction and (indeterminate!) rotation phases, where consequential and sometimes controversial methodological decisions must be made.
- *Open Access.*
  Funded mostly by tax money, all of this scientific work should be conveniently available to the public.
  Research on deliberation and citizen participation, especially, should be *Open* Science.
- *Specialized for Q Methodology.*
  At its heart, Q methodology involves factor analyses and similar data reduction techniques, procedures that are widely used and available in all general-purpose statistics programs.
  However, Q methodology also requires some specialized operations, not easily accomplished in general-purpose programs.
  The transposed correlation matrix, flagging participants and compositing weighted factor scores in particular, are hard or counter-intuitive to do in mainstream software.
- *Programmatic Extensibility.*
  Counter to many Q studies, this research features several conditions (before, after), groups of participants (citizens, moderators, researcher), types of items (values, beliefs, preferences) as well as an extended research question, all of which will need to be analyzed systematically.
  Running and documenting all these variations will be next to impossible without programmatic extensibility of the software used.


The following free / open source and closed / proprietary programs are available:

 | General-Purpose | Specialized
 -|----|----
 Closed Source | [SPSS](http://www-01.ibm.com/software/analytics/spss/), [STATA](http://www.stata.com) | [PCQ](http://www.pcqsoft.com)
 Open Source | [R](http://www.r-project.org) | R: [qmethod](https://github.com/aiorazabala/qmethod) package, [PQMethod](http://schmolck.userweb.mwn.de/qmethod/)

The closed-source, general-purpose programs offer some programmatic extensibility, but are poorly suited for Q methodology.
These commercial, and often expensive offerings also make it hard reproduce and open up research.
The special-purpose `PQMethod` was originally written for mainframe in 1992 by John Atkinson at Kent State University, and has since been ported for PCs and maintained by Peter Schmolck at the University of the Armed Forces in Munich, Germany.
<!-- TODO MCH: add citation for PQMethod -->
While nominally free and open-source, `PQMethod` consists largely of legacy FORTRAN code, which few people can read or write today.
`PQMethod` also offers little programmatic extensibility, because it is a standalone program running within layers of emulators.

The [`qmethod`](https://github.com/aiorazabala/qmethod) package [@Zabala-2014], implementing Q methodology in the free and open source [R programming language and software environment for statistical computing](http://www.r-project.org) [@R-2014] is the best fit for the above criteria.
R is freely available for all platforms, and supported by vibrant community of developers.
`Qmethod`, while released only recently, has been thoroughly validated [@Zabala-2014-a], leverages existing packages for extraction and rotation and conveniently wraps specialized Q functions.
Running inside the R command prompt, `qmethod` can be easily extended, now including several functions developed for this dissertation and contributed to the open source project.
<!-- TODO MCH add citation for my contribs -->
Most importantly, using `R`, every step from raw data to final factor interpretation can be traced back, using publicly available and vetted code, down to base functions.
<!-- TODO MCH find nice open source quote -->


### Literate Programming

Machine-readable code, its results, and human-readable explanation of *what that code does* are often written and stored separately.
Not only does such separation invite mistakes when code, result and explanation diverge, but it also makes research harder to reproduce and is conceptually flawed.
At least in the context of statistical analysis, what we want a machine to do, why we want it, and to what effect are *one* intellectual operation, and should be presented as such.
Programs must not be considered black boxes that "do" things of their own accord, to be explained ex-post, but code should be a near equivalent of the same thought expressed in prose.

Donald Knuth has formulated this central tenet of his *Literate Programming* approach thus:

> Let us change our traditional attitude to the construction of programs:
> Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do.
> --- @Knuth-1984

I try to follow this approach by interweaving prose and code in one document, using the [`knitr`](http://yihui.name/knitr/) R package [@knitr].
Aside from published R packages, all code is run *inside* this document upon rendering.
Code will usually not be reproduced in print, but can always be inspected in the [source of this document](https://github.com/maxheld83/schumpermas), "*underneath*" the respective operation or result.

<!-- TODO MH: see timetable of what was done when at the conference -->


## Import

```{r import-q-sorts, echo = FALSE, warning = FALSE}
q_sorts <- import.q.sorts(
  q.sorts.dir = "keyneson/qsorts/",
  q.set = q_set,
  q.distribution = q_distribution,
  conditions = c("before","after"),
  manual.lookup = as.matrix(
    read.csv(
      "keyneson/keyneson-sample/keyneson-concourse/ids.csv",
      row.names=2
    )
  )
)
```

## Participant Feedback

```{r import-q-feedback, echo = FALSE}
q_feedback <- import.q.feedback(
  q.feedback.dir = "keyneson/feedback/",
  q.sorts = q_sorts,
  q.set = q_set,
  manual.lookup = as.matrix(
    read.csv(
      "keyneson/keyneson-sample/keyneson-concourse/ids.csv",
      row.names=2
    )
  )
)
```


## Missing Data

```{r dropped-participants, echo = FALSE}
q_sorts <- q_sorts[ , !colnames(q_sorts) == "Wolfgang", ]  # delete researcher
q_sorts <- q_sorts[ , !colnames(q_sorts) == "Uwe", ]  # left conference for personal reasons
#q_sorts <- q_sorts[ , !colnames(q_sorts) == "Claus", ]  # incomplete
q_feedback <- q_feedback[ , !colnames(q_feedback) == "Wolfgang", ]  # delete researcher
q_feedback <- q_feedback[ , !colnames(q_feedback) == "Uwe", ]  # incomplete
#q_feedback <- q_feedback[ , !colnames(q_feedback) == "Claus", ]  # incomplete
```

<!-- complete import of `r ncol(q_sorts)`, three participants -->

<!-- Notice that some people complained about sorting several items, but that 0 is in fact an appropriate value.
For more on what 0 means to q, see Brown 1980 199. -->


## Codenames

```{r real-names, echo = FALSE}
real_names <- TRUE  # this is for manually enabling renaming
if (real_names == TRUE & file.exists("../../Google Drive/CiviCon/Data/Codenames.csv")) {  # renaming works only if (private) file is available and is set to true
  codenames <- read.csv(file = "../../Google Drive/CiviCon/Data/Codenames.csv",header = TRUE)  # read in codenames
  for (name in colnames(q_sorts)) {  # loop over names in q_sorts
    colnames(q_sorts)[colnames(q_sorts)==name] <- as.character(codenames[codenames[,1]==name,2])  # assign original name
    colnames(q_feedback)[colnames(q_feedback)==name] <- as.character(codenames[codenames[,1]==name,2])  # assign original name
  }
  warning("This rendering includes real names. Do not publish or pass around!")
}
```


# Q Method Analysis

Results chapters in quantitative research do not usually recount and justify every mathematical operation from raw data to final interpretation.
In reporting mainstream statistics, say, a linear regression (OLM) much in the way of axioms and preconditions is often taken for granted, though perhaps, sometimes too much.
Powerful computers, confirmation biases and time pressure for writers and readers alike may conspire to occasionally stretch thin the argumentative link between elementary probability theory and the results drawn from it.
<!-- TODO kill, make easier? -->

Q methodology is different, and requires a more careful, patient treatment.
While its mathematical core --- different methods of exploratory factor analysis (EFA) --- are well-rehearsed in mainstream quantitative social research, its application to Q is still strange to many.

1.  *Transposed Data Matrix.*

    Because Q method *transposes* the conventional data matrix, positing *people* as *variables*, and *items* as *cases*, all of the downstream concepts in data reduction, from covariance to eigenvalues, take on a different, Q-specific meaning, even if the mathematics stay the same.
    <!-- TODO add brown quote for transposed -->
    For this reason alone, it will be worth tracking every operation and grounding it in the unfamiliar epistemology of Q.

2. *Marginalization and Controversy.*

    Almost 80 years after William @Stephenson1935 suggested this *inversion* of factor analysis in his letter to *Nature*, Q is still an exotic methodology.
    It sometimes invites scathing critiques (@Tamas-Kampen-2014), but more often, is outright ignored in mainstream outlets.
    <!-- TODO cite lack in textbooks etc in tamas kampen -->
    As the dynamics of marginalization go, the community of Q researchers, may, on occasion, have turned insular - though not into the "Church of Q" @Tamas-Kampen-2015 fear.
    Q methodology may have sometimes shied away from exposing itself from rigorous criticism and disruptive trends in mainstream social research, though probably often out of sheer frustration with persistent misunderstandings, and because of genuine disagreement.
    <!-- TODO add examples -->
    In epistemological squabbles, too, crazy people [^freakshow] sometimes have real enemies --- or opponents, at any rate.

    Happily, within Q, too, considerable disagreement remains (for example, on the appropriate factor extraction technique), though legacy procedures and programs sometimes hamper intellectual progress.
    Unfortunately, misunderstanding and marginalization is sometimes compounded by a lack of deep statistical understanding, though rarely dissenting into glib ignorance of "technicalities" or outright mistakes ("varimax rotation maximizes variation"), in the otherwise fine textbook indroduction by @Watts2012.
    <!-- TODO find precise quotes -->
    What may easily appear as articles of faith ("thou shalt not use automatic rotation"), are in fact thoroughly argued reservations, based on a deep mathematical and epistemological understanding, as evidenced in both the works of William Stephenson and Stephen Brown.
    These norms become hermetically sealed ideologies ("reliability does not apply to subjectivity") only when detached from their epistemological underpinnings, as, I suspect, would be the case for other methodologies.
    <!-- TODO find source, argue that in fact, as per Brown, factor reliability DOES matter -->

3.  *Experimental Design*
  <!-- TODO fill in! -->

4.  *Literate Statistics.*

    Not just when, or if, a method is new or controversial, as in Q, mathematical operations and argumentative prose should not be let to diverge too widely.
    The intuition of literate programming holds for statistics, too: we do not *use* unintelligible algebra to *give* us a comprehensible result to be explained.
    *Neither* mathematics nor prose are *primary*, algebra and language are *both* the explaining intellect at work, just in a different registrar.

[^freakshow]:
  A prominent researcher recently described the annual meeting of the [International Society for the Scientific Study of Subjectivity](http://qmethod.org/issss) as an endearing "freak show".
  With heirs of the methods founder, William Stephenson, occasionally in attendance, it sure sounds different from your average disinterested get together.

I suggest then, that at least in the following, initial analysis, some verbosity has its virtue.
Readers will be relieved to learn that I intend not to reproduce the entire mathematical apparatus of Q: Steven Brown has accomplished that, and much more in his authoritative, insight-laced "Political Subjectivity" to my own work is deeply indebted.
<!-- TODO make this better, humbler -->
Mathematics have their place, but formulae alone, inspite of their veneer of rigor, as code comments, need analogy, intuition and impression to cover the distance to the deliberative subjectities on taxation and the economy under study here --- which themselves are mathematized in this dissertation, nor much convincingly in the broader field, and maybe never will.
<!-- TODO find better link to previous chapter -->
One may add, that if science is also bound to a discourse ethic of reaching understanding, it, needs to relate its abstractions to the human lifeworld, from which all meaning eminates.
<!-- TODO make this better or kill it -->

The following pages will hopefully dissuade the Q skeptics, and take along newcomers to the method.

At the same time, some of the details of this chapter may raise the ire of some established Q methodologists who never tire of stressing the substantive analysis over statistical sophistication.
They are right: the key to understanding human subjectivity lies in iterative abduction, in the thorough going back and forth between informed hunches about *what might make sense*, and what the data will bear out.

The following statistical groundwork is a worthwhile, but merely *necessary* --- not *sufficient* --- condition to a *scientific* study of subjectivity (emphasis added, though intended by the [ISSSS](International Society for the Scientific Study of Subjectivity)).

To make sense --- as we must --- of the shared viewpoints on taxation and the economy among the participants of the first CiviCon Citizen Conference, we must first be sure what they are, and if they are, in fact, shared.


## (No) Descriptives {#descriptives}

It is common to reproduce descriptive statistics before turning to more advanced analyses.
Common summary statistics often include measures of central tendency (mean, median) and dispersion (standard deviation, range) on *variables* across *cases*.
As mentioned before, in Q methodology, variables are *people*, and cases are *items*.
The mean rank alloted by Q sorters (as the *variables*) across all items (as the *cases*), $\overline{x} = \frac{\sum{x}}{N}$, is then, unsurprisingly, `r unique(apply(q_sorts[,,"before"], 2, mean))` for *all* `r ncol(q_sorts[,,"before"])` participants.
The *average* position of items has to be zero, because the forced distribution was symmetrical: since participants placed an equal number of cards at `-1 / 1`, `-2 / 2` and so on, item ranks will always cancel out.
The most frequent *median* value, also by definition, will be `r unique(apply(q_sorts[,,"before"], 2, median))` for everyone, because the center value allowed the greatest number of (`r max(q_distribution)`) cards per the forced distribution.
By the same token, the *range* extends from `r unique(apply(q_sorts[,,"before"], 2, min))` to `r unique(apply(q_sorts[,,"before"], 2, max))` for everyone, because those extreme values are defined by the forced distribution.
The *standard deviation*, too, is the same for everyone.
$s_N = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_i - \overline{x})^2}$, gives the square root of the average squared differences of item scores for a participant, `r unique(apply(q_sorts[,,"before"], 2, pop.sd))` for everyone, because the "spread" of items across the mean is defined by the forced distribution.

Conventional *R* type descriptives are, then, quite  meaningless [Nahinsky 1965 as cited in @Brown1980, 265], because they are the same for all participants and are defined ex-ante by the forced distribution [^free-distro-descr].
<!-- TODO fix nahinsky quote -->

[^free-distro-descr]: Descriptives are meaningless *only* if the Q distribution is *forced*, that is, the same for everyone.
  If participants are allowed to sort into differently shaped --- but symmetric --- distributions, the standard deviation $s$ may indicate the degree of polarization in viewpoints that different people hold, that is, about how many items they feel extremely.
  If, additionally, participants are allowed to sort into asymmetric distributions, the mean $\overline{x}$ may indicate the overall tendency of people to agree with the sorted statements.
  "Free" distributions are a frequently discussed, if rarely used possibility for Q methodology.
  <!-- TODO MH add source on free/forced -->

```{r descriptives, echo = FALSE, include = FALSE}
apply(q_sorts[,,"before"],1,mean)[order(apply(q_sorts[,,"before"],1,mean))]  # sort by mean
apply(q_sorts[,,"before"],1,pop.sd)[order(apply(q_sorts[,,"before"],1,pop.sd))]  # sort by sd

person_cor <- cor(t(q_sorts[,,"before"]))  # make cor matrix across persons
person_cor[upper.tri(person_cor, diag=TRUE)] <- NA  # kill upper triangle
person_cor_m <- melt(person_cor, na.rm=TRUE)  # melt it
person_cor_m[with(person_cor_m, order(-value)), ]  # order it
person_cor_m[which.max(person_cor_m$value),]  # pos max
person_cor_m[which.min(person_cor_m$value),]  # neg max
person_cor_m[which.min(abs(person_cor_m$value)),]  # abs min
```

In search for descriptives, one may be tempted then to revert back to the R way of looking at data, and to treat Q-sorted *items* as variables, and Q-sorting people as cases.
One may ask, for example, which items were, on average of all participants, rated
the highest (``r rownames(q_sorts)[which.max(apply(q_sorts[,,"before"],1,mean))]`` at `r max(apply(q_sorts[,,"before"],1,mean))`),
the lowest (``r rownames(q_sorts)[which.min(apply(q_sorts[,,"before"],1,mean))]`` at `r min(apply(q_sorts[,,"before"],1,mean))`),
which items were dispersed the most (``r rownames(q_sorts)[which.max(apply(q_sorts[,,"before"],1,pop.sd))]`` at `r max(apply(q_sorts[,,"before"],1,pop.sd))`),
the least (``r rownames(q_sorts)[which.min(apply(q_sorts[,,"before"],1,pop.sd))]`` at `r min(apply(q_sorts[,,"before"],1,pop.sd))`),
or --- most offending to Q methodologists, as well as spurious ---, which *items* were correlated
the most positively (``r person_cor_m[which.max(person_cor_m$value),c(1,2)]`` at `r max(person_cor_m$value)`),
the most negatively (``r person_cor_m[which.min(person_cor_m$value),c(1,2)]`` at `r min(person_cor_m$value)`),
and the least (``r person_cor_m[which.min(abs(person_cor_m$value)),c(1,2)]`` at `r min(abs(person_cor_m$value))`).
This kind of exploration is fascinating, and it invites seemingly inductive hypotheses:
Do people respond most strongly to `poll-tax` and `simple-tax`, because these --- and other, supposedly similar items --- are easy to comprehend and relate to?
Do people respond quite differently to `pro-socialism`, and quite similarly to `land-value-tax-resource`, because the former obviously aligns with political identities, while the latter does not?
Do people feel similarly about `exchange-value` and `natural-market-order` because of a pervasive anti-market bias,
<!-- TODO add caplan reference -->
and very dissimilar about a `wealth-tax` and a `corporate-income-tax`, because they fall for a flypaper-theory of tax incidence?
<!-- TODO add mccaffery reference -->

While these survey-type approaches to the data are intriguing, they are inadequate for the data gathered here.
It may appear arbitrary to categorically rule out a range of broadly-applicable techniques, let alone summary statistics on the grounds that the data were gathered for a different *purpose*.
For example, Likert-scaled data collected for a (R-type) factor analysis may, if conditions apply, be subjected to a regression analysis.
Q, however, is not just another statistical operation, it is a *methodology*, and while the gathered data bear a superficial resemblance to R methodological research, "[t]here never was a single matrix of scores to which *both* R and Q apply" [@Stephenson1935a: 15, emphasis in original].
<!-- TODO find, verify original source. Browns 1980: 347 bibliography is unclear -->
This postulated incomparability applies to this study, too, and plays out in several ways:

 1. *Generalizeability & Small N Design.*

    In R, the *participants* are usually a *sample* of cases from a broader *population*, and the generalizeability of the results hinges on the quality of this ideally large, random selection.
    In Q, *participants* are the *variables*, on which supposedly shared, subjective viewpoints are measured.
    Sampling theory does not apply to Q, just as one would not ask *random* questions in a survey (or so one hopes).
    Instead, Q method requires researchers to broadly maximize the diversity of participants to capture all existing viewpoints.
    <!-- TODO add saturation sampling reference here -->
    Generalizeability in Q, if applicable at all, concerns the representativeness of the sampled *items* of a "population" of statements, though no straightforward sampling theory applies to this concourse either, as it is both infinite and non-discrete.
    The P-set of this study, the participants in the CiviCon Citizen Conference, are self-selected and probably not representative of the broader population, though recruitment was inclusive and diverse.
    This flagrantly non-random sampling alone, however, may not yet rule out generalizeability to a broader population of *citizens willing to participate in deliberation*.
    As I argue elsewhere, both self-selection and diversity of point of views are crucial for meaningful, *concept-valid* deliberation, and such non-random sampling may therefore be *required* for even R-methodological research into the effects of deliberation.
    <!-- TODO add reference to concept-valid deliberation elsewhere -->
    Given the haphazard recruitment and great demands placed on citizens, the group of CiviCon participants must probably be considered excessively non-random even by the more charitable standards of deliberation, but an even greater problem lies in the small number of participants.
    <!-- TODO add footnote about bias of sample -->
    A "sample" of $N=`r ncol(q_sorts[,,"before"])`$ people (including 2 moderating "confederates") simply does not admit of generalizations toward a broader population, even of would-be deliberators.
    R statistics, even the cursory summary statistics in the above, implicitly rely on this kind of generalizeability:
    If we concede that, say, the low average score of ``r rownames(q_sorts)[which.min(apply(q_sorts[,,"before"],1,mean))]`` at `r min(apply(q_sorts[,,"before"],1,mean))` is, to a large extent an artifact of the people who happened to show up at a locally-advertised, five-day sleepover conference for EUR 50 compensation, it becomes dubious why we should care about this factoid at all --- other than to characterize the *bias* of the group ^[Though that comparison, too, would require some representative baseline.].
    <!-- TODO actually THAT may be a good reason -->
    Given limited funds, and little experience the CiviCon Citizen Conference was *designed* for a small group of people, and this constraint was --- admittedly --- operational in the choice of Q methodology.
    <!-- FIXME wrong verb, it's not operational ... -->
    <!-- TODO add footnote on the large-n/long tradeoff -->

    The small number of participants (even by Q standards) restricts the following Q analysis, too, but in a different, provisionally acceptable way.
    <!-- FIXME maybe this belongs partly somewhere else? -->
    Recall that in Q, people serve as *variables*.
    `r ncol(q_sorts[,,"before"])` variables will still be considered quite crude to extract several latent concepts.
    Consider, for example, the data reduction in Inglehart's and Welzel's human development theory: to arrive at *two* latent value concepts, they condense *35* variables [-@InglehartWelzel-2005-aa].
    <!-- FIXME find out correct number of vars -->
    The issue here, however, is one of *resolution*, not generalizeability.
    With relatively few people-variables, Q method will be able to extract only a few, blurred shared viewpoints.
    But the exercise is not moot: given a decent Q-sample of items, potential additional participants are exceedingly unlikely to render the factors extracted here null and void --- that scenario is later rejected as a Null-Hypothesis of sorts.
    <!-- TODO add reference to why that is in fact what later tests test -->
    By contrast, adding more people to this study may very well render the high average position of, say,  ``r rownames(q_sorts)[which.max(apply(q_sorts[,,"before"],1,mean))]`` at `r max(apply(q_sorts[,,"before"],1,mean))` a product of randomness or bias (a high standard deviation of `r pop.sd(q_sorts["simple-tax",,"before"])` would appear to bear this out).

    The now familiar analogy of LEGO bricks serves to illustrate this difference in concepts of generalizeability once more.
    Tasking, say, 15 people to build something out of a given set of LEGO bricks may seem reasonable to get a preliminary idea of the *kinds* of objects (cars, houses) that people build --- though we will probably miss out on some rarer constellations (spaceships?) and nuances of existing patterns (convertibles?).
    Adding more people may increase the resolution to tease out these details, but it's unlikely that something as basic as "car-like", or "house-like" will completely disappear.
    By contrast, again, it seems much *less* reasonable to take the ratings of *individual* LEGO bricks by these 15 people as anything but random flukes: there are bound to be some people who like red bricks, more so if the study listing displayed red bricks.

  2. *Holism & Non-Independence.*
  3. *Validity & Research Ethics.*




The problem, alas, is that these are R, not Q mfuethodological questions, common in survey research, but not suited to the data gathered here.

> In moving from R to Q, a fundamental transformation takes place: In R, one is normally dealing with objectively scorable traits which take meaning from the postulation of individual differences between persons, e.g. that individual $a$ has more of trait $A$ than does individual $b$; in Q, one is dealing fundamentally with the individual's subjectivity which takes meaning in terms of the proposition that person $a$ values trait $A$ more than $B$.




## Correlation

To extract shared viewpoints, we must first look at pairwise comparisons of every participants Q-sort against everyone else's Q-sort.
Since we would expect people with similar viewpoints to sort items in similar positions, we can use a correlation coefficient $r$ between any two people's sorts as a statistical summary of their similarity.

(@pearsons_r) $$ $$



Q studies do not always display or discuss this *correlation matrix*, but all of the downstream data reduction operations are, in fact, based on this table [@Brown1980, 207].

Correlations are

`Qmethod` --- as other Q software, does not report the correlation matrix, but the below correlation matrix is produced calling the same function also used in later analyses, a simple Pearson correlation coefficient.

```{r cor-viz-before}
# make this a list of before/after
library(corrplot)
corrplot(corr = cor(q_sorts[,,"before"])
  , method = "shade"  # make squares
  , type = "full"  # take only upper triangular
  , is.corr = TRUE
  , addCoef.col = "Black"
  , diag = TRUE
  , addCoefasPercent = TRUE
  , tl.col = "Black"
  , tl.pos = "tl"
  , tl.offset = 0.5
  , order = "alphabet"
)

```


## Factor Extraction

```{r analysis}
keyneson <- list("before"=c(), "after"=c())
keyneson$before <- qmethod(
  dataset = q_sorts[,,"before"],
  nfactors = 3,
  rotation = "varimax",
  forced = TRUE
)
# keyneson$after <- qmethod(
#   dataset = q_sorts[,,"after"],
#   nfactors = 3,
#   rotation = "varimax",
#   forced = TRUE
# )
```



### Which Data Reduction Technique

<!-- Also note somewhere that the talk of vectors distending the items during the sort is not completely arbitrary --- it is exactly this kind of mathematical operation that returns in the matrix algebra of factor analysis. -->

<!--
factor extraction
just use factanal normal factor
or normal princcop as opposed to psych
http://www.statmethods.net/advstats/factor.html
http://cran.r-project.org/web/packages/nFactors/index.html
http://cran.r-project.org/web/packages/FactoMineR/index.html
psych also appears to do parallel analysis by fa.parallel
-->

<!-- Even Brown admits that CFA was originally developed because it was a computationally simpler, though similar in results to PAF. -->

<!-- Why would we get rid of negative cors? What do the columns mean? Brown 210 odd procedure -->

<!-- funny pun from Brown: factor analysis is a complicated tautology 223 -->

<!-- check kline 1994 and Harman 1976 in the above on PCA vs CF
Harman 1976 says they will mostly be the same! -->

- *Exploratory* Factor Analysis (Centroid) @Brown1980's favorite
- Principal Components Analysis
- ...?

<!-- COMMENT MH
read kline 1994 on wattss/stenner on PCA vs CFA, and they will be similar as per harman1976
-->


#### PCA

<!-- Steps in a PCA
- correlation matrix (explain detail, just the covariance, somehow standardized to the variance)
- extract principal components by matrix multiplication (dot product) -->

<!-- PCA is scale-variant, but that is not a problem for the corr matrix, nor for the original matrix -->


#### PAF

<!-- COMMENT MH
agree that in fact, as Kampen and Tamas point out, the representativeness of the q sample to the universe of statements is a weak, weak spot.
Important: as cuppen in  kampen and tamas writes 3112 it's not the number of supporters that count, just the perspectives.
Consider the discussion of q methodology and validities oin kampen tamas 3112
do cluster analysis instead of factor; according to kampen and tamas. This might be interesting.
-->


### Number of Factors {#nfactors}

```{r how many factors?}
# library(paran)
# hornvectors <- paran(t(q_sorts[,,"after"]),
#       iterations = 1000,x
#       centile = 95,
#       quietly = FALSE,
#       status = TRUE,
#       graph = TRUE,
#       color = TRUE)
```

<!-- Use the lego example to explain the logic of the combinatorics - this can actually be good news, even with the small number of people -->

<!-- Also notice this in the number of factors discussion: on the one hand, loom non-factor factors, spurious things that I should avoid.
On the other hand looms the danger of ignoring factors, false negatives, that area really out there and without whom the following results may turn out to be quite trivial, and not faithful to the people who volunteered these sorts. -->

<!-- notice: there may be stuff in the data that suggests how many factors make sense (loc 2287)
notice: I am definetely doing exploratory factor analysis; no ex-ante reason to expect one or the other result -->

<!-- for why validity makes no sense, see Brown 1980:4, that may (or may not) be so, but what definietely needs to be tested is whether, in fact, these tautologically valid viewpoints are also *patterned*, wheter some shared can be extracted.-->
In this (above context) Q methodologists often advise that, occasionally, researchers would know what to look for,
<!-- TODO MCH: find reference, find source from Watts Stenner on this -->
and that therefore some substantive discretion would be warranted.
In the extreme case, any number of extracted factor would be acceptable, so as long as it *made sense*.
The problem here, oddly not obvious to many Q researchers, is that this kind of "makes-sense"-discretion comes perilously close to the imposition of deductively hypothesized meaning so criticized in survey research. (see brown 3 or so)

This comes close to massaging data.
There is something falsifiable here; are there, or are there not common factors.

- Experience / Common sense: 6--8 persons per ex-ante factor (Stenner \& Watts 2012: loc 2652):
    `r ncol(q_sorts)/8` factors
- Magic Number 7 (Brown 1980: 223):
    `r ncol(q_sorts)/7` people per factor?!
- Eigenvalue of nth factor > 1 (Kaiser 1960, Guttman 1954)
- Two or more loadings (Brown 1980: 222)
- Humphrey's rule: Cross-Product of 2 highest loadings greater than twice the standard error
- Scree plot (2nd derivative < 0)
- Parallel Analysis (Horn 1965)

<!--
stephenson on number of factors, via verena
OS-9-3-Stephenson.pdf S. 89 "Second, a little simple factor analysis is
all that the operations demand: It will be the end
% of work in this domain if anyone thinks that its be- all and end-all is factor analysis. The less of it, the better. Three or four factors are all that most well planned studies require; there's something loose in the works if anything like ten or so factors are carved out for interpretation. The key to sound work, i.e., to make discoveries, is what one puts into Q method as abduction, not what factor analysis turns out deductively.
-->

<!-- look at communality -->

<!-- \cite[6]{Exel2005} recommends 4-5 people per viewpoint, which given 17 participants yields 3-4 factors, or viewpoints. -->

<!-- \cite{Wittenborn} says explicitly what the problem is: 132, namely that there is no test as to whether there are other people who would sort like this. -->

Brown's case (42) against statistical (as opposed to, ominously, "substantive") criteria for the number of cases seems to rest on particular people's viewpoints being important, which is not the case in my study, nor probably will it ever be in this kind of work.
This *does* make sense because you don't want to rest on numbers game along, namely the amount of people who load on a factor, that's true --- but this is still a necessary, if not a sufficient condition.
Or something like that?

Brown (43) seems to be aware of the problem of false positives from Eigenvalues, but doesn't seem interested in parallel analysis.

He also argues (and more convincingly!) that (43) factor size depends on the people involved, and since the sampling of these people does not follow any kind of method, the result would be meaningless.
I think this is an overstatement; it merely implies false negatives?!? (or something)
Think this through!
Also, point out that absent any kind of such a criterion, we're left in a very uncomfortabel spot
also, in my case, arguably sampling *is* ok, because deliberative participants will always be self-selected (as argued elsewhere).
<!-- TODO MCH: notice in the recruitment part that a) too much money may drive out other incentives, and b) already, the VHS-kind-of-orientation-what-do-i-learn was a bit of a problem, might need greater stakes, better output format, more responsibility. This is all under the heading of external validity of the conference -->


### What Rotation?

- Judgmental / Manual
- Automatic (varimax, equamax etc.)
<!-- good piece on why judgemental rotation on brown 224: but it's also an invitation to reify stuff, or viewpoints the researcher has. notice that there's already place for judgment in the *interpretation* -->

<!-- I don't Have people in my sample who really stand out, as Brown 1980 261 hopes there would be. -->

<!-- The problem is of course that judgmental rotation is theory laden; for example, a somewhat naive architectural theory that expects man-made objects to meaningfully exist perpendicular to earh, and what's more, perpendicular to broadly parallel lines.
this may work for run-of-the-mill cards and single-family homes, but other classes of objects, say, a tree, a vibrator might be harder to recognize.
This analogy --- if a little forced --- offers more instruction: cards and houses are easy to recognize in the same coordinate system because they are, in fact, build in much the same way, by an industrial system (compare that to natural objects, or to works of art) --- and they exist under gravity, which enforces some kinds of discipline.
the same of course, *could* be said for viewpoints.
But then, we get into trecherous territory, assuming that kind of dimensionality.
A mathematical criterion may, in fact serve us better.
Or not? -->
<!--
It is also unclear, how theoretical rotation is even supposed to work with more than 2 dimensions! (or is it, do they do that pairwise or what?) -->

<!--
explain the rotation business with lego; it is easier to recognize a house when the axis by which height etc. is described are, say, parallel to the ground --- though they need not. -->


<!-- worry about bipolar factors -->

<!-- COMMENT MH
via qlist Since Mary Furnari decided to rely on varimax rotation, Watts & Stenner (2012), for instance, would actually recommend PCA. (But note that Watts & Stenner erroneously think that varimax maximizes the amount of explained variance. To clarify: Varimax searches a simple structure solution characterized by a maximal number of either high or near zero loadings for every factor which is arrived at by maximizing the variance of the factors' loadings. The amount of explained variance is not affected by rotation.)
In my view, the meaningfulness of certain quantitative coefficients in Q, like so-called 'significance' of factor loadings is often overrated. So as if observing coefficient based rules could provide mathematical-statistical  proof for the soundness of the researcher's decisions and conclusions. Without referring to the wisdom of inferential statistics (which is not applicable in Q, IMO), however, I would dare to bet that given 60% explained variance of the 1st factor and 4% and less for the following, that Mary Furnari won't be able to assemble groups of sorts (to be flagged on different factors) that represent distinct = uncorrelated views. But that's just a bet which I possibly can lose. So nothing is lost by just trying out varimax solutions with 2, 3, 4 ... factors. Two simple (but not 100% unambiguous) rules for accepting a factor solution: (1) At least 2, better 3, defining sorts (load strongly on the respective factor only). (2) Intercorrelations of factor scores at a moderate level (possible choice of critical level: not higher than the size of the loading accepted for a defining sort).
-->

## Loadings

For the analogy that the factors are the loading people's lego buildigns superimposed on another, see Brown 240.


## Factor scores

<!-- Brown 34 explains the ideal type (his choice of words) analogy -->


### Flagging

<!-- COMMENT MH
check whether the two flagging criteria make sense, aren't they too restrictive?
-->


### Scoring


#### QDC

QDCs are not calculated, as one might think, as t-tests, but using Brown 1980 244.


### What visualization

notice that while z-scores have more information, it makes this hard to see because of overplotting.
A neat visualization, in this case, is more than just cosmetic; easy navigation can be considered crucial to arrive at good factor interpretations.

Notice that "consensus statements" is something of a misnomer, because the significance cutoff point is asymmetric.

the items *together* make sense (or not) Brown 1980: 257

<!-- TODO MCH: write to q list about this -->

<!--
notice that I look at sd of pop, not sample, because it's not r stats
but still, dispersion is interesting - those are the loose lego blocks
is there maybe a need to also look at the loose lego blocks *overall*? And what are those? What are the loosest blocks?
-->

```{r array-viz, fig.width = 20, fig.height=18}
arrayviz <- array.viz(
  QmethodRes = keyneson$before
  ,f.names = c("resentment","critical","moderate")
  ,incl.qdc = TRUE
  ,color.scheme = "Set1"
  ,extreme.labels = c("very much disagree","very much agree")
)
arrayviz[3]
```

```{r add-type}
# array.viz.data <- merge(  # add type of item
#   x = array.viz.data
#   ,y = q.sampling.structure  # that is where the types are from
#   ,by.x = 0  # these are rownames
#   ,by.y = "handle"  # that is how they are called
#   ,all = TRUE
# )
# rownames(array.viz.data) <- array.viz.data$Row.names  # restore rownames
# g <- g + geom_text(
#   aes(
#     ,fontface=c("plain","bold","italic")[metaconsensus]
#   )
#   ,size = 3.5
#)
#g <- g + scale_family_manual(c("serif","sans","mono"))
```

<!-- TODO MH: per-participation visualization see new issue -->


## Interpretation

<!--
Frank destroys the SD of pro-socialism
-->

```{r frank discussion pro socialism}
# q.feedback["pro-socialism","Frank","after"]
```


## Discussion

<!-- what we're loosing when then R2 increases is, by definition, some residuals.
interesting idea: maybe look at what kind of residuals was lost? could we tell a story about that? -->

<!-- Notice towards the end: these are not revolutionary insights.
In part, because item development may still be in its infancy, and in large part, because the sample was quite limited in diversity --- missing liberals, in particular.
Still, it is very interesting, and it would be crucial to have larger samples to extract more items. -->
