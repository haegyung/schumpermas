[
["index.html", "Give-and-Take Abstracts English German Colophon", " Give-and-Take What citizens would think about taxation, if they could. Maximilian Held Bremen International Graduate School of Social Sciences (BIGSSS)18 November, 2018 This is Maximilian Held’s dissertation at BIGSSS. This dissertation is advised by: Martin Nonhoff (BIGSSS / IniS, University of Bremen) Olaf Groh-Samberg (BIGSSS, University of Bremen) Claus Offe (Hertie School of Governance, Berlin) The spirit of a people, its cultural level, its social structure, the deeds its polity may prepare — all this and more is written in its fiscal history, stripped of all phrases. He who knows how to listen to its message here discerns the thunder of world history more clearly than anywhere else. – Joseph Schumpeter (1918, 1918: 6, emphasis added) The unforced force of the better argument: (Habermas 1996, 305) The speaker must choose a comprehensible expression so that speaker and hearer can understand one another. (Habermas 1976: 2f) [A]nyone acting communicatively must, in performing any speech act, raise universal validity claims and suppose that they can be vindicated. (Habermas 1979: 2) Reaching understanding, (…) the inherent telos of human speech. (Habermas 1984: 287) Abstracts English Everyone pays taxes, and taxation affords us citizens some democratic control over the mixed economy, yet we often poorly understand its fundamental tradeoffs and alternatives. Reasoning with one another may strengthen our democracy, but little is known how such a deliberative ideal fares on abstract policy or how deliberative quality might be measured. The first CiviCon Citizen Conference tests in a quasi-experiment how deliberating policy as abstract as taxation might work, and how people’s thinking on taxation might change as a result. During the week-long conference, 16 diverse, self-selected citizens were tasked to design a tax system “from scratch”, choosing among possible combinations of base and schedule. They participated in learning phases, deliberated in moderated small group and plenary sessions, met with experts and held a concluding press conference. Before and after the conference, citizens sorted 79 statements on taxation and economics according to their subjective viewpoint. Following Q methodology, sorts were factorized to extract ideal-typical viewpoints shared by participants. Before the conference, citizens expressed resentful, radical and moderate viewpoints, including some apparent inconsistencies between beliefs, values and preferences on taxation. After the conference, citizens shared decommodifying, pragmatic and critical viewpoints and displayed a simpler, lower-dimensional structuration of viewpoints. Results indicate that indeed, deliberation changes people’s thinking on taxation, strengthening their viewpoint consistency and better structuring their subjectivities. Results also indicate that deliberating abstract policy can be meaningful: While citizens considered themselves ill-prepared yet to recommend a tax, they felt more confident in deciding on tax policy and advocated for more and longer citizen participation on such abstract matters. Long-form deliberation may or may not yield rational consensus on taxation or other complex economic policy, but at least, it should help reduce the rough-and-tumble of economic ideologies to fewer, deeper and reasonable disagreements, more amenable to last resort majority decisions. German Jeder zahlt Steuern und Besteuerung ermöglicht Bürger_innen demokratische Kontrolle über die Mischökonomie. Trotzdem bleiben die ökonomischen Abstraktionen oft unverstanden. Der wechselseitige Austausch von Gründen könnte unsere Demokratie stärken, aber bisher ist nicht bekannt wie abstrakte Politik deliberiert werden könnte oder wie die Qualität dieses Prozesses gemessen werden kann. Die erste CiviCon Bürgerkonferenz untersucht in einem Quasi-Experiment wie Bürger_innen auch komplexe Politik selbst in die Hand nehmen können, und wie sich ihre Sichtweisen auf Steuern ändern. 16 Bürger_innen mit unterschiedlichem sozio-ökonomischen Hintergrund hatten während der 6-tägigen Konferenz die Aufgabe, ein Steuersystem “von Grund auf” zu gestalten, basierend auf den möglichen Kombinationen von Bemessungsgrundlage und Tarif. Die Bürger_innen nahmen an Lernphasen teil, berieten sich in moderierten Kleingruppen und Plenarsitzungen, konsultierten Experten und präsentierten auf einer abschließenden Pressekonferenz ihre Ergebnisse. Vor und nach der Konferenz sortierten die Bürger_innen 79 Aussagen über Steuern und Ökonomie nach ihren jeweiligen subjektiven Sichtweisen. Der Q Methodologie folgend wurden aus den Sortierungen mittels explorativer Faktoranalyse idealtypische, unter den Teilnehmer_innen geteilte Sichtweisen, extrahiert. Vor der Konferenz äußerten die Teilnehmer_innen ressentiment-geladene (“resentful”), radikale und gemäßigte Sichtweisen. Außerdem wiesen die Sichtweisen scheinbare Inkonsistenzen zwischen Annahmen (“Beliefs”), Werten (“Values”) und Präferenzen über Steuern auf. Nach der Konferenz teilten die Bürger_innen dekommodifizierende, pragmatische und kritische Sichtweisen und zeigten eine einfachere, niedrig-dimensionalere Strukturierung der Sichtweisen. Die Ergebnisse deuten darauf hin, dass Deliberation tatsächlich Einfluss darauf haben kann wie Bürger_innen über Steuern denken. Zudem scheint sie die Konsistenz geteilter Sichtweisen zu verstärken und die Subjektivitäten besser zu strukturieren. Die Ergebnisse lassen hoffen, dass auch über abstrakte Politik deliberiert werden kann: Zwar erschien den Bürger_innen die Konferenz noch als zu kurz, um eine Steuer empfehlen zu können, sie schätzten sich aber als kompetenter hinsichtlich Steuerpolitik ein und sprachen sich für mehr und intensivere Bürgerbeteiligung – auch zu abstrakten Themen – aus. Intensive Deliberation könnte möglicherweise sogar Einigung über Besteuerung hervorbringen. Sie sollte aber in jedem Fall dabei helfen, widerstreitende ökonomische Ideologien in wenige, tiefere und vernünftige Uneinigkeiten zu entflechten, die – falls unumgänglich – dann durch Mehrheitsvotum entschieden werden können. Colophon Free as in freedom, not free beer. — Richard Stallman, Free Software Foundation This book was written using free and open source software (FOSS). This is a bookdown book, written inside RStudio and Atom. The book is available in several formats at https://maxheld.de/schumpermas. The complete source of the book can be found on github. The website is updated after every commit by travis-ci. The rmarkdown source of this book interweaves code and prose, following Donald Knuth (1984)’s suggestion for Literate Programming. The R code will usually not be reproduced in print, but can always be inspected in the source of this document, underneath the respective operation or result. This research project strives towards the ideals of open and reproducible science. All results can be reproduced from the raw data using only the scripts contained herein. References "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements "],
["preface.html", "Preface", " Preface "],
["introduction.html", "Chapter 1 Introduction 1.1 A Method of Elimination 1.2 Structure of the Dissertation 1.3 A Long Story", " Chapter 1 Introduction One should always look for a possible alternative, and provide against it. It is the first rule of criminal investigation. — Sherlock Holmes in Conan Doyle (1904)’s The Adventure of Black Peter (1904, 567) There is something fishy about our current historical moment, sometimes ominously referred to as “late capitalism” (first as description Sombart 1902; then as prophetic critique Offe 1972; Mandel 1978; today as snarky sarcasm Lowrey, n.d.) or “post(-truth) democracy” (Crouch 2004). On the one hand, in absolute and aggregate terms, the developed world has never had it so good (Easterlin 2000: 10f; DeLong 1998). At least for now, and albeit threatened by a barrage of geopolitical risks and global imbalances, our global market economy continues to produce marvels of material prosperity. 10 years after the 2017ff Great Recession, many members of the Organization for Economic Cooperation and Development (OECD) report relatively healthy real gross domestic product (GDP) growth rates averaging 2.5% and harmonized unemployment rates (HURs) of around 5.3% in 2017. These mainstream economic indicators are highly imperfect,1 but they suggest that far from collapsing under the weight of its ostensible internal contradictions, capitalism is still humming along. On the other hand, the material and institutional foundations of OECD welfare capitalisms appear to be crumbling beneath our feet. Economic growth in the developed world, while still positive, appears to be slowing in the long run, perhaps never to return to the high (~3%) rates of the catch-up during the post-war era (Easterlin 2000: 10f). Income and wealth inequality appear to be rising (for example Grabka et al. (2007, 138) for Germany, Bucks, Kennickell, and Moore (2006) for the US) though the magnitude and proper context are contested. Incontrovertably, in the face of obscene riches, many people are still working harsh, back- or soul-breaking jobs and suffering from relative and absolute poverty (Bundesregierung der Bundesrepublik Deutschland 2006; Wilkinson and Pickett 2010). Our economies also appear to be increasingly unsustainable. In the medium term, macroeconomic imbalances and populist rule threaten global free trade, and renewed asset bubbles (real estate, corporate debt) and fresh systemic risks (index-replicating exchange-traded funds, ETFs) could burst, yet again, into economic crises, merely forestalled by low interest rates, excessive public or private debt or asset rallies (for example, Streeck 2013). In the long run, the real dissavings taken out against our planetary ecosystems, including, but not limited to CO2e-emissions, may come to roost as grave environmental degradation of our livelihoods (Stern 2006). Add to that coming burdens of aging populations in much of the OECD (Demeny 2003: 2; Börsch-Supan 2000) and technology-driven divergences in productivity (originally Baumol and Bowen 1965). At the same time, the institution traditionally tasked with reigning in the economy, the modern welfare state, appears severely constrained in its ability to redistribute market outcomes, to fund public goods (Baumol 1992) and to internalize externalities. Marred by an increasingly regressive tax base, high debt, homegrown inefficiencies and incompetence, as well as, recently, populist and illiberal upswings, the state seems to be supremely outmatched. This disturbs the balance of state command and market economy, and may well fray the postwar social contract of relatively widely shared, stable growth, under largely private ownership of the means of production. A similar conundrum besets the state of self-government. On the one hand, this should be a golden age for democracy. Citizens today are more formally educated than at any previous time, and they have ready access to realms of information that would put to shame the greatest libraries of yesteryear. Formal and informal boundaries to citizen participation have fallen, and the electorate has become more demanding of its representatives and government. New technologies, from cheap and easy web publishing to social media to distributed software systems have made it possible for ever greater numbers of citizens to make their voices heard, to organize themselves and to challenge existing institutions and incumbents. On the other hand, citizens appear to be ill-informed (Carpini and Keeter 1996), irrational (Caplan 2007) and increasingly disaffected with democratic incumbents (since Putnam, Pharr, and Dalton 2000), or perhaps — according to recent research — even with democratic institutions. Populism, illiberalism and outright xenophobia are on the rise in many OECD-countries, challenging both established parties and established democratic norms. Voters appear to become more polarized, at least in some countries, and at some level of government. Political debate, too, seems to fray, with increased tribalism, utter disregard for facts and a decline in civility. Here too, the postwar social contract appears to be crumbling: liberal democracy, with its finely tuned balance of individual rights and collective obligations, guarded by an iron-clad rule of law is now under attack. How, then, are we to square these seemingly antithetical diagnoses of our current historical moment? Has there been some foul play with welfare capitalism and liberal democracy? Or is this just the way the world works, bursting our delusional, progressive bubbles? Critical social scientists, from Marx (1867) to today’s gloomy pronouncers of “post-democracy” (Crouch 2004) and “late capitalism” (Mountford 2011) are always prone to suspect foul play. They highlight ostensible contradictions of current economic and social arrangement, and contrast a deplorably imperfect present reality against preferable arrangements past or future (also see Streeck 2013; Nachtwey 2016). This ambition to mold evolved institutions, and sometimes human behavior itself, to birth a better world may have provided, if sometimes in a roundabout way, the emancipatory impetus for many progressive achievements (Blühdorn 2007) [^politicisation], such as Bismarckian social insurance or universal suffrage but it has also fueled the hubris of social planners (see Spufford 2011), and is associated with the evils of 20th century left authoritarianism (Hayek 1944; but compare Arendt 1973). By contrast, conservative and libertarian observers, from Burke (1790) and Hayek (1960) to modern-day critics of democratic rule (Caplan 2007), liberal multiculturalism (Haidt 2012), rentier capitalism (Zingales 2014) and economic utilitarianism (McCloskey 2006) find nothing outrageous, let alone suspicious about welfare capitalism and liberal democracy in disarray. From this vantage point, an overextended state and overly inclusive or demanding polity (???-1968-aa) is simply crumbling under the weight of its unkept promises and collective action dysfunctions, as was to be expected all along. Many, more moderate critics may be worried what the postwar compact might take with it, should it go down violently (see Acemoglu and Robinson 2012). But fearing collateral damage to the rule of law, property rights or even liberal democracy does not imply that there would be anything wrong with a retrenchment of state intervention. Conservative and libertarian viewpoints easily come across as curmudgeonly or even cynical, but they are ignored and disavowed in the much of the social science at all our peril. This healthy skepticism of man-made societal designs and respect for the wisdom contained in evolved, and therefore stable institutions must not be ignored. In the extreme, however, conservatism and libertarianism risk collapsing entirely that what is, and should be, negating all collective political action in Panglossian tautology (Voltaire 1759). We should interrogate our present situation through both ideological lenses — and any other viewing aids not neatly foldable into these two camps — if only to be aware of our blindspots, and perhaps, even to expand our moral horizons. Still, both approaches are equally limited in their scientific rigor. Critical observers always assume we have been duped out of some better world and jump right to the “whodunnit” question. In the extreme, historical materialism conflates entirely the questions of economic constraints and political causes, because it conveniently assumes the latter to be mere superstructure to the former (Popper 2013; compare Marx 1844). The modern-day heirs of post-structuralism and associated post-isms take a similar tack when they define away, or straight up ignore such at least pragmatically useful concepts such as productivity, marginal use or even rational discourse as mere language permeated by power (see Gibson-Graham 2006, and @Peters2001; broadly Laclau and Mouffe 2014; distantly Foucault 1972). Not only are concrete critiques of the supposedly corrupted intellectual apparatus undergirding, say, “neoliberalism” such as the first theorem of welfare economics (graphically by Lerner 1944; mathematically by Lange 1934; Debreu and Arrow 1954) conspicuously hard to come by amidst all the intramural casuistry, but these nominally political theories are also surprisingly mum on policy for the here and now (see Rorty 1999). Conversely, conservative or libertarian commentators reject the mere possibility of collectively decided improvements, and therefore will never open any investigation into the causes of, or culprits for what might otherwise be recognized as shortcomings. Either way, assessments of our present situations become unfalsifiable. The critical social scientist assumes no (relevant) exogenous constraints on collective designs, thus hermetically suspecting all limitations to be “inside jobs”. To the conservative, evolved institutions are already largely determined by inviolable exogenous limits and therefore leave little way for reform or wrongdoing. The libertarian assumes that collective choice, however constrained, is impossible or undesirable to begin with, and therefore rejects any comparison of coercive institutions. In the rare case that these strands of research engage each other at all, a fruitless shouting match ensues, about the proverbial half-empty, half-full, or un-fillable glasses, respectively. This dead-end can be ameliorated by less ideologically committed, more empirically minded research, but never entirely avoided. Because history does not afford proper experimental designs, positive science typically relies on longitudinal comparisons against some point in the past (e.g. Streeck 2013). These comparisons are complicated by the fact that in the past, all other things were emphatically not equal. The limitations of these designs are best illustrated using quantitative variables, but may extend to qualitative observations as well. For example, consider seemingly widening inequality, frequently operationalized or surveyed as household income inequality (e.g. Piketty and Saez 2014). Troublingly for these accounts, household size in much of the OECD-world has decreased substantially over the past decades, potentially contributing to inequality independently from economic or tax changes. For instance, the otherwise increase in income inequality in Germany from 1991 to 2007 is “strongly related” to changes in household composition (Peichl, Pestel, and Schneider 2012: 118). Of course, it is not clear whether we should take solace in supposedly exogenously smaller, poorer households. Smaller households may well, in part, reflect a preference for greater autonomy over greater household incomes, but they may also be, in part, an effect, not a cause, of broader economic shifts, such as increased job mobility, the decline of relatively well-paid manufacturing jobs, overhangs in marriage markets or any number of other changes in the same time period. Adjusting for changing household sizes is possible, but results appear to be highly sensitive to the some arbitrary choice of equivalence scales (Aaberge and Melby 1998). Similar problems beset even the painstakingly careful and seminal “Capital in the 20th Century” (Piketty and Goldhammer 2014, but compare @McCloskey2014) or seemingly simple matters as to how to appropriately deflate increasing computing power in national accounts (Schreyer 2002). Accounts of citizen disaffection are likewise marred in controversy: while some have long worried about increasing demands overloading government (???-1968-aa), others interpret popular discontent as a good sign of greater emancipation (Inglehart and Welzel 2005). Dig deep enough into the operationalizations, extend the time horizon long enough, accrue enough changes in context, and longitudinal studies turn increasingly on ontological and axiological choices, in the worst case reverting to an ideological Rohrschach tests. These limitations at the margin notwithstanding, careful longitudinal research continues to be valuable, especially because it — in contrast to more ideologically committed, self-referential works — ensures that the social sciences remain a cumulative enterprise of discovery. But, given our lingering disagreement, and the potential gravity of the situation, perhaps it is worthwhile to try another investigative strategy. That is what I offer here. 1.1 A Method of Elimination In this crime story of the welfare state, tax and democracy, I follow Holmes’ advise and attempt to rule out all possible alternatives. The first alternative to be ruled out is — counter-intuitively — that, in Margaret Thatcher’s words, there is no alternative. Before a criminal investigation of a corpse can begin in earnest, detectives have to show that it was not, in fact, a natural death. Put another way, they must demonstrate that the deceased person might have lived on, had not someone or something intervened. –&gt; Why, in spite of all our technological marvels and economic wonders are we still plagued by widening inequality, poverty, instability and depleted commons? Why, with all the information and levers of participation at our fingertips, are we so thoroughly confused about governing our mixed economies, and so perilously disaffected with the results our democratic rule produces? This dissertation is fairly pedestrian. I tread in some of the mundane minutiae of modernity; initially the twists of tax, and later the details of democratic rule. And yet, in that small print of the social contract, I have found a veritable crime story. The story begins with (, p. ): why, in the richest of countries, in the most enlightened of times (current day oecd-world) do we still find ourselves amidst (, p. ): of welfare states, too constrained to elegantly improve upon the equity, efficiency and sustainability of markets, of democracies, too paralyzed to meaningfully rule their economies and consequently, of political equality and economic opportunity? This lament alone does not mean there was crime: maybe, these are no crises, but just facts of life, and a thorough investigation is unnecessary. Likewise, we do not call in murder police when some centenarian does not wake up one morning. However, at least, we task a doctor to establish a cause of death. So it is with these three crises: before any criminal investigation can start, I must establish a causal theory for constrained welfare, paralyzed democracy and rampant inequality. I find that causal theory in the complex interactions of markets and plans, as they coexist in a (, p. ). Specifically, I find that of the political institutions that make up an intact mixed economy, most to welfare, democracy and equality (, p. ). This dissertation is staunchly reformist. Just because a system may be in crisis, it does not require a revolutionary overhaul, nor does it warrant dialectical glee about its supposed inevitability. I stick to the market economy and the welfare state, and merely tinker with tax to make them coexist better, and to mutual advantage. There, I have found a scandal. Our choice of tax can only be scandalous, let alone criminal, if there is, in fact, a better tax. And so, in (p. ) I ask: what makes a (, p. ) and what makes it (, p. )? I evaluate all real and hypothetical taxes on these criteria, and find: (progressive) taxes on the (unimproved) value of land and (postpaid) consumption are much than the ones we have (, p. ). By comparison, our existing taxes on income and (prepaid) consumption appear staggeringly inefficient, inequitable and unsustainable. Because the tax foundations of our mixed economies are thus sabotaged, our democracies must needlessly trade off efficiency, equity, and sustainability. This dissertation is sociological, not just economic. I cannot merely posit a suboptimal choice in tax, but I must theorize and test some social process to account for the supposedly suboptimal choice in tax. is the social process that ought to rule collective choice (, p. ). It, too, needs specification: what it strive for (, p. ), and what it deliver (, p. )? In the balance of these questions I find: liberal, but deliberative democracy may be much than the representative institutions we have (, p. ). By comparison, the status quo of pluralism appears to be a minimal formulation of liberal democracy, as skeptical about what people can do, as it is restrictive about what democracy should do. Once a historical achievement, it today appears hopelessly outmatched by the vast complexity and tightly concentrated special interest of late capitalist society. (p. ) investigates the link between . I start by rounding up for suboptimal taxation (, p. ) and proceed — as Holmes advises — by the method of elimination. First up: democracy itself. Maybe, “suboptimal” tax is, in fact, the popular choice. Only if it is not, is there a failure of the political process to be puzzled over, and accounted for. I suggest that tax choice depends on the kind of democracy. Tax choice may suffer , because it is very complicated and offers tightly concentrated special interest (, p. ). More broadly, the two crimes of suboptimal taxation and limited democracy are intimately related. People might not just widely misunderstand taxation, but they might err systematically about the trade-offs of ruling a mixed economy. These systematic misunderstandings under the dysfunctions of pluralism might interact with the unattractive alternatives of a dysfunctional tax regime to dramatically constrain any popular choice of tax, and divert the polity away from the equity, efficiency and sustainability they might otherwise prefer. Democracy and taxation might be closely intertwined in their supposed mutual crises, but they also constitute two sides of the same coin that is the liberal-democratic, capitalist social contract. Democracy concerns the making of collectively binding decisions, taxation is the chief means to implement these agreed-upon plans within the market exchanges of free agents. Conversely, democracy legitimates tax and calls the trade-offs of the mixed economy, and taxation also shapes the material conditions under which people decide, and collects the resources to power policy. Not surprisingly, democracy and taxation share , and their crises and reform hinge on the same questions of equality, justice, cooperation and human nature (, p. ). This dissertation is also empirical. To stay clear of hermetic ideology, and self-referential critique, I must get up from the normative armchair and show that really, a better democracy and better tax are related and can be had. A good crime story cannot rely on conjecture alone, it needs supporting evidence. I cannot provide an instrument of crime because history does not leave material what-ifs in its wake. But I can try to capture these hypotheticals in an : if ordinary people, under an alternative, democratic process, choose an alternative, preferable tax, that would be as close to a smoking gun (, p. ) as it gets. If, moreover, that experimental democracy can be shown to alleviate some of the dysfunctions of pluralism, and misunderstandings of the mixed economy, and if, consequently, through that democratic fora, people prefer an alternative tax, we know for sure that some crime has happened. If world history is still written in tax, as Schumpeter (1918) believed, us democratic citizens, too, must be fluent in fiscalese, to live up to the emancipatory promise of modernity. If communicative action can heal the disagreement and confusion wrought as modernity differentiated System and Lifeworld, as Habermas (1984) hopes, in our discourse on tax, too, universally acceptable validity claims instead of money and power must rule to uphold our faith in liberal democracy. I here probe into one intersection of legitimate inputs and outputs of the social contract (see Scharpf 1997b): the discourse on, and economics of tax. I ask, how — if at all — people under current democratic arrangements think and speak differently about tax, from how they would think and speak about it, if they lived under conditions of ideal speech. I test, how — if at all — people change their thinking and speech about tax, if they participate in a democratic process that is closer to the Habermasian ideal? I hypothesize that — as a result of more ideal speech — people will prefer different taxes, including a PCT. What if there were alternative taxes, which afforded us more attractive tradeoffs between economic efficiency, equity and sustainability or other ends What if we could agree on such alternative taxes, but would not, because we lacked the necessary information or suitable fora, and because our capacity for mutual reason-giving had been diminished by alienating inequality or clouded by special interest? –&gt; 1.2 Structure of the Dissertation The following chapters follow a fairly conventional structure from theory to methods to results and discussion, but the status of especially the first couple of chapters may be unclear to readers. Out of substantive necessity, the references and disciplines in these first couple of chapters can be quite wide-ranging, though these are merely introductory (for the reader) and remedial (for the participants). To clarify the role of these chapters, let me first situate them within the remainder of this dissertation. Chapter 2 outlines some pragmatic conditions for doable and desirable hypotheticals. These conditions serve to preliminarily justify the economic abstractions of the mixed economy (Chapter 3) and taxation (Chapter 4) and ground the critique of the status quo (Chapter 5). By explicating these assumptions, I also list the irreducibly political choices underlying taxation, which cannot be resolved by experts, but require citizen deliberation (Chapter 6). These cursory ontological and axiological foundations later also inform the learning phases at the CiviCon Citizen Conference (Chapter 7) and feed into the statement concourse to measure participant subjectivity (Chapter 8), as well as its interpretation (Chapter 10). Chapter 3 introduces the mixed economy as the quintessential pragmatic institution by which market and plan can coexist. I describe its ends and means for providing human welfare, and highlight the importance of taxation (Chapter 4) to maintain the social contract. By properly explaining a welfare state as a well-designed mixed economy, the chapter expands the existing welfare state literature and prepares a more damning critique grounded in elementary public finance (Chapter 5). The mixed economy also serves as a broadly acceptable frame for the CiviCon deliberations (Chapter 5), educates the participating citizens (Chapter 9) and justifies the focus on taxation (Chapter 7). Chapter 4 : This project is modeled on a straightforward experimental design, as it is frequently used in clinical studies. However, because the study deals not with easily measurable biomarkers, but with subtleties of deliberation and the abstractions of tax, the design needs a little more conceptual groundwork. Consider an analogous example to let me guide you through the following chapters: A doctoral candidate in medicine wants to study how a daily morning exercise affects mindfulness. In this exercise study, the absence (or presence) of the morning routine is the independent variable (IV), the kind and level of mindfulness the dependent variable (DV). Analogously, in the present study, the participation in the CiviCon citizen conference is the IV, and changes in the subjectivities and preferences concerning taxation are the DV.2 A fully randomized treatment is impossible, because participants in a deliberative forum will (and should) always be free to exit the sample. Because the universe of participants are all citizens, or even all human beings affected by the policy in question, even a reasonably representative quota sample will be hard to come by. Largely because of limited resource, the present study is a self-selected haphazard sample, described in more detail in chapter 7. A control group of non-participating citizens would be possible in the present study, but was not included because of limited resources. In any event, there were no expected background events or trends affecting the subjectivities concerning taxation of both treatment and control groups, so a control group was deemed dispensable. Whatever changes the participating citizens displayed are likely to be a result of the deliberation. A blind design is impossible because participants will, by the nature of the treatment, always know whether they are participating in a deliberative forum, or not. There should be no placebo deliberations, though there may well be those formats worthy of such a label. Table 1.1: A Commented Table of Comments with Analogous Experimental Design Research Design Concept Hypothetical Mindfulness Study This Study Chapter Research Topic Exercise and mindfulness Deliberative democracy and taxation 1 Background Stress and the body Why taxation matters ?? Theoretical Background Diet (mitahara), body cleansing (shatkarma), breathing (pranayama) and some philosophical background Optimality, Incidence, Haig-Simons Equivalence, Circular Flow, Personal Taxation as well as some axiology and ontology ?? Operationalisation 5 selected asanas (positions), including the rare firefly pose Fundamental choices in taxation: base and schedule ?? Treatment regimen In-Person group sessions at local gym Weeklong deliberation with room and board and extensive learning phases 7 Measurement - Q Methodology 8 Field Report - Report from the CiviCon Citizen Conference ?? Results (Baseline) - ?? Results (Treatment) - ?? Discussion Conclusion We need to complicate this simple design only a little bit to bring it close the present study on deliberation and taxation, as shown in table 1.1. First, let us assume that our M.D. to be presents good reasons to concentrate on a particular kind of exercise, say, hatha yoga. Perhaps, the aspiring doctor argues convincingly that hatha yoga is an especially promising treatment, or that it has been previously understudied, or that really, “exercise” without the kind of body-awareness honed by hatha yoga is a meaningless concept, likely to yield only spurious effects, if any. So it is with this research on deliberation and taxation. In chapter ?? I argue that taxation is a key, yet understudied precondition for a functioning mixed-economy and that to be effective, deliberation needs to engage with highly structured and abstract topics such as tax. Secondly, suppose that our M.D.-hopeful insists on teaching the participants not just some positions, but also other related practices such covering diet (mitahara), body cleansing (shatkarma), breathing (pranayama) as well as some philosophical background. At this point, her supervisors get a little anxious and worry where this might all lead. However, our medical student maintains that concentrating on exercise alone is alien to the tradition and shows how the elements of hatha yoga are neatly intertwined. She is the first to admit that she is not an expert in philosophy, let alone buddhism, but she is confident that she can provide at least some helpful background for participants do understand the broader context. Analogously, this dissertation also requires some deep background for participants and readers alike. In chapter ??, I rehearse a few selected abstractions of taxation and microeconomics, including optimality, incidence, the Haig-Simons equivalence of income, the circular flow of the economy and implications of personal taxation. I describe why these are necessary concepts to any reasoned debate on taxation. Because these concepts, along with their underlying ontological and axiological assumptions are also contentious, I also provide a working vocabulary of economic philosophy to explicate the ideological import of these concepts. Thirdly, imagine that our medical student is also quite particular about proper postures and requires subjects to concentrate on, say, the somewhat obscure and demanding firefly pose, along with four other precisely defined positions, and nothing else. Her supervisors wonder why she could not use simpler, more mainstream positions, such as the downward-facing dog, or really, just some plain breathing exercises. The medical student counters that only these selected five positions, evolved over centuries, form a well-rounded exercise regimen and rehearses some biomechanical research to support her claim. This dissertation, too, is quite particular about the taxes to be deliberated on. In chapter @(hypotheticals) I argue, citing some mainstream economic research, that there are only a few, fundamental choices in taxation, given by the interaction of base and schedule. I show that these choices are highly consequential, irreducibly political, and therefore, essential for deliberating citizens. Fourth, the medical student informs her supervisors that, unfortunately the planned study cannot be undertaken in the usual clinical setting, but that because of all the required instruction, she will need to meet participants in person every morning for a group session at a local gym, which she will decorate appropriately with sandalwood incense.3 Aghast upon hearing this latest proposal, one of the supervisors resigns from the committee and vows to never supervise a student again who so much as mentions Eastern philosophy. The student studies existing treatment regimens, and argues that an intensive intervention is the only way to do this research. She also finds two other studies with similar treatments, one even flew participants out to a Caribbean yoga retreat. The remaining supervisors stay on, perhaps just hoping that the philosophizing will now come to an end, and the study reach firmer ground. This study too, required an unconventional treatment regimen. In chapter @(civicon) I explain why, instead of the more usual short and large-n deliberative fora, to study taxation, a longer and more intensive format is necessary. I review existing deliberative fora, and argue why their experimental value remains limited. I describe the design of the CiviCon Citizen Conference I hosted as part of this dissertation. Fifth, our aspiring clinician recognizes that measuring “mindfulness”, her DV, is quite tricky. Existing survey instruments seem too closed-ended, but entirely qualitative methods do not seem scientific enough for her. Instead, she finds a clever way to holistically measure different experiences of mindfulness, and records how they change during the course of the treatment. Measuring whether a forum was deliberative, and if so, what the substantive effects of the deliberation were may be yet more vexing than quantifying “mindfulness”. In chapter @(q), I review existing measurements for deliberative quality, as well as their results and, finding most of them unsatisfactory for the present needs, suggest an application of rarely used Q-Methodology. 1.2.1 Theory as Explanandum 1.2.2 Theory as Treatment What may strike readers as odd about the first couple of chapters is that these are wide-ranging, but in no way original. They have the feeling of a textbook. This is by design, and follows from the remedial aspiration of deliberation itself. Emphatically, the content of the theory chapters is also nowhere subject to an empirical test in this dissertation. Much like our hypothetical medical student would not test whether participants could successfully do a firefly pose, I never test whether participants understand the deadweight-loss (DWL) of taxation, let alone whether, under which circumstances or to what extent, this abstraction is a positive phenomenon. All these chapters have the status of justifying and explicating the treatment, that is, the framing and briefing for the deliberative forum. 1.2.3 Theory as Measurement 1.3 A Long Story This is probably an unconventional, and possibly risky project, but it has promise, too. If, given the right design, deliberative democracy can enable citizens to rule on complex issues, political scientists will have a very able, and attractive hypothetical to compare with, and deliberative experimenters should have more courage to venture out to more topics facing our sovereigns. Not just as social scientists, but as citizens too, we must know whether the once historical achievement of aggregative democracy is now withering away under the assault of tightly concentrated special interest and obscuring complexity. If it can show its stripes, deliberative democracy may well be our last, and also our best hope, to reveal the perils of pluralism, then to live up to our greater capacity for communicative action. If, under a normatively more attractive democratic process, people were to resolve some misunderstandings about, and agree on different, but doable and desirable taxes, welfare state research and political economy would have to explain a much greater retrenchment and democratic failure. Not only as social scientists, but as citizens, too, we must know that better tax we could agree on, and if it exists, what is keeping us from it. Taxation, underneath it all, is the social contract (Schumpeter and Swedberg 1942), and its vitality will determine the prospects for modern progress. This dissertation is, lastly, , despite it all (, p. ): the crime remains unsolved, and I can point to no single societal culprit. Maybe, that is for the better, as few good things ever come from such structuralist exorcisms. There may be many reasons why we have no better tax, and the kind of democracy we have might plausibly be one of them. That alone ought to worry us. If, in fact, our pluralist democracies are becoming illegitimate and inefficient, their increasing failures will plague many arenas of collective choice other than taxation. The death knells of legitimate and efficient pluralism — complexity and tightly concentrated special interest — are the conditions of late capitalist society. I that taxation may just be one these policy fields in which the limits of pluralism show early, and clearly (, p. ). Indeed, the entire story could be told the other way around, with popular rule as the corpse that found its master in the intricacies of tax. This dissertation is fairly long and not very original. Others already have, in different places and with different foci, written almost all that is contained in the next pages. And yet, so far, no one has bothered to connect the dots. That is what I offer here. Raymond Williams (1992) has argued that the detective story appears at roughly the same time as sociology, and for the same reason: to penetrate an increasingly opaque modern world, as Conan Doyle’s Sherlock Holmes does, “by an isolated rational intelligence” (Williams 1992: 88), otherwise covered by London fogs, or compartmentalized by functional differentiation (Durkheim 1893). And so it is with this crime story of taxation and democracy: shrouded in nebulous complexity, pigeonholed into disjunct institutions and areas of expertise, it needs a lot of detective work to connect these dots. To hear this case, I urge the reader to consider all of the evidence reviewed in this exposition, even if some of the detail might, at first, seem trivial. It is not. As Sherlock Holmes reminds us, “there is nothing so important as trifles” (Doyle 1891, 238). From the incidence of the corporate income tax (CIT), to the liquidity effects of taxing property and the malaggregation of ill-structured preferences — these “trifles” are all important exhibits to test the case that democracy, taxation, and with it, all our modern lives, could, and therefore, should be better. Some stories just cannot be told in the short form, and the story I have stumbled upon and here report may be one of those. Luckily, detective stories can also be fun to read, and so, I hope, is this one. References "],
["hypotheticals.html", "Chapter 2 Pragmatic Hypotheticals 2.1 The Epistemology of Hypotheticals 2.2 Axioms for Desirable Hypotheticals 2.3 The Ontology of the Doable Hypothetical", " Chapter 2 Pragmatic Hypotheticals There is a war between the ones who say there is a war and the ones who say there isn’t. — Leonard Cohen: New Skin for the Old Ceremony (1974) If they can get you asking the wrong questions, they don’t have to worry about the answers. — Thomas R. Pynchon (*1937) To proceed by the method of elimination is always to conceive of such hypothetical outcomes, and then, to falsify them. Still, hypotheticals4 are rarely invoked in social scientific explanations: ever hear of an empirical sociologist or political scientist comparing the continental welfare state (Esping-Andersen 1990) or Westminster democracy (Lijphart 1999) to a non-existent alternative? We can see why few empirical social scientists would ask such a question: to say that taxation could be more efficient and equitable, and to suspect that someone (for example, the rich) or something (for example, capitalism) is preventing it from being so reeks off conspiracy theory. Forensics, in some ways, has it easier: if a coroner diagnoses heart failure from old age as the cause of death, there is no murder. Just which social ills are unavoidable and thereby the social scientific equivalents to a natural death is, unfortunately, more contentious. Plausibly, without the bullet, an otherwise healthy person might have lived to see another day, but writ on a societal level, such hypothesizing about alternative, preferable outcomes is hard. Would democracy not always fall short of its ancient ideal, because we can no longer all meet and debate in person? Would taxation not always be wasteful, because it suppresses otherwise pareto-improving exchanges? Would welfare not always be a drag on growth, because it dampens incentives? Empiricism \\[itm:empiricism\\] “We cannot know”, might reply the empiricist (Bacon 1620; Locke 1689a; Hume 1739) because such hypotheticals can, by definition, not be experienced with our senses and nothing can be induced from them. Whatever democracy, taxation and welfare could be, we cannot know. Similarly, positivists might find these hypotheticals unknowable, either because absent, they have been falsified (Comte 1842; Durkheim 1895), or, because absent, they cannot be falsified (Popper 1934). Idealism \\[itm:idealism\\] “We know only through ideas”, might respond the idealists (broadly Kant 1781; Hegel 1807), including the ideas invoked in these questions. Whatever democracy, taxation and welfare could be, is beside the point, because our knowledge of them exists independent from their factual or hypothetical reality. Specifically, interpretive sociologists (Weber 1897) might argue that the above questions may contain in them already one of several, cultural perspectives, ideas or terminologies in which all answers, too, would be have to be phrased. “Waste”, “growth” and ‘incentives’’ rather than objective things, already are cultural reality (compare Béland and Cox 2010). Constructivism \\[itm:constructivism\\] “Asks who?”, might retort the constructivist (Berger and Luckmann 1966; Paul 1984), because these are not questions about an objectively knowable world, but rather, the act of asking these questions and of invoking these concepts makes that social world. Whatever democracy, taxation and welfare could be, is beside the point, too, because legitimacy, efficiency, equity — or any other criteria — become real, as we think, demand and reject them. Rationalism \\[itm:rationalism\\] “We can know”, might announce the rationalists (Descartes 1637; Spinoza 1662; Leibniz 1704) — “but hypothetical or actuality do not matter, either way”. Whatever democracy, taxation and welfare could be, we can know by deductive reasoning, without relying on our experience — or absence thereof. Clearly, none of these epistemologies taken to the extreme, can help investigate those desirable, and doable hypotheticals upon which I have stumbled. Welfare, taxation and democracy certainly are ideas, but they are also bound by empirics. They are accessible to deductive reasoning, but such reasoning can make a social world, as much as it seeks to explain it. What I need, is a (p. ). Equally clearly, such a cartoonish overview of epistemological traditions does not suffice to ground this dissertation. Fortunately, this is — as promised — a pedestrian thesis and I will not abscond into any philosophy of science. Unfortunately, I am also largely ignorant of these theories of knowledge, and can only provide a provisional epistemology that seems to work for democracy, taxation and welfare. I take no pride in my epistemological ignorance, but I am weary of the obliviousness of these, and other meta-(meta?)-concerns. In a slightly different context, Noam Chomsky (1997) has observed that “ontological questions are generally beside the point, hardly more than a form of harassment” (1997, 132). As harassments go, I have found that such epistemological and ontological debates often arouse what would appear to be deeply-felt passions in even the drowsiest of graduate seminars. This fervor always struck me as eerily performative (compare Goffman 1959; Butler 1997), as if, in addition to — or as part of5 — doing gender (West and Zimmerman 1987), we were also doing social science. If only to stick to my limited last, I prefer a social science that serves people other than its researchers, and accordingly sympathize with pragmatist epistemologies, where human problems are primary and reified “philosophical fallacies” (Dewey 1929) take a back seat to the problems they were invented to solve. In Wikipedia (2013)’s apt description of an (ecologically) pragmatist epistemology: &quot;inquiry is how organisms can get a grip on their environment&quot; (retrieved in March 2013). Even though, because hypotheticals are hardly invoked in the social sciences, are hard to specify, and are harder to know about I must devote this chapter to explicating the (p. ), (p. ) and (p. ) of my research. 2.1 The Epistemology of Hypotheticals 6 Detective shows often confine the forensic medicine to some expository dialogue between coroner and inspector at the morgue. I need a little more exposition here, stretching over several chapters of welfare economics, normative political theory and public choice, before the empirical action even begins. Again, I merely proceed by the method of elimination. To open an investigation, I must first know if there are alternative welfare states, taxes, and democracies out there, and if so, what they are. Surely, to suggest that we might learn something about the world by asking what is not appears to be an odd epistemology. Natural scientists probably do not spend much time thinking up, say, a counterfactual universe without gravity, and explaining why it is not (or maybe that is what they do at the Large Hadron Collider?!). Positive social science, at least, needs to pose these why-not-questions, because, unlike physics, it is concerned with who or what made the world the way it is.7 Positive sociology, political science, and this dissertation, all ask such second-order questions. Even to pose them, we need all the possible first-order answers of how the world could be, but is not. The hypotheticals I establish in the following chapters are these first-order non-answers. 2.1.1 First-Order Questions {#1st-questions} Hypotheticals must be (p. ), and — if you allow some humanist bias — they must also be at least somewhat (p. ). Those criteria both raise inquiries of the first order, asking what is normatively good and what is positively possible, respectively. Normative (or prescriptive) social science asks such first-order questions as how to emancipate people (critical theory, maybe from Gramsci (1971) to Adorno (1974), Horkheimer and Habermas (1984)), what makes rule legitimate (political theory from Aristotle (n.d.) to Dahl (1989)) or what allocation might be fair (distributive justice including Friedman (1962) and Rawls (1986)) and even what makes an economy rich (economics from Smith (1776) to Hicks (1939)). Even these apparently normative and first-order questions turn into positive and second-order questions when their underlying assumptions on human nature are tested or problematized, respectively (more on that twist on p. ). Positive social science asks few, if any, first-order questions, because as it asks about the social world (for example, health insurance), it seeks to explain the social conflict of answering a first-order question (for example, how to spread risk). To the extent that nominally social sciences have carved out positive research agendas of the first order (for example, cognitive psychology, behavioral economics), they cease to be social science but revert to a natural science of some ideally physically rooted phenomenon (for example, neuroscience). Tautologically, if the social sciences are to be concerned with explaining social choice, they know no positive first-order questions, because first-order status negates choice.8 The aspects of the social world under study here are welfare, taxation and democracy. To the extent that these social choices hinge on last reasons, they raise normative first-order questions. These first-order questions of efficiency and equity, fairness and legitimacy have been widely discussed, and I will reference them only briefly in this dissertation. But welfare, taxation and democracy also raise two kinds of positive questions:9 A Priori Knowledge \\[itm:a-priori\\] There may only be finite or even unique ways to organize these institutions that abide by formal logic, per reasoned, a priori knowledge. For example, a welfare state financed solely by printing money may be an economically illogical institution (possibly resulting hyperinflation resembles a pyramid scheme). Similarly, taxing judicial persons, such as corporations, may be economically nonsensical because such organizations are no moral subjects and the incidence would be arbitrary (). As a last example, a democratic aggregation mechanism may not be able to maximize both majority rule and proportional representation, simply because the two criteria conflict. Knowledge about what is logically possible or impossible flows from a epistemology (p. ). A Posteriori Knowledge \\[itm:a-posteriori\\] These formally logical designs may be further limited by empirical, first-order findings on human nature or other material conditions, per experiential, a posteriori knowledge. For example, a welfare state financed by extracting and burning fossil fuels may soon run into physical constraints if we observe limited resources and related global warming. Similarly, a fully-planned economy (instead of taxation) may, amongst other things, dramatically overestimate the cognitive ability of human planners to centrally aggregate and process dispersed information. Lastly, a democracy modeled on ancient Athens, but with universal suffrage and in globally integrated economies may fail simply because human beings interact too slowly to listen to every fellow human on the planet, let alone to get anything done.10 Knowledge about what is empirically possible or impossible flows from an epistemology (p. ). These are, admittedly, uncontroversial constraints and ludicrous suggestions — I want to keep the more interesting problems for later — but they illustrate an important point: there probably are some first-order, positive limits the social world faces, even though we may know little and disagree a lot about them. Crucially, we may disagree on whether a given question is of the first or second order. The social sciences, in particular, have a habit of reassigning first-order questions to second-order status: that is the project of constructivism (for example, Berger and Luckmann 1966) and (p. ) to see social phenomena not as “things in the world, but perspectives on the world” (Brubaker 2002, 174 on ethnicity, emphasis in the original) or as contested and consequential second-order answers. Still, even a die-hard idealist or constructivist may concede some irreducibly positive, first-order questions, and to those, the social sciences offer no answers. So it is with welfare, taxation and democracy. Their design probably faces some — albeit uncertain and contested — positive limits of the first order. To ask about these limits is, emphatically, not a question for the social sciences. Instead, nominally social scientific, but formally mathematical-logical disciplines such as equilibrium economics11 and public choice ask about the internal consistency of such societal institutions, based on assumptions on (p. ), which are in turn hypothesized and falsified by such natural sciences as evolutionary psychology (a.k.a. sociobiology), social psychology (initially Kahneman and Tversky 1979), cognitive science, or even physics. I suggest two examples of such first-order, positive substrates relevant to my subject matter: The first theorem of welfare economics is often invoked as a first-order constraint on welfare state interventions or taxation (the dwl), and it also shines through in some radical defenses of pluralist democracy (for example, Caplan 2007). The theorem states that over any given distribution, free competition equilibrates at pareto optimality12 and that therefore, no allocative intervention can make anyone better off without making someone else worse off. It is a mathematically formulated argument about some internal consistency of the market mechanism: given certain (p. ), market equilibria cannot be pareto-improved. As such, the theorem is just that, an exercise in formal logic, not more: not empirical claim (no “proof” that actual markets pareto-optimize) and not normative statement (no justification for pareto-optimization over existing distributions as desirable). But the first theorem is also not less: properly understood, it is a first-order constraint and invites no second-order critique. Similarly, homo economicus is often invoked in the field of welfare (“knaves” as in LeGrand (2003)), taxation (“people react to incentives” as in Mankiw (2004, 24)) and democracy (“rationally irrational” as in Caplan (2007)). The concept implies that human beings make rational, self-interested and utility-maximizing decisions (maybe first Mill (1848), Smith (1776), recently including Robbins (1976) on rational choice, all summarized in Persky (1995)). The ideological campaign (and backlash) wrought by homo economicus and its offspring notwithstanding, it is, properly understood, merely a falsifiable13 assumption about human nature that welfare, taxation and democracy might have to heed if it were, in fact, correct. But even if it were true, homo economicus would be just that, an empirically verified model: no more (no normative claim of how we should be), but also no less (no socially contingent phenomenon in need of deconstruction). 2.1.2 Second-Order Questions {#2nd-questions} As forensics informs a criminal investigation, the first-order answers provide the reference for the second order inquiries. These ask who or what decides first-order answers, or, metaphorically, who or what brought about the observed forensic outcome (see , p. ). Second-order inquiries seek to criticize, explain or test the social conflict over first-order questions. Social science asks plenty of those questions on welfare, taxation and democracy. It asks, for example, why and how welfare states — a first-order answer — evolve(d): because modernization replaced inherited, familial status with citizenship and the market (Titmuss 1974; Marshall 1950), because industrialization required an appeased, reliable and healthy workforce (Flora and Alber 1981; Wilensky 1975), because workers gained power (Korpi 1983; Jessop 2002), because institutions prevail (for example, Rothstein 1998), because ideas matter (for example, Stiller 2009) because initial class cleavages lead to different degrees of commodification (Esping-Andersen 1990) or because capitalism comes in variants (Hall and Soskice 2001).14 So it is with the second-order theories of taxation: it arose as proto-states extracted the resources enabled by, and necessary for greater economies of scale in their state-making and war-making (Tilly 1985), it (sometimes) erodes to lower levels as internationally mobile factors of production and consumption arbitrage over different national rates (recently reviewed in Genschel and Schwarz (2010)), it persists at similar levels as domestic politics prevents roll-back (Swank and Steinmo 2002), or it changes bases and schedule in response to these pressures (Kemmerling 2009). And so, too, it is with the second-order theory of pluralist democracy: states introduced popular rule, because the costs of otherwise considered illegitimate extraction became prohibitive (Tilly 2006), democracy belongs to a broader syndrome of emancipating modernization, including a market economy and corresponding rational and self-expression values (Inglehart and Welzel 2005) or the sequential development of citizenship (instead of other, pre-modern statuses) entailed democratic rights (Marshall 1950). These are variations on the questions of sociology: what binds us together (social integration), and what keeps us apart (social inequality).15 These are also the questions of political science: how do power, norms, ideas and institutions rule human interactions? These second-order questions are open to all epistemologies, including the the anti-positivist traditions of and . They can also be — as seems to be currently en vogue16 —, but need not be, entirely or, inquiries. And important questions, they are, asking us, that lone “hypercultural” species (Henrich 2003), how we make our own history. In our rich time and in our unequal place, welfare, taxation and democracy may just be the historical battlegrounds, on which these sociological and political forces operate. This dissertation develops a second-order theory of social change to explain the defeats these institutions have suffered in late capitalism, and tests it empirically.17 image The distinction between first-, second-order, normative and positive questions summarized in (p. ) matters for at least two reasons: Clearly addressing one of these questions at a time, and striving to keep them separate makes for better social science. With these categorically different questions come different goals, languages and methods. With blurring or negating these categories come “politics-based evidence making” (The Economist 2012), apolitical ignorance or both. Because positive social science — including this dissertation — is out to explain the second-order decision on first-order questions, it must consider some first-order theory first. 2.1.3 First Order Theory First {#1st-questions-first} I am ultimately interested in the second-order questions of welfare, taxation and democracy, but proceeding by the method of elimination, I must first ask and answer the first-order questions: what should and could welfare, taxation and democracy look like. Consider the two alternatives, that I must rule out before any second-order questions can be raised: No Desirable Hypothetical. If I could find only the presently observed design of welfare, tax and democracy to be at least somewhat desirable, there would be no social conflict to be explained, much like there is no need for a political science of wearing sunscreen. (I partly take this , p. ). In the crime story metaphor, if the corpse in question had previously run amok shooting innocents, rupturing that persons coronary artery with a bullet might be have been self-defense or the last resort, and the only desirable course of action. No further criminal investigation may be necessary. No Doable Hypothetical. If I could find only the presently observed design of tax and democracy to be materially doable, there would be no social conflict to be explained, much like there is no need for a sociological theory of gravity. In the crime story metaphor, if said deceased suffered from an irreparable birth defect that caused the heart failure, no outcome but death is materially possible. In that case, too, no criminal investigation will be necessary. Conceiving such to-be-ruled-out hypotheticals raises normative (desirability) and positive (doability) questions of the first-order. These first-order questions cannot, as I explained above, be answered by social sciences (alone), but I must instead reference other disciplines, much like a detective hears forensic evidence. If these ancillary disciplines reveal one, or several such desirable and doable hypotheticals that I cannot rule out, the non-occurrence of such hypothetical welfare, taxation and democracy beg a second-order explanation just as the observed arrangement requires explanation. In fact, the two are the same thing: to explain the presence of the present arrangement is to explain the absence of all absent arrangements. For illustration, consider the inverse scenario, in which I find no desirable and doable hypotheticals in welfare, taxation and democracy. If that were the case, there would be no second-order decision to be explained, because without such hypotheticals, there is no human choice. If however, there are desirable and doable hypotheticals, any second-order question always invokes these hypotheticals. 2.1.4 Fallacies and Risks To be sure, this reasoning is not logically watertight and might turn into an argument from ignorance: even if I find no reason why hypotheticals should, or could not be, they might still be undesirable or impossible for some other reason. Absence of evidence, here, too, does not constitute evidence of absence. The same problem must plague medical forensics: no evidence of natural death does not prove a violent death and any conclusion to the contrary might cause an unnecessary murder investigation. In practice, this possible fallacy simply raises the question of where you place the burden of proof, because you can err either way: if you take absence of evidence of violent death as evidence of natural death, you might foreclose a necessary investigation and let someone get away with murder. Tellingly, when it comes to human corpses we place the burden of proof on the status quo: if the cause of death cannot be established — if there is no evidence of natural death — the prosecution customarily opens an investigation. The positive social science of welfare, tax and democracy seems to have adopted a somewhat laxer standard. Here, much empirical work implicitly places the burden of proof on the hypothetical: without evidence that the hypothetical is normatively desirable and materially possible, the fundamental status quo of welfare and tax needs no social explanation. Conveniently, as hypotheticals go, they rarely produce conclusive evidence of their desirability and doability. Maybe, this is just the skepticism that positivism calls for — or maybe, , and such social science invariable turns latently affirmative (, p. ). Either way, I here place the burden of proof on the status quo. If I find no reasons to the contrary, I assume that there are several doable and desirable designs of tax and democracy, and require social science to explain the absence of all but the presently observed design. If we apply the standard of forensics, any second-order theory of social change must be able to explain how the social conflict under study resulted in the non-occurrence of these alternative designs, especially the attractive ones. I still want to shoulder part of the burden of proof. In tax, I can only plausibilize the hypotheticals by reviewing reputable work by others, and deduce my case from reasonable assumptions about human nature, and the economy. I must rely mostly on a epistemology (p. ). Natural experiments — the gold standard — are unavailable, and all others suffer from limited external validity: a modern economy does not fit in the laboratory and equilibrium simulations poorly model such inframarginal institutional changes. In democracy, I similarly plausibilize the hypotheticals, but I can also point to and undertake some preliminary experimental tests. To suggest, as I do, that there are desirable and doable hypotheticals in tax and democracy, must remain a preliminary proposition open to falsification. To the (???) scientist, there is always some absence of evidence, and therefore, no evidence of absence. If these particular hypotheticals turn out to be undesirable or impossible, so collapses any second-order hypothesis I might wager later, but, I hope, we will still have learned something. Not only the advancement of science depends on such falsification, the history of progressive causes is also littered with what might charitably be called “false positives”. My chosen hypotheticals — deliberative democracy and progressive taxation of (unimproved) land value and (postpaid) consumption, too, may inspire such false hopes. They, too, should be approached with great care, especially, when they necessitate constitutional or otherwise reform of liberal democracy, as deliberative democracy may one day do. Tax reform, at least, should be open to falsification and is not an end in itself, especially because it thoroughly hinges on economic contexts and human motivation. Then again, by any historical standard — once-hypotheticals failed (for example, socialism) and successful (for example, universal suffrage) alike — here are some fairly incremental reforms as carefully liberal in their (p. ) as they are conservative about (p. ). 2.2 Axioms for Desirable Hypotheticals “I believe in clear-cut positions. I think that the most arrogant position is this apparent, multidisciplinary modesty of ‘what I am saying now is not unconditional, it is just a hypothesis,’ and so on. \\[...\\] I think that the only way to be honest and expose yourself to criticism is to state clearly and dogmatically where you are. You must take the risk and have a position.” — Slavov Zizek and Glyn (2003, 45) Normatively desirable hypotheticals are preferable to others if we assume a basic humanist, or critical intention of positive social science and policy analysis to improve human lives. From a strictly positivist point of view, this is the flimsiest of epistemological decisions I take: we might learn just as much about the social world from the nonoccurrence of bad outcomes, as from the nonoccurence of good outcomes.18 My bias for desirable hypotheticals might be epistemologically arbitrary, but it is — again — pragmatic: there may just be so many undesirable, but doable counterfactuals (why not return to workhouses rather than welfare, tariffs rather than taxation and mob rule rather than liberal democracy?) that it becomes plainly easier to pick amongst the supposedly fewer, attractive hypotheticals. Moreover — if mostly implicitly — modernization theory, (structural) functionalism and related traditions might have convincingly explained away some of those graver regresses of civilization. Lastly — only slightly tongue-in-cheek —, on a second look at the factual horror chamber of welfare, tax and democracy, there might not be that many even less desirable, doable hypotheticals left. What then, makes for desirable hypotheticals? Emphatically, hypothetical tax, welfare and democracy, and with them, this research, hinge on last reasons. Desirable tax and welfare are rational, efficient and fair. Democracy, too, must be these things and more: emancipatory, equal and deliberative. Unfortunately, these last reasons sometimes conflict, and they do not even flow from any single ethic. What makes my hypothetical tax, welfare and democracy desirable is, instead, a hodgepodge of mongrels from quite distinct normative theories. I here list five of them and show how they apply to taxation, welfare and democracy: Virtue Ethics \\[itm:virtue\\] Tax, welfare and democracy are not desirably merely to the extent that they constitute, or foster virtue inherent to human action and character (from Aristotle (n.d.), Plato (n.d.), St. Thomas Acquinas (1274) to Schwartz and Sharpe (2006)). At least under neoclassical dictum, tax and welfare are desirable to the extent that they do not depend on, nor improve human virtue, but efficiently orchestrate our selfish demons (compare Smith 1776). Liberal democracy, similarly, is desirable to the extent that it sidesteps questions of personal virtue and guarantees an agnostic process for all people, no matter the quality of their character (compare Dahl 1989). And yet, I rely on virtue ethics when I praise markets and states for letting us reap (p. ), that supposed destiny of our nature (for example, Wright 2000). The case for deliberative democracy, too — as all virtue ethics — implies a telos of human life, to the extent that it posits intersubjective understanding or communicative action as a last reason (Habermas 1984). Consequentialism \\[itm:consequentialism\\] Tax, welfare and democracy are also not desirable merely by the outcomes they produce, be they utility (Bentham 1789; Mill 1863) or even self-interest (Rand 1957). Attractive consequences, especially different aggregate functions of utility19 go a long way to justifying taxation and welfare, but I would not bet the farm on them. Even if, say, relative inequality were empirically unrelated to “subjective happiness” (as Kalmijn and Veenhoven (2005) seem to imply), and progressive taxation therefore not a maximizer of aggregate happiness, at least two other reasons would remain: Straightforwardly, we might wish to maximize consequences other than some measure of hedonistic gain, including equality, growth, knowledge or liberty. Utilitarianism — as other consequentialisms — side-step entirely the question of how the desired consequences would be measured, and who would do the observing.20 Utilitarianism merely posits an ideal observer (Rawls 1988) — such as Veenhoven (2000)’s subjective happiness — and does not allow us to problematize the conditions under which consequences are enumerated, or measured.21 More fundamentally, progressive taxation — or some other policy — may remain attractive not because of any distribution or measurement of consequence, but because equality might be inherently virtuous, or the goods it can buy (including universal health care) may be a matter of deontological rights. The consequentialist case for democracy is even thinner: as Dahl (1989, 176) reminds us, equal intrinsic worth of humans alone might be achieved by a benevolent dictator. Only in conjunction with (deontological? virtuous?) personal autonomy does it require democratic rule. And yet, hypotheticals about taxation and welfare, and even democracy cannot ignore consequences, especially (p. ) and its (p. ). Deontological Ethics \\[itm:deontological\\] Tax, welfare and democracy are also not merely desirable to the extend that they abide by some set of absolute rights and duties. On the one hand, most such deontological ethics are not specific enough to inform choices of tax, welfare and democracy. For instance, the Golden Rule — do unto others as you would have done to yourself — may not tell us much about what we should tax, when we should intervene in the market or how we should count votes. Kant (1781)’s similar categorical imperative is equally mum on these matters, maybe because his is a philosophy and deals with man in the singular, not men in the plural as Arendt (1958) demanded of political theory.22 Natural rights theories (from Grotius (1625)), be they liberal (from Locke (1689b) to maybe Rawls (1993)), spinoff (right-)libertarian (diverse, prominently Hayek (1944), Nozick (1974)) concerned with negative freedoms from (for example, false imprisonment), or positive freedoms to (for example, human dignity, both Berlin (1969)) provide more guidelines, but they, too, are limited. Natural rights theories, and especially liberalism, set outer limits on what a government or person must not do to, or must not fail to do for the bearers of these rights, but they are too digital for a normative ethic of welfare, taxation and democracy. For example, market interventions may be either permissible (freedom-to) or illegitimate (right-libertarianism), but natural rights do not offer much qualifications in-between. It is of course the great appeal of natural rights that they are unconditional (as in “Human dignity shall be inviolable”, German Basic Law 1948) and pre-social (as in “…that all men are created equal”, US Declaration of Independence 1776), but that makes them a little too lofty for inherently social, and contingent institutions as welfare and taxation. There may, for example, be many (or no) taxes that allow for life, liberty and the pursuit of happiness (ibid.), but it would be a stretch to argue that these natural rights are best respected or least harmed by any particular tax on consumption, income or wealth. Crucially, too, many collective decisions in late capitalism must balance one persons liberty, against another persons pursuit of happiness, or similar rival natural rights. Contract theories (from Hobbes (1651), Rousseau (1762) to Rawls (1971)) are more specific still, by prescribing a decision-making process or condition to balance and protect rival rights. While this may suffice to specify a desirable democracy (authoritatively Dahl (1989) on liberal, pluralist poliarchy), contract theories still recede into procedural norms on taxation and welfare. Dahl (1989) — but not Rawls (1971), as we shall see! — may inform us about how we should decide between, say, an income or a consumption tax, but abstains from substantive judgement. When, however, the very quality of the (second-order) decision process over these tax choices is in question — as it is in this dissertation — evaluating first-order hypotheticals by a procedural standard risks infinite regress: a tax is good if the process was good, and the process is good if the tax is good, which is good if …and so on. On the other hand, tax and welfare especially, are institutions that negotiate and trade off competing rights and duties, rather than strictly abide by them. Natural rights to, say, property and health care, offer no all-or-nothing propositions in taxation. Instead, one tax may encroach less on property than another, or at a greater (utilitarian) gain in health care than another. In sum, deontological ethics, and especially its popular, liberal, contractual and procedural representatives are both not specific enough, and too demanding for desirable hypotheticals in welfare, taxation and democracy. And yet, I cannot do without deontological ethics, especially not without liberalism. Even if governments could expropriate some owners at supposedly minimal welfare losses — as the Eurogroup has allowed (encouraged?) the Cypriot government to proceed in 2013 with big savers — it should not do so (nor be allowed to), because protection of confidence is a deontological right. Similarly, even if democratic rule found executive pay excessive, and anticipated no dwl fallout, it should not — as Switzerland did in 2013 — regulate uncoerced exchanges (Nozick 1974) even between CEOs and shareholders, when a less intrusive policy (tax!) is available, simply because liberal states do not do that. Ethics of Care \\[itm:ethics-of-care\\] Lastly, tax, welfare and democracy are, for the most part, not desirable because they establish caring relationships, as some (difference?) feminists have demanded (Noddings 1984; Gilligan 1982). Whether we like it or not — in fact, we should like it — our world is ruled by abstractions, too, and to these must respond our institutions of tax, welfare and democracy. Absent a regress to lower, poorer levels of functional differentiation — or some yet unforeseen explosion in human capacity — we depend on wide-ranging abstractions such as a price system to orchestrate at least some of our relationships by mutual self-interest.23 Ethics of care govern individual, concrete and personal relationships, and not those anonymously expressed in price signals, or some other abstraction. Taxation, welfare and democracy, however, have to ontologically respond to such abstractions, and to that extent, cannot be informed by a normative ethic that insists otherwise. And yet, I cannot abandon an ethic of care entirely. For once, caring reminds me that we may have other, more intimately personal capacities (or duties?) for goodness and desirable taxation, welfare and democracy may consequently have to leave room for caring to flourish. For example, I imply an ethic of care when I later suggest that taxation should respect the promises of (mutual) care in marriage and family and not discourage, nor commodify such unions. More broadly, a good welfare system might have to carve out realms where such necessarily priceless care can be extended — not exchanged! — and might have to reallocate resources to those, especially women, who spend much of their energy caring for others, without compensation. Something akin to ethics of care are also implied by some proponents of deliberative democracy, maybe including those who stress the importance of story-telling in democratic participation (Poletta 2006). Deliberative formats may also raise ethical questions of care simply because they make people engage with other people. Intersubjective understanding certainly requires the kind of personal relationships to which ethics of care supposedly apply. I know then, that I know nothing — at least nothing consistent — about what makes tax, welfare and democracy desirable. That might be a socratic moment, but not a happy one for me or this dissertation. It frustrates me that I can summarize these ethics only in the crudest of terms, and that I fail to synthesize them. Such ignorance is troubling, too, for research as this, based on a social science education as devoid of normative theory as mine: clearly, these theories matter for tax, welfare and democracy as for any scholarship about them. Tracking down the choices and controversies in tax, welfare and democracy to different ethics is important work that I have to do without, resting this work on quite murky foundations. All I can offer is a modicum of argumentative transparency, by labeling any downstream axioms in tax, welfare and democracy with the roughly appropriate normative ethics from which they flow. Because none of the above ethics are sufficient, but each of them necessary to formulate desirable tax, welfare and democracy, I subscribe neither to virtue, nor consequentialist, nor deontological, nor care ethics but pick and choose amongst them. I guess that makes me an ethical pragmatist of sorts, if a confused one. Pragmatic Ethics : \\[itm:pragmatic-ethics\\] — not identical with pragmatism, and never to be confused with realism or Realpolitik — may best enumerate what makes tax, welfare and democracy desirable, for at least two reasons: 1. In pragmatic ethics, morality can progress over time [@Dewey1932], much as science advances through iterative inquiry. Such tentative morality implies neither relativism, nor inaction: there may still be absolute values out there, we just cannot be sure that we have distilled them yet, but should act on whichever approximation we have tentatively arrived at. Tax, welfare and democracy, too, are such inherently tentative and contingent enterprises. For example, when comprehensive income taxes were introduced to more neatly bifurcated class societies, the ethical quandaries of taxing labor and capital incomes might not have been foreseeable: labor income generally accrued to poor workers, capital incomes to rich capitalists, end of story. Similarly, as universal suffrage was wrung from the *ancien regimes* and liberal democracy enshrined to fend off mob rule around the same time, the limits of a merely electoral and pluralist democracy in a complex world might not have been conceivable. So too, no doubt, will the reforms of tax, welfare and democracy I suggest here reveal their blind spots, come progress. Acknowledging that our moral understanding may be imperfect, or even somewhat historically contingent, also in no way restricts us to the status quo under which this understanding was reached. Pragmatic ethics can be quite radical, maybe *because* it accepts its own tentativeness and contingence: we can &quot;transform the character of our relation to social and cultural worlds we inhabit rather than just to change, little by little, the content of the arrangements and beliefs that comprise them.&quot; [@Unger2007 6-7]. I am inspired by such *anti-necessitarianism* [@Unger1987], even if --- or because? --- tax, welfare and democracy are not the stuff of revolution, but rather the &quot;indispensable if insufficient \\[...\\] piecemeal and cumulative change in the organization of society&quot; [@Unger1987 xix]. I am keenly aware that especially reformed tax and welfare, but even deliberative democracy will, at best, progress us a little further to a point where we --- probably our progeny --- can better conceive morality, and undertake further tentative transformations we cannot even dream of. 2. Pragmatic ethics are also, as the name implies, intensely practical. Pragmatic ethics cannot be conceived of in other than practical terms: &gt; *&quot;Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. &gt; Then, our conception of these effects is the whole of our conception of the object.&quot;*\\ &lt;!-- %add first name, correct? --&gt; &gt; --- William @Peirce1878 [293] This *pragmatic maxim*, as it became later known, works well, especially for tax and welfare. The proverbial proof of their ethical pudding, too, lies in the eating. For example, a tax on income or consumption becomes good or bad greatly by how it works in practice: what it curtails, what it wastes or what it hurts --- very little of which can be read in the Platonic idea of either of those alone. Admittedly, such pragmatic ethics are fairly amorphous, and they do not constitute any of the clear-cut positions that Zizek and Glyn (2003) demand in the above. They merely provide the normative theory under which I can organize, and justify the following hodgepodge of axioms for desirable hypotheticals in tax, welfare and democracy. Here then, are my positions. 2.2.1 Liberal Limits and Procedure Desirable hypotheticals in tax, welfare and democracy are liberal. They do not infringe on the most extensive basic liberties compatible with similar liberty for others (Rawls 1971), a liberal formulation of the Golden Rule of reciprocity. These include the political and civil liberties enshrined in various liberal constitutions, but, following Rawls (1971) do not extend to unencumbered private property of the means of production or unlimited freedom of contract, as libertarians would have it. Desirable hypotheticals in tax, welfare and democracy must be liberal both in the substance of, as well as in the process by which they bring about social chance. Substantively, these regimes must not constrain the lifestyles of people, except if and to the extent that such choices conflict with — as Rawls (1971) posited — the choices others. This is, for example, a very real concern in making consumption taxes progressive, or in taxing savings, both without dictating consumption baskets or prescribing a financial biography. Similarly, democratic fora should encourage “alternative conceptions of the common good” (Cohen 1989, 18, emphasis added) rather than promote a specific social change, even if such a coherent scientific agenda could be found. This insistence on a plurality of lifestyles and ideas of the good distinguish a desirable hypothetical from totalitarian zeal and hermetic ideology. Procedurally, desirable hypotheticals rely on democratic advocacy, not armchair guardianship to become real. No matter, say, the (p. ) value of a pct, its introduction must respect, as Dahl (1989) has highlighted, both (!) intrinsic equality (1989, 84) and (liberal!) personal autonomy (1989, 97ff). In his influential formulation (Dahl 1989, 109ff.), it can only be realized by a democratic process marked by at least: effective participation, voting equality at the decisive state, enlightened understanding and, control of the agenda and inclusiveness These criteria imply a fairly conventional catalogue of negative rights, or freedoms from and a standard formulation of democratic process (but not substance). Together, these liberal norms take precedence over any of the other, below axioms for desirable hypotheticals: in Rawls (1971)’s formulation for his Theory of Justice, violations of any of these rights “cannot be justified or remedied by other \\[...\\] advantages” (1971, 81). They cannot be traded off other benefits, but can only be limited when they come in conflict with one another, as for example, freedom of speech and defamation legislation may. I posit these rights (), but I also think (p. ) that for the foreseeable future, these norms may be our best, if tentative and minimal bet of what people and government should never do to people. Maybe, liberal and other deontological norms can also serve under some kind of precautionary principle in ethics: given at least some uncertainty over, or contradiction within other normative ethics — especially consequentialism — we should accept liberal limits to policy because they might help reduce some assymetric downside risks. 2.2.2 Rational Preferences Desirable hypotheticals in tax, welfare and democracy are based on and help people express rationally coherent preferences. (von Neumann and Morgenstern 1944) have axiomatized rational coherence thus: Completeness \\[itm:completeness\\] For any two alternatives, \\(A\\) and \\(B\\), we either ordinally prefer \\(A\\) to \\(B\\), or \\(B\\) to \\(A\\) or are indifferent between \\(A\\) and \\(B\\). Transitivity \\[itm:transitivity\\] For every three alternatives \\(A\\), \\(B\\) and \\(C\\), where we prefer \\(A\\) over \\(B\\) and \\(B\\) over \\(C\\), we must prefer \\(A\\) over \\(C\\). Continuity \\[itm:continuity\\] For every three alternatives \\(A\\), \\(B\\) and \\(C\\), where we prefer \\(A\\) over \\(B\\) over \\(C\\), there is a lottery comprised of a known number of \\(A\\)-lots and \\(C\\)-lots between which and \\(B\\) we are indifferent. And, most controversially, Independence \\[itm:independence\\] If between two alternatives \\(A\\) and \\(B\\), we prefer \\(A\\), given a third option, \\(X\\), we still prefer \\(A\\) over \\(B\\). Such vnm-rational actors can be said to have well-formed, ordinal preferences over given alternatives, and can, by von Neumann and Morgenstern (1944)’s work, express equivalent, unique and cardinal preferences over these alternatives. That is, if an actor can rank alternatives vnm-rationally, she can also assign them a set of equivalent real numbers, such as willingness to pay. von Neumann and Morgenstern (1944)’s transformation of ordinal preferences into cardinal utility can be roughly imagined thus:24 An agent is offered many lotteries with known probabilities of the alternative outcomes in question. If the agent is vnm-rational, her ordinal preferences dictate her choice between any such known probabilities revealing her expected utility function, including those probabilities where she is indifferent between alternatives. Probabilities being cardinal, the agent’s choices between them reveal her cardinal utility from the alternatives. A vnm-rational agent’s ordinal preferences can thereby always be expressed in some cardinal utility function, and vice versa: if an agent has a cardinal utility function over ordinally preferred alternatives, she must be vnm-rational. Crucially, cardinal utility thus revealed need not be a linear function of some cardinal value: agents expected utility may differ from the expected value of some outcome, where its value is merely multiplied by its probability. Agents, can, for example be risk averse and display a correspondingly concave utility function.25 In the social sciences, expected utility theory — as its brethren the first theorem of welfare economics — often arouses great passions: it is either considered elegant and self-evident, or vulgar and misleading. Here too, I must briefly explicate its status: Expected utility theory is knowledge (p. ) of the first-order flowing from a epistemology (p. ). It implies no (first-order, empirical) observations of actual human decision making. 26 As other exercises in formal logic, it is not up to social scientific debate. Expected utility theory invites no intersubjective comparisons (such as those on which the first theorem of welfare economics rest). Any talk of aggregate expected utility is meaningless, absent additional assumptions. That said, expected utility theory can provide no justification for any particular aggregation or comparison of individual utility, but once such aggregation has been accomplished on other grounds (for example, by democratic rule), it can posit rationality for and guide decisions based on such aggregated preferences. Why now, readers may ask, does any of this matter? Expected utility theory matters a great deal for desirable hypotheticals in tax, welfare and democracy. In tax and welfare it suggests the very necessary and sufficient (!) conditions under which it even makes sense to speak of utility, or its manifold incarnations in (cardinal!) economics. Markets can work their hypothesized magic of pareto-optimization under the first theorem of welfare economics if, and only if people can express their preferences vnm-rationally. If people displayed, say, preferences between good \\(A\\) and \\(B\\) depended on a luxury good \\(C\\) becoming available in a market (Frank 2010), the whole intellectual artifice of “utility” and any pursuant neoliberal imperatives with it, may crumble. Tax and welfare may have to step in, to rectify the original irrationality. Conversely, to justify any of my desired welfare interventions into the market economy is to argue for enhanced expected utility: assuming (reasonably, I think), that people prefer an \\(A\\) of no insurance premium, livelong health, to a \\(B\\) of some insurance premium, cared-for sickness, to a \\(C\\) of some insurance premium, livelong health, to a \\(D\\) of no insurance premium, uncared-for sickness, but suggesting that, when they give the probabilities and their risk aversion some thought, people (should) find \\(B\\) to be of maximal expected utility, is to invoke von Neumann and Morgenstern (1944). The case is even clearer in democracy. On the one hand, vnm-rationality specifies what can be accepted as ordinal preference inputs (votes), if their bearers (citizens) are to have some consistent expected utility in the face of risky choice (strategic voting!). On the other hand, these vnm axioms must also be accepted by any democratic aggregation mechanism — even if, absent meaningful intersubjective comparisons, von Neumann and Morgenstern (1944) cannot justify any particular mode of aggregation. As I hypothesize later, popular tax choice under pluralism may well be plagued by irrational, inconsistent preferences of voters, and suboptimal choice in tax may, in part, be attributed to an aggregation mode that exploits these flawed preferences. Conversely, it appears that even assuming vnm-rational voters, aggregative democracy alone may not be able to produce minimally attractive decisions. Expected utility theory not only serves me to elucidate such a priori consequences, my normative conviction goes further and deeper. To speak of rationally maximizing utility, as I wish to do, is to imply von Neumann and Morgenstern (1944), at least for now. Without their axioms for rationality, we do not know what we are talking about when we say “utility”, and conversely, without their formulation of utility, we do not know what we are talking about when we say “rationality”. If the two are to mean anything, I suspect, they must be related in the terms that von Neumann and Morgenstern (1944) have worked out. 2.2.3 Pragmatic Utilitarianism Desirable hypotheticals in tax, welfare and democracy maximize outcomes that are valuable to people. Unfortunately, such a utilitarian axiom raises more questions than it answers, including: \\[itm:aggregation\\] How do we aggregate value over different people? \\[itm:utility\\] How can we know what people value? I cannot explore these questions in full, let alone answer them. Yet, to suggest desirable tax, welfare and democracy without raising these questions would be charlatanry. Without at least a tentative and pragmatic response to these questions, there can be no meaningful talk about their (p. ). Question \\[itm:aggregation\\] is easier to dismiss (if not answer), so I will deal with it first. 2.2.3.1 The Problem of Aggregation Utilitarianism runs into an empirical, or even ontological problem when it comes to aggregating value over different people: it posits an ideal observer capable of comparing value between people.27 Sadly (or not?) such an ideal observer is not readily available (Rawls 1988), and we do not even know whether such intersubjective comparison is ontologically possible. This is not to say that absent an ideal observer, we are off the hook. We should care about other people with whatever roundabout empathy we can muster, but, strictly speaking, we should not do it for utilitarian reasons. The last reasons for which we should care about the consequences of others are precisely not consequentialist, but may be, for instance, deontological (equality!), virtuous (charity!) or caring (empathy!). Utilitarianism cannot, as (Bentham 1789) might have hoped, reduce distributive justice to an empirical question, it can merely provide the cardinal language in which we can ask about taxation and welfare. Still, as I have argued above (), only (cardinal) utilitarianism can speak to the kind of aggregation that especially taxation and welfare require. What we must do then, is to problematize — not presume — ideal observation, as Rawls (1988) has demanded. In his Theory of Justice, Rawls (1971), too, has suggested the mode for such observation that I choose here, and discuss further (p. ). 2.2.3.2 The Problem of Utility Question \\[itm:utility\\] on utility is even thornier. Behavioral economics, decision science and related disciplines present empirical findings that question whether humans have any consistent sense of utility, let alone a cardinal one. This poses much harder questions for institutional design than the cognitive heuristics and biases, because from a utilitarian perspective there can be no such a thing as “flawed emotions”. Whatever people feel — no matter how inconsistent — are the last, and only consequences about which a utilitarian cares. Certainly to a utilitarian, flawed cognition can and should be mitigated to improve its consequences, but there are no last reasons to correct human emotion, precisely because these are the last reason.28 I cannot here discuss at length the empirical findings on human emotion, but illustrate the questions it raises drawing only on some of Kahneman (2011)’s findings in prospect theory. I suspect that related approaches will pose similar questions. In his recent summary, (Kahneman 2011) poses at least two empirical critiques of vnm-like human utility: \\[itm:inconsistence\\] Humans express their utility in inconsistent, non-vnm-preferences. For example, people are not just risk averse — as a convex utility function might explain —, they are also loss averse (Kahneman and Tversky 1979). From any given reference point, they seem to reap more hedonic gain from avoided losses, than from equally-sized missed gains. As people make choices, their reference points can shift, and with that, their preference over the original choice can even reverse: (Kahneman and Tversky 1979) find that when people have acquired mugs, they often accept only selling prices higher than their buying prices. All of this spells trouble for vnm-consistent preferences, and especially cardinal utility — or willingness to pay — on which orthodox economics relies: prospect theory “is in many ways the least satisfactory of those considered since it allows individual choice to depend upon the context in which the choices were made” (Grether and Plott 1979, 634). Such deviations from consistent utility may also affect judgments of fairness in real life labor markets, as Kahneman (2011 L5248) notes. If confirmed, these findings may become relevant to macroeconomic policy and hypotheticals in welfare and taxation, too. If, for example, a wage cut by any given amount causes more hedonic loss than a raise by the same, or even a higher amount creates hedonic gain, creative destruction (Schumpeter and Swedberg 1942) may not be a utility optimum, even if it nominally were a positive-sum gain.29 Instead, from a utilitarian perspective, a general bias to conservatism may be warranted. Similarly, taxation might have to be revamped to account for loss aversion. Speculatively, if people suffered inordinate hedonic losses when parting with market earnings, taxation might have to be less progressive than otherwise desirable. We might also have to time taxation so that it minimizes the apparent if not the real sense of loss. For instance, withholding taxes at the source rather than a comprehensive pit, or generally indirect rather than direct taxes may be loss-aversively utilitarian, even if they were conventionally inefficient. Interestingly, then, the equity implications of loss aversion are unclear. Clearly, however, the abstractions of orthodox economics, and with it, conventional desiderata for hypothetical tax and welfare, are incapable of adequately capturing prospect theory, or other non-vnm-utility. \\[itm:undefined\\] More generally — or equivalently? — people may not only be unable to express consistent preferences, but there may be no such thing as well-defined utility, or subjective well-being. Kahneman (2011, 7056), for one, describes three different operationalizations of subjective well-being:30 \\[itm:experienced\\] Experienced well-being, reported as hedonic states occur. \\[itm:remembered\\] Remembered well-being, reported after some interval of hedonic states. Hypothetical well-being, reported as the hedonic gain or loss ascribed to some non-outcome.31 If there were one such thing as subjective well-being, these three operationalizations — and especially \\[itm:experienced\\] and \\[itm:remembered\\], falsifiable within-subjects — should yield the same hedonometer. Trivially, remembered well-being should simply be the time-integral of experienced well-being. Alas, it is not, as people seem to ignore duration and instead remember some (peak-end) average (Kahneman 2011 K7056). What then, do we — and happiness researchers — mean, when we talk about subjective well-being? Clearly, none of the operationalizations alone can be satisfactory (pun intended) (see Kahneman 2011 K7056), but together, they do not add up to any one concept: they are just inconsistent. Utilitarianism, and with it, orthodox economics and consequentialist hypotheticals in welfare, tax and democracy face another empirical dead-end. 2.2.3.3 The Emptiness of Utilitarianism These twofold empirical dead-ends of utilitarianism pose not merely academic conundrums, they raise very fundamental problems in welfare and taxation. Orthodox economics — especially the welfare kind — and many of the concepts I take for granted here all revert back to an empirically untenable assumption of vnm-consistent, subjective consequences. To say that an economy grows is to say that the net subjective consequences of people are improving along their vnm-preferences. To say that an economy has a positive savings rate is to say people are foregoing part of their vnm-preferences now, and store them in some material (a house) or immaterial (a patent) form that will be vnm-preferable in the future. To a utilitarian economist — almost a pleonasm! — there is no objective value, other than vnm-ranked subjective preferences, handily reported as utility, or willingness to pay. 3233 There may then be no no uncontested, objectifiable basis to value human choices and activity, as orthodox economics and much of the following desiderata in welfare and taxation. Even if we subscribe to utilitarianism, we may not be able to reduce desirability to an unambigous empirical question.34 Rather, it appears, asking about preferences, or other subjective consequences, always begs more questions (confer Kahneman 2011), including about the kind of preference consistency, or the kind of subjective satisfaction — about all of which we must find some kind of agreement. Empirically, then, desirability cannot be reduced to a measurement of pre-social preferences, because these preferences, along with all that we may deduce from them, are only socially defined. Willy-nilly, first-order inquiries into the utility of some choice have a habit of turning into second-order questions on the social conditions under which said utility was defined, measured and expressed. What then, are we to make of this emptiness of utilitarianism? Should we just abandon it and go for other ethics, instead? Surely, some caution is warranted, and we should back up normative claims with other ethics, because if nothing else, utilitarianism runs into these empirical dead-ends.35 I maintain that no matter these contradictions, we should stick to (some) form of utilitarianism, because normatively as ontologically, the world demands of us risky choices, that to choose one alternative is to negate another, and that as individuals and as societies, we must be willing and able to meaningfully trade-off desired outcomes. The only way to do this at a scale, and under risk is to compute, as best we can, expected utilities. Welfare and taxation, especially, ontologically presume, as much as they seek to improve comparable and consistent self-reported consequences. If we are to make meaningful normative choices between factual and hypothetical welfare and tax, we must have a notion of utility. But — is that not cheating? After all, I am justifying hypothetical taxation, welfare and democracy with last reasons that I know to be unconvincing, but posit them anyway because without them, I cannot justify hypothetical (or any!) taxation, welfare and democracy. Admittedly, I am cheating, but I like to call it ethical pragmatism, instead. I accept that in utilitarianism, as elsewhere, our moral judgements may be imperfect and tentative, but, to progress to greater clarity, must rely on them anyway, for three reasons: Pragmatically, we must rely — in part — on utilitarian ethics because, for now, we are ontologically as empirically faced with a world that — in part — works according to it. If you want to get something (deontologically?) good done today, you will probably have to appeal to popular notions of utility, no matter how ill-conceived they might be as last reasons. Pragmatically, it is a safe bet that whatever we will deem good in the future, will, among other things, probably require not only negative entropy, but more specifically, stuff that people prefer, including shelter, food and clothing. Of course, we may be less certain about many other things that people ostensibly now prefer — luxury cars, computers carved out of solid aluminum, espresso machines — and end up producing (future) “utility”, that does not hold up on (future) ethical reflection. Still, we should better be safe with some attractive subjective consequences, rather than sorry without any such utility. Lastly, and also pragmatically, we can further — but not perfect — our ethical understanding of subjective consequences by problematizing, then improving those very social conditions under which we empirically seem to be forming our preferences. If utility is, in fact, not pre-socially given, we can meaningfully talk about it. Of course, that is cheating, too. By problematizing the social conditions of forming preferences, I have just reassigned a first-order question (utility) to second-order status, a move I otherwise despise. To do this in a normative ethic, is really just to rephrase the question: after all, to what, if not a utilitarian standard, do you hold the social condition of preference-formation? I allow myself this trick not because it resolves the confusion, but simply to organize my discussion of welfare, tax and democracy. Welfare and taxation, are — by definition — the realms in which the social conditions of preference formation are not problematized. Here, I ontologically assume, and normatively require preferences to be (largely) pre-social. Democracy, in turn, is the social condition under which we form preferences in intersubjective deliberation.36 Here, I ontologically assume, and normatively require preferences to be (largely) social. 2.2.4 Justice as Fairness Desirable hypotheticals in welfare, tax and democracy are just if they treat people fairly. Justice — especially distributive justice — deals with men in the plural as Arendt (1958) demanded of political theory, and regulates competing claims between people. As we have seen, some of the previous ethics tell us little about how to resolve conflicts between different bearers of rights, virtue, consequences or care, respectively. Deontological liberalism provides some digital rules, but these tend to be either minimal, or overly restrictive. Consequentialist utilitarianism promised to make aggregation an empirical matter, but cannot do so convincingly. Crucially, neither of these ethics suggests how conflicts over the meaning of, or conflicts between competing claims can be resolved. Here again, I turn to John Rawls (1971) to provide a meta-standard for finding desirable hypotheticals. In his influential “Theory of Justice”, he provides that standard: justice as fairness. Ever the liberal (and similar to above-mentioned Cohen 1989), he suggests no definite desiderata, but instead proposes a thought experiment under which moral claims have to qualify to be admissible for consideration. In his thought experiment, Rawls (1971) imagines an original position, where deliberators know nothing about their endowments, status and power in the real world. Only moral claims that can be defended under such a veil of ignorance, suggests Rawls (1971), may — but need not be — just. Oddly, this imagined original position is almost a liberal procedure for justice, or a normative claim of the second order — but not quite. The veil of ignorance, is, of course, materially impossible and operates only metaphorically. Justice as fairness, is, instead, an end-state theory of (distributive) justice (Fried 1999, 1007). Genially, Rawls has thereby crafted a standard of substantive justice, avoiding both the contingency of immediate imperatives (“\\[shellfish\\] shall be an abomination to you”, Leviticus 11: 8, KJV), the nihilism37 under procedural prescriptions (“Law to Rectify the Destitution of the People and the Empire”, Berlin 1933) and the vagueness of substantive universalisms (“life, liberty and the pursuit of happiness”, US Declaration of Independence 1776). If ever there can be a synthesis between natural and positive law, it must be similar to Rawls. Rawls (1971)‘Theory of Justice suits me, because as a liberal proposal, it lets me “economize on moral disagreement” (Gutmann and Thompson 2004 K226). Moreover, both Rawls’ original position and the distributive justice he deduces from it, align neatly with deliberative democracy and progressive taxation of (postpaid) consumption, as I argue in (p. ). 2.2.5 Meta trade-offs These are the axioms for desirable hypotheticals in tax, welfare and democracy. I hope they will garner wide-spread support. Admittedly, the aforementioned axioms are woefully unspecific to design the institutions of tax, welfare and democracy. For the time being, they must remain so. I develop them into domain-specific desiderata in later chapters. The aforementioned axioms have also not resolved all conflicts between initial, tentative desiderata nor have they resolved the contradictions between different ethics. I suggest two modes of resolving such conflicts. In part, I resolve these conflicts by hierarchy. Following Rawls (1971), desirable hypotheticals must be liberal and must maintain the most extensive basic liberties compatible with similar liberty for others. Such norms — as is typical for deontological ethics — are categorical: these liberties are either given, or not. Such categorical values may also conflict, as for example, freedom of speech and human dignity may, in cases of alleged defamation. In these cases, desirable hypotheticals are those arrangements that satisfy both freedom of speech and human dignity until and unless they conflict. Crucially, neither norm is superior and they cannot be continuously traded off one another: there is no amount of cardinally “more” free speech that would justify a cardinal loss in human dignity. Categorical values defy trade-offs. What we must look for, instead, are intersecting sets in a figurative Euler diagram, as in (p. ). Euler Diagram of Three Values All remaining values must be reconciled within this intersect of categorical values of liberalism. Here too, even supposedly outsized cardinal gains in any of the remaining values cannot be traded off nominal violations of categorical values. In part, I resolve these conflicts by offering trade-offs. This works only for values that are continuous in their realization, as may be the case for the tired conflict between equity and efficiency in taxation. I suggest that the trade-offs offered by such conflicting, continuous values depend crucially on the institutional context under which these trade-offs have to be engaged. Calling trade-offs, and designing the conditions thereof can be illustrated well in a diagram of ppfs, frequently used in economics to illustrate different possible baskets of goods that can be produced by an economy. Because paper as this is two-dimensional, ppf diagrams frequently only show two goods, but the abstraction carries to any number of \\(n\\) goods in a basket. I adapt the traditional ppf in (p. ). Production-Possibility Frontiers of Two Competing Values What are competing goods in a ppf diagram are here competing values \\(I\\) and \\(II\\). Let us assume for the sake of simplicity that these values can be easily measured and are ratio scaled. More of each value is better. The four ppfs are comprised of those possible combinations of values \\(I\\) and \\(II\\) which are furthest from the origin, and thereby strictly preferable over all possible policies under the ppf. As in a static model of the economy, the ppfs are exogenously determined by a priori, and posteriori limits of the first order. In addition, I argue, these exogenous limits are modified by institutions \\(1\\) through \\(4\\). Policies \\(A\\) through \\(J\\) are defined by specific combinations of continuous value \\(1\\) and \\(2\\), along the respective institutions which modify what is logically and empirically possible in this world. illustrates different policy choices. The simplest kind of trade-off is that between two policies along a linear ppf, as between \\(A\\) and \\(B\\) on \\(1\\). A straight ppf implies that the values are substituted at a constant rate: any increase in \\(I\\) will requires a decrease in \\(II\\) by the same amount. The choice between \\(A\\) and \\(B\\), as all other points on \\(1\\) is a zero-sum proposition. As we shall see, trade-offs in welfare, taxation and democracy are frequently, if implicitly, presented as such zero-sum choices. Alternatives of this sort are inevitable, but they are also normatively less interesting. Once values \\(I\\) and \\(II\\) are reduced to the same scale, the choice along the resultant ppf becomes trivial or even arbitrary.38 ppf \\(4\\) is curvilinear, and more interesting. Here, the rate of substitution varies over different levels of \\(I\\) or \\(II\\). For example, around \\(G\\), you have to give up relatively little in \\(I\\) to gain relatively much in \\(II\\). The reverse is true around \\(H\\), and substitution is roughly constant around \\(J\\). The trade-offs between \\(I\\) and \\(II\\) are non-zero-sum: you can gain more than you loose. Assuming a reasonable aggregate indifference curve (linear or convex), optimal policy will probably lie around \\(J\\). As we shall see, concave or other curvilinear ppfs abound in welfare, tax and democracy. Recognizing the convexity of trade-offs offered by any given institution, or, if possible, moving from lower, linear ppf \\(1\\) to a higher, convex ppf \\(4\\) will be important to identify desirable hypotheticals. ppfs \\(1\\) and \\(3\\) are both linear, but they have different slopes. At any level of \\(I\\), compared to ppf \\(1\\) you have to sacrifice more in \\(I\\) to increase \\(II\\). The moves along curve \\(3\\) are otherwise as normatively uninteresting as those along ppf \\(1\\) — the two are just scaled differently — but the choice between these two institutions will be very consequential. Compared to institution \\(1\\), institution \\(3\\) will always make it costlier to increase value \\(II\\). There are many such consequential choices of institutions in tax, welfare and democracy. ppfs \\(1\\) and \\(2\\) both have the same slope, but \\(2\\) is further from the origin. By definition, all policies along this higher curve are preferable to all policies on the lower curve — to everyone.39 This is the most important of institutional choices to be made. 2.3 The Ontology of the Doable Hypothetical But what is government itself, but the greatest of all reflections on human nature? — James Madison (1788, 143) Doable hypotheticals are preferable to others because, in Dahl (1989)’s words, “we must avoid comparing ideal oranges with actual apples” (1989, 84). Deciding just what can, and cannot be done is hard. If asked as a positive first-order question, the answer is sometimes empirically unclear. If turned into a second-order question — as the social sciences are prone to do — the answer becomes politically contested. Both extremes do not serve us well: When all inquiries into the social world are reduced to positive, first-order questions the social sciences effectively abolish themselves and make the status quo epistemologically endogenous, as in: social inequality is inevitable human nature, because if it were not, it would not be observed, or “the Laws of commerce are the laws of Nature, and therefore the laws of God.” (Burke (1790) as cited in Marx (1867, 1:834)). Conversely, when such inquiries assign all positive questions to second-order status, the social sciences hermetically seal themselves off from other disciplines and turn critique into futile, infinite regress, as in: “social being \\[...\\] determines \\[...\\] consciousness” (Marx 1859 Preface), including, one might add, Marxist consciousness. Instead, the social sciences — and especially policy analysis — should find a middle ground, taking on one second-order question at a time, while provisionally leaving all other questions to first-order status. I here ask second-order questions about the welfare state, taxation and democracy and I therefore assume these institutions to be malleable. Not under study here are (p. ) or the (p. ), especially of developed states and late capitalist economic production. I assume these to be constant in the medium run and reasonably approximated in the following sections. 2.3.1 Human Nature Any second-order prescription for how to organize production, distribution and decision making rests on assumptions about human nature. For evolutionary anthropology, psychology, behavioral economics and other offspring of now disreputable (Wright 1994) sociobiology (Wilson 1975) this is a positive first-order question about our prehistoric baggage: what behavioral, cognitive and emotional dispositions might have been adaptive in the environment of our evolution, and what do we observe today? 2.3.1.1 Evolution and Morality Maybe, evolution deserves to be the master ontology of life, and therefore, of human life, too. Because I sometimes refer to such Neo-Darwinian arguments (Wright 1994) and hear them often misunderstood I must reiterate the ontological status of the theory of evolution: Evolution may dispose us to think, feel and act in certain ways, but does not determine us to do so (deterministic fallacy). Evolution yields more (“gradual” according to Neo-Darwinism) or less (“punctuated” according Elrdredge and Gould (1972)) stable equilibria between environmental conditions and (more or less) adaptive traits of organisms. It does not necessarily yield optimal configurations, just survival of the relatively fitter. Conversely, not all biological features that are observed are necessarily adaptive, but may simply be side consequences of other, adaptive features or developmental vestiges (Gould and Lewontin 1979). Or, in Bryson (2003)’s succinct formulation, “life wants to be, but it doesn’t want to be much” (Bryson 2003). \\[itm:nonmoral\\] Evolution is a “nonmoral” process (Gould 1982). It may tend towards greater complexity and cooperation, including human-like intelligence (Wright 2000), or it may just pursue a random walk, departing from a “left wall” (Gould 1994, 4) of “simple beginnings” (Gould 1977, 7), and inevitably bring some complexity, including homo sapiens as a freak outlier (Gould 1996). Either way, even if evolution were directional, it would not be along any moral dimension intelligible to humans. This cuts both ways: to praise evolutionary results is to fall for a “naturalistic fallacy” (Moore (1903), as cited in Wright (2000 K5987)) to criticize them on any moral basis is to commit a “moralistic fallacy” (Davis 1978). Within these ontological limits, evolution serves me well in this dissertation, because as a materialist, positive and first-order perspective it allows me to limit my second-order inquiries on taxation and democracy. Unfortunately, evolutionary logic has a way of straying beyond the positive, of encroaching on and negating normative questions, precisely because it purports to explain all life, including human life. For example, if exclusive fitness were operative in human evolution, does that not, as Social Darwinism suggested, imply that humans are unequal, end of story? More fundamentally — and less obviously ideological — if all our flesh, including our brain tissue, evolved in an aimless process, does that not, as overzealous neuroscientists like to test, imply that whichever subjective experiences of consciousness and free will that flesh reports must be wholly illusory, and all questions of morality therefore beside the point? If we were merely organic, delusional robots, as Wright (2000 Chapter 23) provocatively asks, what would be wrong with unplugging a few? This pedestrian thesis is not the place for another mind-body debate, and I am equally awed and outmatched by these and other “hard problem(s) of consciousness” (Chalmers 1995). Still, I must ask the reader to allow me a crude bit of lay metaphysics. For starters, positive questions on consciousness — including the downstream issue of free will — easily run into seemingly obvious logical problems. If consciousness were positively illusory, who would be left to do the observing? Conversely, if consciousness were some positive emergent neuronal phenomenon, would not precisely that physical genesis negate subjective experience? Either way, infinite regress ensues. Perhaps, as James Trefil (1997) muses, consciousness “is the only major question in the sciences that we don’t even know how to ask” (1997, 15) and maybe, therefore, we need not be bothered, for now. More fundamentally, whatever we may learn about the (aptly named) “neural correlates of consciousness” (for example, Koch (2004), emphasis added) none of this positive research can, or ever should negate, or infringe on normative questions. The first-order positive questions of natural science, and the first-order normative questions of the social sciences should be kept neatly apart, because they are noma (Gould 1997). Akin to science and religion, first-order positive and normative questions, too are distinct “domain(s) where one form of teaching holds the appropriate tools for meaningful discourse and resolution” (Gould 1999, 3): The magisterium of science covers the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry (consider, for example, the magisterium of art and the meaning of beauty). — Steven Jay Gould (1999, 6) No matter then, how hard-nosed a question we may ask about our evolved nature, these positive inquires must never be mistaken for, or negate normative questions, because these fall into categorically different realms: Our failure to discern a universal good does not record any lack of insight or ingenuity, but merely demonstrates that nature contains no moral messages framed in human terms. Morality is a subject for philosophers, theologians, students of the humanities, indeed for all thinking people. The answers will not be read passively from nature: they do not, and cannot, arise from the data of science. The factual state of the world does not teach us how we, with our powers for good and evil, should alter or preserve it in the most ethical manner. — Stephen Jay Gould (1982, 43) Ought may imply can (Kant 1794, 65), but they are still not the same thing. However much constricted positive science may find us to be, these limits of what we can do must never drown out the imperative ought. Perhaps, tautologically, to be human and not another animal, is to arrogantly insist that, in spite of the limited and banal flesh we are made off, we ought to be conscious, we ought to have free will and we ought to say “ought”. Maybe, this is the this-wordly obedience to Jesus Christ that Dietrich Bonhoeffer meant — and died for — when, faced with fascism he chose “costly discipleship” (Bonhoeffer 1937): “Only he who shouts for the jews is permitted to sing Gregorian chants” (Bonhoeffer 1933 as cited and translated in de Gruchy (1999, 35)). Fascism, after all, was the modern ideology to radically negate any “ought” for whichever group (“race”!) was supposedly biologically superior, and de-facto militarily stronger. Perhaps, such rejection of “ought”, too, makes the “banality of evil” that Hannah (Arendt 1963) recognized in an Adolf Eichmann on the stand in Jerusalem. Eichmann, after all, might “excuse\\[...\\] himself on the ground that he acted not as a man but as a mere functionary \\[...\\] since after all, someone had to do it” (Arendt 1963 K286f.). Such acts of state defy ought, because, to an “unthinking” Eichmann (Arendt 1963 K187f.), history — as evolution — just is. Such lay metaphysics might be crude, but they are not entirely beside the point of welfare, taxation and democracy. As I argue later, a milder, but similar conflation of can and ought, of second-order positive, and first- and second-order normative questions plagues some social science, especially when it (, p. ). In the social sciences, too, the line between an emerged, evolved, “grown order” and a “made order” is sometimes blurred, negating political oughts — and not always as elegantly and explicitly as by (???). 2.3.1.2 Evolution and Institutions We need not confine evolutionary explanations to our biology alone, or, conversely, reduce all behavior, cognition and emotion to some strictly physical (genetic) reason. Instead, we can apply evolutionary explanations to culture and institutions, too. In fact, the tired — and often unproductive — nature vs. nurture controversy is moot: we are neither blank slate40 for behaviorism to condition or society to write on, nor a physically determined animal, but essentially both nature and nurture. Our bodies and culture-ready brains genetically co-evolved with co-adaptive memes (Dawkins 1976) to make us the “hypercultural species” that we are (Henrich and Henrich 2007 K175). Relatively ill-equipped in instincts, we need to learn (or imitate) what to eat and hunt — and how to build a blast furnace. Perplexingly, it is in our nature to rely on culture: as our brain allowed us to learn easily, our culture developed cooking, and our digestive tract adapted to broken-down proteins, as illustrated in (p. , Henrich and Henrich (2007)). The coevolution of nature and culture, with some examples \\[fig:coevolution-nature-culture\\] This is the evolutionary blockbuster of humans: we moved the locus of our evolutionary adaptation from genes to memes (Dawkins 1976). Instead of “hard-coding” all adaptive traits, we learn (or imitate) the more complex and more malleable software of culture (Boyd and Richerson (1985), Henrich and Henrich (2007 K196ff)), written not in dna, but coagulated into institutions. Paradoxically, as the naturally hypercultural species, we might gain some leeway from our biology, but simultaneously loose it to culture, because evolved culture too, must be (co-)adaptive to our biology, environment and, crucially, our past, path dependent culture (evolutionary anthropology, for example, Wright (2000)). As sociologists like to say about institutions — the coagulates of culture — culture, too, may enable us as it constrains us (for example, Hodgson 2006, 3). Here lurks another positive first-order dead end: if present culture is, by definition, sufficiently (co-)adaptive to persist, does that not imply that any new, improved culture could and would emerge by said evolutionary process if, and only if, it were (co-)adaptive to our nature and environment and sufficiently incremental from past culture? If, therefore, culture and institutions, too, were strictly positive phenomena unamenable to human will, would the social sciences and political critique not be completely beside the point? Hardly so. Here, as in all evolutionary arguments, the can need not, and must not eclipse the ought. We may not be able to make arbitrary institutions, but, axiomatically, we can build progressive institutions: they help us achieve normative ends, by first responding to our evolved cultural and natural dispositions, then transcending them. At least since Enlightenment dawned, we get to make our own history (a little), and build or break (some) institutions as an act of will — that is (p. ). To build institutions is to emancipate ourselves from the meaningless process that bore us as expendable containers of “selfish genes” (Dawkins 1976) and to heed that call for morality, that we among the earth’s animals may hear clearest, because through institutions, we can turn reflexive on our innate behavior, cognition and emotions. 2.3.2 The Modern Condition Maybe this is as good a definition of modernity as any: the project by which we reflect on what might be innate to our existence and inevitable about our physical world, and deliberately build institutions accordingly. Thus advises a machiavellian (1532) Mandeville (1714) to turn “private vices by dextrous management of a skillful politician (…) into public benefits” (1714, 213), thus threatens a somber (Hobbes 1651) our inner wolves with an even more powerful metaphorical beast, and thus asks us a moralizing (Smith 1776) to receive from others “from their regard to their own interest” — or any number of other “idea\\[s\\] of the world as open to transformation, by human intervention” (Giddens 1998, 94). Modernity, as evolution, is often misused or misunderstood, and so again, I must briefly clarify its ontological status for my purposes: The modern project need not be confined to a specific location or era, nor need it be one-directional or linear, but it might arise in different places at different times, it might wax and wane, explode and collapse. While Europe and North America in the early 21st century may constitute high points in this project, the modern project is not wedded to any specific historical context or even the institutions which it bore, but rather, modernity is a cultural and cognitive phenomenon (Jones 2003, 26, emphasis added). The modern project need not be inherently good nor necessarily lead to desirable outcomes: modernity may well bear alienation (Adorno 1966), or even lead to catastrophe (Bauman 1989) and not all of its (ongoing) advances may yield progress. However, the inverse is true: to think progress implies a modern mindset, and to get it done requires modern institutions. It should be redundant to say that my dissertation ontologically assumes a modern condition, because tautologically, sociology is about finding answers to the questions modernity raises (Harriss 2000, 325), and “postmodern sociology \\[therefore\\] an impossibility” (Miles 2001, 169). And yet, “modernity” seems to be a concept fallen out of fashion, superseded by (occasionally loose) talk of various post-isms (for example, Lyotard (1984), Baudrillard) or second and reflexive modernities (for example, Beck, Bonss, and Lau 2003). Whatever the value of these and other contributions, in welfare, taxation and democracy I find no contradictions that would require such categorical labels or the nascent theories they stand for. In fact, I suspect that a thoroughly modern analysis of welfare, taxation and democracy may resolve many of those contradictions which reflexive modernity prematurely identifies, and clarify some of the uncertainties in which postmodernity revels. For instance, maybe, equipped with a rational and deep understanding of the mixed economy and externalities, we find that “de-nationalization” (Beck and Grande 2007) and “risk society” (Beck 1992) are neither inevitable, nor particularly descriptive but simply avoidable and unsurprising cooperation problems, instead. I here reflect on many dysfunctions of the presently observed, undoubtedly modern institutions of welfare, taxation and democracy, but I need not get “all meta” about modernity, because precisely such “reflexivity is a modern, not postmodern set of attitudes and practices and that ritualized postmodern caviling against modernity is ahistorical and inaccurate, not to mention dispiriting” (Sica 1997, 1119). Also, as I promised earlier, this is a pedestrian thesis. What then, are these indispensable modern institutions that I here assume as ontological entities, that which defines the “doable”? 2.3.2.1 States and Markets For the production and distribution of goods, they are state plan (but not nation state) and market exchange (but not any particular x-capitalism). For collective decision making, they are a state equipped with an effective monopoly on the use of force and devoted to some popular participation (compare Giddens 1998, 96). These two, states and markets, both brought and constitute the modern condition: they hyper-charged functional differentiation (Smith 1776) to reach near-planetary breadth (for example, international trade, citizenship) and near-universal depth (for example, commodification, family law), they altered or replaced much of the evolved culture (for example, kin) and institutions (for example, tribe) adaptively fit only at smaller scale (Diamond 1997), replaced such mechanical by organic solidarity (Durkheim 1893) and bore a social world that is complexly interacting (for example, Merton 1968, 1936). These two are also the meta-institutions that both define and allow welfare, taxation and democracy. Welfare and taxation exist only, as I explain later, in mixed economies of both market exchange and state plan. Taxation and democracy alike, to become real, must be enforced by a state with an effective monopoly on the use of force. As all modern institutions, state and market are based on a particular model of human nature, in this case, a rational and individual utility-maximizing homo economicus (maybe first in Mill 1848). The state gets an amoral, but opportunistic homini lupus to behave by threatening monopolistic force (as in Hobbes 1651). The market, in turn, lures a self-seeking, but rational “butcher, \\[...\\] brewer, or \\[...\\] baker” into providing “our dinner” — and all else — “from their regard to their own interest” (Smith 1776). 2.3.2.2 Non-Zero-Sumness But, as other modern institutions, state and market not only assume this human nature, they also allow homo economicus to transcend her own limits. As individual utility-maximizers, she runs into problems whenever her supposed selfishness prevents otherwise mutually advantageous interactions. This rough typology specifies the class of games which homo economicus is ill-equipped to solve: Zero-sum games display constant sums of payoffs over all cells, or possible interactions. One player’s gains equal another player’s losses: the cake does not grow, but is only sliced up differently. By definition, a zero-sum game is pareto optimal: no one can be made better off, without making someone else worse off.41 Terms of trade, the ratio of units of goods that a country has to export for any unit of imports, is an oft-cited (rare) example of zero-sumness in advanced economies. A trivial example of zero-sumness, theft, (without spillovers) is illustrated in : all cells yield the same \\(\\sum{\\text{payoffs}}=2\\). Non-zero-sum games have different sums of payoffs, or possible interactions. One players gains are not equal to another players losses: the cake can grow or shrink. Positive-sum games are equivalent to negative-sum games, because the absence of a loss (a negative-sum outcome) is a gain (a positive-sum outcome). For the sake of simplicity, I often refer to only gains or positive-sum outcomes, but mean all non-zero-sum games. Crucially, non-zero-sumness does not imply that gains are shared equally, or at all. Non-zero-sum games need not even imply (weak) pareto-improvements, if one players looses less than another gains in a Kaldor-Hicks-improvement (Kaldor 1939; Hicks 1939). Non-zero-sum games further break down into: Games of total harmony are situations in which self-interested players will (???) at an interaction that reaps all non-zero-sumness.42 By definition, a game of total harmony offers pareto improvements: at least one player can be made better off, (at least) without making someone else worse off. Under some assumptions, opening up to free trade famously is such a game of total harmony (Ricardo 1817). It is roughly illustrated in : at \\(\\sum{\\text{payoffs}}=6\\), the north-western cell yields the highest, social optimum and it is also the only (!) set of mutually best responses, and thereby, the unique Nash equilibrium. In short: trade and other games of perfect harmony should take care of themselves. Cooperation problems plague non-zero-sum situations where the social welfare optimum is not also (at least) a Nash equilibrium. Here, at least one player may gain more than all others stand to loose. Such games cannot be Pareto-, but only Kaldor-Hicks improved. The canonical example of a cooperation problem, a pd, is given in : here the socially optimal (smallest) payoff \\(\\sum{\\text{payoffs}}=2\\) prison years its in the north-western cell, but it does not constitute a mutually best response. In fact, no matter what the other player does, one is always better off betraying: the south-eastern cell is not only a mutually best response and thereby the Nash equilibrium, but even a mutually dominant strategy at a dismal aggregate prison sentence of \\(\\sum{\\text{payoffs}}=6\\) years. The pd and related cooperation problems can model many of the economic and political challenges facing us today, including global warming (for example, Stern 2006) and herding in financial markets (from Keynes (1936) to Banerjee (1992)). I also identify pd-style problems in welfare, taxation and democracy. Theft as a Zero-Sum Game n{2}{c}{} Does not steal Steals n{1}{c}{ ltirow{4}{* }{}} n{1}{|r|}{1 } n{1}{r|}{2} n{1}{c}{} n{1}{c}{} n{1}{|l|}{1 } n{1}{l|}{0} n{1}{c}{} n{1}{|r|}{0 } n{1}{r|}{1} n{1}{c}{} n{1}{c}{} n{1}{|l|}{2 } n{1}{l|}{1} Ricardian Trade as a Game of Perfect Harmony n{2}{c}{} Trade Autarky n{1}{c}{ ltirow{4}{* }{}} n{1}{|r|}{2 } n{1}{r|}{1} n{1}{c}{} n{1}{c}{} n{1}{|l|}{4 } n{1}{l|}{3} n{1}{c}{} n{1}{|r|}{1 } n{1}{r|}{1} n{1}{c}{} n{1}{c}{} n{1}{|l|}{1 } n{1}{l|}{1} The Prisoners’ Dilemma as a Cooperation Problem n{2}{c}{} Stays Silent Betrays n{1}{c}{ ltirow{4}{* }{}} n{1}{|r|}{1 } n{1}{r|}{0} n{1}{c}{} n{1}{c}{} n{1}{|l|}{1 } n{1}{l|}{2} n{1}{c}{} n{1}{|r|}{2 } n{1}{r|}{3} n{1}{c}{} n{1}{c}{} n{1}{|l|}{0 } n{1}{l|}{3} If we are to reap the (great) benefits of non-zero-sumness, homo economicus must be able to successfully play these classes of games. She should do well in games of perfect harmony that cater to her selfish utility maximization, but might flounder cooperation problems. As a maximizer of individual utility, she cannot see the bigger picture of aggregate welfare. Such is, for example, the logic of a (???) war of everyone against everyone: all would be better off if they ceased fighting, but in a world where everyone is armed and angry, any individual homo economicus better not be a pacifist. In fact, security may be modeled as a pd, as in . Security as a Prisoners’ Dilemma n{2}{c}{} …ovis …lupus n{1}{c}{ ltirow{4}{* }{}} n{1}{|r|}{2 } n{1}{r|}{3} n{1}{c}{} n{1}{c}{} n{1}{|l|}{2 } n{1}{l|}{0} n{1}{c}{} n{1}{|r|}{0 } n{1}{r|}{1} n{1}{c}{} n{1}{c}{} n{1}{|l|}{3 } n{1}{l|}{1} In a more realistic model with many players instead of just two, the cooperation problem may only exacerbate. If you were the only wolf amongst many sheep, violence may be especially cheap, because your potential victims are plenty and helpless. Conversely, being a lone sheep amongst many wolves will be especially risky for the sheep, and costly for the wolves who need to be competitively violent. At such a larger scale, the pd of security evolves into an arms race with ever costlier, but fruitless violence: the original tragedy of the commons (Hardin 1968). Naturally “stuck in-between” (Lehrer 2012) the selfishness of homo economicus and our capacity for altruism,43 as ever the “hypercultural species” (Henrich and Henrich 2007 K175), cooperative exploitation of non-zero-sumness, as much else, does not come to us robotically, or by instinct as it does to the eusocial insects (Wilson 2012). In humans, such cooperation is contingent on culture or institutions. 2.3.2.3 The Genesis of Cooperation Luckily for us, history listened to Hobbes (1651) and brought such an institution for large-scale cooperation, when it bore those powerful Leviathans, whom we have since successfully trained into rule-of-law, and even democratic states. Two theories of the genesis of states are instructive here: Maybe, states are simply the largest remaining of pre-historic, atomistic racketeers who threatened with well as protected from violence (Tilly 1985, 182). These initially small-scale racketeers morphed into proto-states and then states as new technology, and — equivalently — economies of scale allowed them to produce and distribute violence at a larger scale, for lower cost (Tilly 1985). Or maybe, (nation!) states grew out of kinship ties and our capacity for genetic nepotism (Hamilton 1964; Axelrod and Hamilton 1981), as the initially real blood lines became ever thinner and eventually illusory and fake, yet effective (van Den Berghe 1981; Gellner 1983). The two stories need not be mutually exclusive, but may instead complement one another. After all, Tilly (1985)’s materialist account of state genesis does not so much explain away the cooperation problem, but reverts it to the level of individual thugs who make up the racketeering mafia. As Coppola’s “Godfather” trilogy illustrates (1972, 1974, 1990), even in the 20th century, successful organized crime is difficult, and hinges on trust. Anecdotally, in the case of the Corleone organization, cooperation is built on family ties, alluding to theories of inclusive fitness as nepotism. Maybe, economies of scale and first genetic, later imagined nepotism both drove state making, only at different levels. Against this backdrop, the development of governance in the modern era, including sequentially security, the rule of law, democracy and welfare (compare Marshall 1950) is a project of both consolidating, and steering a powerful but dangerous monopoly on violence or illusion of kinship, respectively. Absent such training, states are quite scary beasts, as Hobbes (1651)’s frontispiece of The Leviathan illustrates. Both theories of their genesis imply a slippery slope. Supercharged by economies of scale in the production of violence, those monopoly producers of violence may tend towards parasitic government. “Imagined communities” (Anderson 1983), in turn, enabling cooperation by increasingly fabricated nepotistic sentiment, may always risk turning exclusive, or even fascist if the quasi-biological notions run loose. Here again, the modern age has borne deliberate (meta?-)meta-institutions to govern an evolved structure, including constitutions, rule of law and democracy. However, all these modern additions to statehood should not distract from its defining innovation: an effective monopoly on the use of force, without which we are back to a very, very dismal square one. “You cannot get to Jefferson and Madison without going through Thomas Hobbes” — in Iraq (Diamond 2004), or elsewhere. Modern markets, and the intricate games of perfect harmony they provide for homo economicus to solve and thrive, too, depend on this monopoly of force. From property rights, to contract law and fiat money, commerce always rests on effective enforcement: to enter any such deals in the first place, you must believe that really, pacta sunt servanda, that promises will be kept — or made to be kept, anyway. It is, in fact, this shadow of violence that enables markets in the first place, that allows us to transform cooperation problems into games of perfect harmony, and thereby, to solve them. States and markets are, the atomistic reading of Hobbes (1651) and Smith (1776) aside, a feat of great cooperation. Almost magically, they freed us off that (???) curse, by which our geometric population growth must always outstrip our arithmetric economic growth, and starve us.44 The modern escape from that iron law of population (or resources, or carbon, and so on) was, is and will be to make economic growth above-linear, too: to reap non-zero-sumness, to functionally differentiate and to harvest economies of scale. If at different altitudes of abstraction, these all imply the same prescription: cooperation is our one ticket out of hardship and subsistence (a notion to which I return in the in , p. ). 2.3.3 Contingent Homo Economicus To think about welfare, taxation and democracy, then, homo economicus is both inescapable and inadequate. 2.3.3.1 Inescapable Homo Economicus As an ontological model, homo economicus is inescapable, because it comes part and parcel with the meta-institutions of state and markets. To think of coercive power or supply and demand is to invoke an individual utility-maximizer. By extension, to ponder welfare, taxation and democracy, too, implies this view of human nature, for if we were neither individual nor utility maximizers, altruism, allocation and decision making would come to us automatically. Alas, it does not. 2.3.3.1.1 Materially Possible. Moreover, market and state, along with their view of human nature are good ontologies for a second-order inquiry into welfare, tax and democracy because market and state, if nothing else, are materially possible under first-order theory. Market and the state may not be the only materially possible means to organize cooperation, but they are the only (meta-)institutions that have demonstrably orchestrated large-scale production and distribution of many kinds of goods. They, and the (p. ) they tap into are the only known way to prosperity. For now, only states can solve commons problems (Hardin 1968 for example,), and only markets efficiently and credibly gather, process and signal dispersed information (Hayek 1931). Other institutions to facilitate cooperation, such as kinship (van Den Berghe 1981; Axelrod and Hammond 2006) or even the nuclear family (on which the conservative/continental welfare state still relies heavily, according to Esping-Andersen (1990)) and community (Ostrom 1990) are often narrow in scope and reach. Self-organizing scientists (for example, The Human Genome Project), programmers (for example, Linux OS) and web-users (for example, Wikipedia) have lately accomplished impressive achievements, but their mode of production seems to complement state and market, rather than replace it: scientists are often paid state salaries, free software runs on commercial hardware, and wikipedians need day jobs. These goods, incidentally, are also all common or public goods, the non-state production of which we are only just beginning to understand (Ostrom 1990). Eventually, great hopes set in volunteerism, a communal “governing the commons” (Ostrom 1990) or some other alternative mode of production, distribution and decision-making may come to fruition. In the meantime, we must stick to at least a bit of state and market, the two meta-institutions which, empirically, have civilized whichever aspects of homo economicus we might positively harbor into the intricately complex interdependency of the modern world. This interdependency is, as I have explained, the very condition of our prosperity. Modernity and its riches, we can only hope, are here to stay (Diamond 2005) and any first-order ontology, must — as states and markets do — abide by its conditions. The modern economy is, and needs to be so functionally differentiated that no subset of people can ever organize, let alone meet all their material needs in isolation. Only elegant abstractions, such as global price systems, can enable this feat of cooperation. Likewise, modern society is so complex that no large share of society can ever comprehend, let alone decide on all matters of the polity. Only some people, some of the time, can comprehend in some detail and decide on some matters. In modernity, autarky is always regress.45 Small-scale, intimate interactions, such as of the ancient polis or the prehistoric tribe will not, and should not return. Any ontology or policy that purports a return to such simpler times does not, as it might claim, provide an alternative to state or market, but instead merely defines away the question which these meta-institutions have answered and threatens to roll back modernity. 2.3.3.1.2 Logically Consistent. Homo economicus and the meta-institutions it inspired also make for a good ontology for hypotheticals, because whatever its shortcomings, at least, we have a logically consistent understanding of states and markets, and know something about how they work, and how they fail. If you assume some homo economicus in us, an appeasing Leviathan, and the pareto-improving qualities of free markets, are, in fact, logically consistent. These abstractions ride on a lot of (heroic) assumptions, but at least, they clarify our thinking and generate falsifiable hypothesis. That is more than can be said about suggested remedial institutions such as “governance”, “Big Society” (Cameron 2011) or a philanthropic “Third Sector” (Anheier 2002). These, as many other recent contenders of states and markets, are shrouded in impenetrable (“problem solving”, “community” and “giving back”, respectively) (p. ), they lack a coherent (any?) model of human nature, and give no account of their successes and failures. Civil society, in particular, is yet only negatively defined (it is not the state, not the market and not the family), its mode of production (volunteerism?) is underspecified and its vaguely optimistic ignorance of structure and material interest border on (hegemonial?) ideology. Here, too, defining away the contradictions of modernity will not solve them, it will merely render the advocated institutions inaccessible to critique and improvement. 2.3.3.2 Inadequate Homo Economicus Yet, homo economicus is the kind of quasi-evolutionary concept that blurs theory and data, and easily eclipses ought with is. Less metaphysically, it is also plainly inadequate to investigate welfare, taxation and democracy because these projects, as states and markets, more generally, are, and always have been plagued by precisely these selfish demons of our nature. 2.3.3.2.1 Logically Incomplete. Homo economicus is logically incomplete as a first-order ontology, because it cannot explain the initial nucleus of cooperation from which states and markets must have sprung. The infinite regress of an ontology inhabited only by homo economicus is evident in the competing (or complementing) theories of state genesis referenced earlier. Tilly (1985)’s (and similar) stories of state-making-as-organized-crime do not so much explain Leviathan-level cooperation, as they merely relegate it to the level of nascent racketeers: just how these thugs initially ganged up, we do not know. van Den Berghe (1981)’s (and related) stories of polities-as-extended-kinship, likewise, do not so much explain Imagined Communities, as they simply relegate it to a sociobiological explanation of reciprocal altruism: just how reciprocal altruism emerged at a group level, in an evolution of individual-borne genes and memes, we do not know, and even sociobiology is unsure.46 Whatever their genesis, states, markets and modernity may well need such nuclei of cooperation to sustain themselves, even today. In fact, much of their current crises might be described as the inevitable wreckage of pure homo economicus. This dissertation, too, chronicles the limits of homo economicus as much as it ontologically rests on this view of human nature. 2.3.3.2.2 Empirically Incomplete. Luckily for us, this rational, individual utility-maximizing model of human nature may also be incomplete — but not entirely incorrect — on all counts: humans may be hard-wired altruists (for example, Zak, Kurzban, and Matzner 2004), are only boundedly rational (Simon 1999; Kahneman 2011), poor planners of utility (summarized in Gilbert 2006), think in relative, not absolute terms (Frank 2005a) and display diminishing marginal utility (Ng 1997; Veenhoven 2000; Nickell, Layard, and Mayraz 2008). Homo economicus provides, in other words, at least an incomplete description of human nature. 2.3.3.2.3 Domain-Specific Homo Economicus. Nevertheless, and because homo economicus is both inescapable and inadequate to investigate welfare, taxation and democracy, we should make this model of human nature and its related (meta-)institutions domain specific. Even if, as I suspect, there are no alternatives to state or market in some realms of modern society, we need not “economicize”, commodify or regulate all aspects of life. Instead, different tasks call for different modes of production, distribution, decision making and associated views of human nature, as summarized in . At the economy or industry level, markets may be our best bet, but perhaps not at the firm or team level, where we can tap into other human motivations. Similarly, a state may need to impose health and safety standards, but not a teacher’s lesson plan (as Schwartz and Sharpe (2010) alarmingly report). Market and state, along with their impoverished view of human nature, can be applied selectively, to some domains. As alternatives become available,47 market and state can recede.48 For the time being, I suspect, the domains exclusive to market and state will remain considerable. In this dissertation, I constrain homo economicus to the realm of the market, and summarize welfare and taxation policies to remedy, as well as make do with its shortcomings. In democracy, by contrast, I find a realm in which homo economicus can no longer be accommodated, but where we must instead find new (deliberative) institutions to tap into other, better angels of our nature. 2.3.3.3 No New Man Neither policy nor social scientific ontologies can ask for a new man. For better (see above) or for worse (for example, Schwartz and Sharpe 2010)49 state and market made their own humans: as we live under hierarchy and competition, we adapt to it. In the oecd-world and in our time, many people (including me) will expect and respond to incentives and regulation. Consequently, political institutions have to reckon with homo economicus, even if — and because — it is partly of their own making. Good policy and good social scientific ontology does not ask for a new man, but makes do with the women and men we have now, but at the same time, recognizes that for our “hypercultural” species (Henrich 2003), homo economicus, as other views on our indeterminate nature will always be highly contingent on institutions. None of this is to preclude a study or reform of human nature, modern society, states or markets. Rather, such a contingent ontology is, to me, the only way to study these meta-institutions and their incarnations in welfare, taxation and democracy. Some, ontologically contingent homo economicus can clear our hazy eyes, but too much, ontologically endogenous homo economicus will blind our “sociological imagination” (Mills 1959). Contingency and domain-specificity lead the middle way between the hermetic ideology of utopias, and the apolitical mindlessness of TINAs. Also, none of this is to proclaim state and market as the “end of history” or to blazon homo economicus as “the last man” (Fukuyama 1992). Rather, state and market along with their homo economicus are the only place to start from, if we want to write, or make history in welfare, taxation and democracy today. Precisely because our nature is contingent on institutions, that is where we must start: institutions are all we got, “just because institutions are the kinds of things that can can be changed directly, whereas cultures and psychological dispositions are less subject to collective intervention and experimentation” (Warren and Pearse 2008 K182). States and markets are those prime meta-institutions through which we can make, and remake the institutions of welfare, taxation and democracy. Through them, we respond to the homo economicus in all of us, and unfold our greater capacities. And transcend the limits of homo economicus, we must. Clearly, we face grave problems in (, p. ). Clearly, too, disintegration is not an option: that way lies regress and hardship. And clearly, further rational-functional or identity-embellished integration is not an option: that way lie democratic deficits, and exclusionary identity politics. These are the challenges cut out for us. To fail them, to fail these cooperation problems of non-zero-sumness, is to fail specifically as a human being, the one “conspicuously exceptional” species (Frank 2011, 85) that is socially capable of great cooperation, but not biologically determined to give up individualism (Wright 2000), and is therefore reliant on institutions. Welfare, and especially taxation and democracy are those modern institutions, to square the circles of individuality, inequality and cooperation. And these are the institutions I here assume to be malleable, to make up the desirable and doable hypotheticals, whose absence begs explanation. These, too, are the institutions that moderate the very contingency of homo economicus, and our other natures. As such, they are precisely the place to look at, taking on one second-order question at a time, leaving all else to first-order status. For sociology, “the science of institutions, their genesis and their functioning” (Durkheim 1895, 45), this, I would hope, is a good approach as any to learn — as we must — whether, and under which (institutional!) circumstances humans are “knights, knaves, pawns or queens” (LeGrand 2003), what our capacity for altruism is (Henrich and Henrich 2007) and how it can be fostered (Axelrod and Hamilton 1981), all to imagine, and then fulfill our human capacity of non-zero-sumness (Wright 2000). References "],
["mixed.html", "Chapter 3 The Mixed Economy", " Chapter 3 The Mixed Economy The devil may indeed dwell in the details, but we first need to find an angel or two in the abstractions that govern […]. — Edward J. McCaffery (2002 K117) Welfare states are governed by the economic abstractions arising from the co-existence of market and command. When you organize production and distribution of goods and services by market and by command, as welfare states do, the two contradictory systems “complexly interact” (Perrow 1999) and easily produce “unintended consequences” (Merton 1936) as for example, when (). To this day, the mixed economy of Postwar Western Europe, in its idealized form, is the closest thing to an angel to ever emerge from this uneasy coexistence between market and plan. To understand first-order desiderata of welfare state design, we need to understand the conceptual compromise of the mixed economy. Let me first reiterate the logic of its two constituent systems. 3.0.0.1 Market vs. Planned Economy. \\[sec:market-vs-command\\] We can organize economic production and distribution in at least one of two ways, by centralized, coercive command or by decentralized, voluntary exchange.50 In an ideal-typical (Weber 1920) command economy, whoever wields an effective monopoly on the use of force also directs the economy. A worker constructs, say, a railroad (production) and is fed by a farmer (distribution), both in fear of — however indirect — bodily harm from the monopolist of violence.51 In an ideal-typical market economy,52 violence is threatened only to maintain property rights and enforce contracts. People freely exchange goods and services at equilibrium prices that balance the costs to the producer and the utility to the consumer (, p. ). A worker constructs a railroad (labor) in return for an enforceable promise to consume (property) a given amount (wage), which he then redeems in a similar exchange with a farmer for food. 3.0.0.2 Capitalist Welfare State is a Pleonasm. \\[sec:interface\\] Welfare states combine elements of command and market in the service of equity.53 Specifically, welfare states coercively adjust the distributional outcomes of markets.5455 Welfare states insure their citizens against certain individual risks (disability, sickness, unemployment), fight “poverty” by instituting (unconditional or means-tested) minimal living standards and, sometimes, reduce inequality by compressing the income and wealth distribution of the citizenry. To reach each of these goals, welfare states have to intervene in voluntary exchanges between buyers and sellers. For better and/or for worse, welfare states change market equilibria. No matter the legal structure, welfare state institutions never exist outside the market: even “nationalized”56 health care needs to buy doctors (at what salary?) and drugs (at what price?) on free markets. No matter the labeling, welfare state institutions never exist independently of the market: even social “insurance”.57 alters labor market outcomes (who should, and can bear the burden?). Welfare state institutions can interact with markets in more or less attractive ways: they can have a smaller or larger dwls (, p. ), and they can have well or ill-defined incidences (, p. ), but they always interact. Welfare state institutions can expand (UK socialized health care) or contract (Germany social health insurance) the scope of its command, but they will always interface with the market at some frontier between the two systems, as illustrated in (p. ). This interface is ill-defined, as two incompatible logics collide, twice: The state demand curve (dashed in green) for doctors or hospitals breaks down, as the marginal utility of an additional doctors to citizens (the ultimate consumers) cannot be known.58 The state therefore has to determine the citizenry demand, usually based on a cba or a related procedure — all of which are really nothing but fancy names for sophisticated economic planning. The state cannot command the required supply (of doctors or hospitals), but must instead buy the supply from state revenues on free markets.59 Again, the logic of the market breaks down: the state as the only buyer (of doctors or hospitals) creates a monopsony, causing distributive effects (to the disadvantage of doctors or hospitals) and welfare losses (a dwl). Alternatively — maybe more plausibly — the producers of medical care may capture the planning body and extract rents, distributing away from the public and also causing a welfare loss. Either way, pricing poses unavoidable problems: somewhere down the line, producers require a price, but without atomistic demand, no pareto-optimizing price can be found. This is not to say that if health care — or some other welfare service — were completely privatized, such pareto-optimal equilibrium would be reached. In fact, government procurement of health and other services can be understood as a response to for these goods (p. ). Arguably, government procurement replaces one (horrendous) market failure with another, (milder) form of failure. I aim here to distill for welfare, what Edward McCaffery (2002) urges us to do about tax: “to find an angel or two in the abstractions that govern \\[...\\]” (2002 K117). I look for these angels in an ideal closed, mixed economy. The account I provide in the (p. ) – ) does not resemble any real existing economy, where abstractions are often shrouded in historical idiosyncrasies, and angels rarely found amidst imperfect policies. But this is a question of the (p. ), and to know what is materially doable and normatively desirable we need (p. ), not a posteriori reality. Even without the details, the abstractions alone need considerable space to be explained. I urge readers to take the time, even if much will seem familiar and some things appear remote to welfare, let alone taxation or democracy. They are not: from (p. ) to network effects (p. ), (p. ). Missing any one of these abstractions, we cannot know what welfare and taxation can, and should do. Four disclaimers apply to my tentative answer on this very big question:\\[sec:disclaimers\\] \\[itm:not-original\\] Not Original. The perspective I take here is hardly original. Many others have, in greater width (Stiglitz 2002) or depth (Sinn 2004), with narrower (Scharpf 1997b) or different foci (Zürn 2000) discussed the first-order shortcomings of “negative” integration (in the eu: Scharpf (1997b)), and economic liberalization everywhere (Stiglitz 2002), and have, in that context, defined the conditions of a mixed economy. I aim here to review the works of others and to restate some fairly conventional economic concepts in order to build a first-order checklist of welfare state design. \\[itm:no-test\\] No Positive Test. I cannot myself muster the methodological rigor or provide the econometric data, to test the first-order theories of welfare state design, but rely on mainstream literature instead. The economics of the welfare state are vastly complex, incompletely understood and any policy initiative requires careful (empirical) investigation to balance the often contradictory imperatives of economic policy. Moreover, as I later find for taxation, reforms of the magnitude implied herein may strain against the current limits of economic science: experimental designs lack in external validity of a vastly complex modern economy and simulations often lack the data, or even computing power to model such inframarginal changes. \\[itm:no-calibration\\] No Calibration. I offer no calibration of the mixed economy and its institutions, and, for the purpose of this chapter, advocate no particular balance between market and state, efficiency and equity or any of the other trade-offs a mixed economy may face. Instead, I highlight the capacities and dysfunctions of markets and potentially able to mitigate these shortcomings (p. ). I consider under (p.  ) and hypothesize how they might explain the (, p. ). \\[itm:little-macroeconomics\\] Little Macroeconomics. I limit this discussion to very basic concepts of the real economy, and ignore many of the more complex models, especially of finance and money. Modern macroeconomics, including such powerful frameworks as the IS/LM model are important (originally Hicks 1937), but would go beyond the already lengthy treatment here. I also suspect and hope that macroeconomics is best investigated by experts and its policy imperatives safely implemented by technocrats. Monetary policy, for instance, may not raise deep normative questions or offer vexing trade-offs in need of democratic adjudication: its imperatives hinge on contested and imperfect, but merely first-order, positive findings on a mass psychology of price and cost signals (see , p. ). To a lesser extent, finance, too, may be politically epiphenomenal: money and other property rights move in tandem with, and are secondary to material economic exchanges in the ideal mixed economy (see p. ). To the extent that polities can agree on specific and measurable objectives (such as price stability, or risk diversification), macroeconomic policy really can be delegated to independent central banks or other regulatory bodies. By limiting the discussion to a few rudimentary, but deeply understood concepts of the real economy, I also hope to reconnect regional integration and the welfare state to an econonomic imagination (paraphrasing Mills 1959) of our material affairs as a household — only with a cast of billions.60 Inevitably, much of the detail and complexity that policy makers have to consider, will fall by the wayside. If the defining characteristic of a welfare state is its uneasy union of market and plan, we must first understand the broader interplay of exchange and command in the mixed economy. (p. ) summarizes exchange (or market) and command (or state) institutions to address five material dimensions of the human condition: (p. ), (p. ), (p. ), (p. ) and (p. ). This is a slightly expanded set, inspired by Musgrave (1959)’s (1959) seminal definition of basic public policy functions: allocation/efficiency, distribution and stabilization (for example, as cited in (Bordo, Markiewicz, and Jonung 2011, 4). It also corresponds to Samuelson (1954)’s authoritative textbook (recently 2005) desiderata of a mixed economy: to let scarce resources be efficiently allocated by competitive markets, to improve the equity of market outcomes through redistribution, to provide public goods by government procurement and to limit inherent market instability by financial regulation and well-directed monetary and fiscal policies As stands, the game appears stacked against the market, as there is no list of government failures. In fact, failures abound in the command economy: without credible information about individual utility (confer Hayek 1931), and an elegant mechanism for their aggregation (confer Lerner 1944; Lange 1934; Debreu and Arrow 1954) resources are easily wasted and misallocated and even command components in market economies are prone to government failure (Coase 1964). Moreover, a market economy seems to be closely related to liberal democracy,61 and bloated command economies may corrupt politics,62 or even threaten the very constitution of freedom (Hayek 1944; Friedman 1962). Here again — to “economize on moral disagreement” (Gutmann and Thompson 2004 K226) and to lend credence to my later conclusions — I conservatively place the burden of proof on the state: production and distribution by markets should only be replaced or altered by command when the market can demonstrably not achieve the desired outcomes, that is, when problems materialize (row 4 in ). I now discuss the five material dimensions of human need in turn. 3.0.1 Circular Flow 3.0.2 Positional Races ## Taxation and Welfare Why does taxation matter for human welfare? As @Schumpeter knew, &quot;in the modern world, taxation *is* the social contract&quot; [@Martin2009a, 1, emphasis in original], even though social scientists have since paid little attention to it [@Tilly2009, K191]. Tax matters especially for current OECD-style welfare regimes, in which markets and states must co-exist [@Stiglitz2011]. In such a mixed economy, government must be able to draft some privately-owned resources to serve its --- ideally democratic --- *command*, without unduly altering the prices of --- ideally competitive --- *exchanges* [@Ardant1975, 165f]. &lt;!-- TODO ref chapter on mixed econ here --&gt; Much of modern social integration occurs in the balance of these two contrasting logics to produce and allocate resources, &quot;enmesh[ing] us in the web of generalized reciprocity that constitutes modern society&quot; [@Martin2009a, 3]. State plans make the conditions *for*, and set limits *to* *individual* action, as when government builds roads or collects sewage fees. Conversely, market exchanges produce much of the &quot;fungible resources&quot; used for *collective* choice, as when government pays road builders or procures sewage pipes [@Martin2009a, 4]. Of all conceivable institutions to govern the interface of states and markets, taxation --- not price controls, not expropriation, not debt, not printing money, not tariffs --- is the most equitable, efficient and sustainable [@MusgThet1959; @Stiglitz2011]. &lt;!-- TODO ref tax-matters chapter --&gt; Welfare states, with their penchant for market interventions for equity, efficiency *and* sustainability, especially, rely on good taxes. &lt;!-- %bad überleitung, the instead doesnt work --&gt; Still, taxation everywhere in the OECD is in crisis. As alternative sources of economic relief --- monetary expansion and sovereign debt --- are maxed out, structural misalignments persist, and previously forestalling (asset) bubbles have burst into their days of reckoning, public revenues appear to be strictly limited by the longtime coming contradictions of current tax regimes [@Streeck2013]. &lt;!-- %not sure about this soure, must read it --&gt; The popular mixture of (progressive) income, (proportional) consumption and (regressive) payroll taxes appears to offer only harshly unattractive tradeoffs between equity and efficiency [@McCafferyHines2010], as bases have shrunk and schedules flattened [@Ganghof2006]. At the same time --- possibly partly as a result --- inequalities of incomes and wealth have widened [@Butterwegge; @Wagner2007; @Grabka2007].[^no_wealth_distro] &lt;!-- TODO mention here that inequality findings are contentious --&gt; [^no_wealth_distro]: Data on the distribution of wealth is conspicuously hard to come by [@Crouch2004, 158] or ordinally summarized in deciles, rendering much of the inequality invisible. If governments cannot raise the resources necessary to meet democratic demands without incurring prohibitive costs, the social contract is fraying [@Crouch2004]. &lt;!-- %so so shource --&gt; Whichever way governments now turn, absent better tax, they will violate the post-war capitalist compact of stable, widely shared growth [@Pierson2002; @StreeckMertens2010]. [\\[sec:market-solutions-production\\]]{#sec:market-solutions-production label=&quot;sec:market-solutions-production&quot;} Assuming, as I do, (p. ) and conditions for (p. ) markets have at least two attractive properties: [^20] 1. when all participants have made all profitable exchanges, [^21] markets produce at the quantity and price where the costs to the producers equal the willingness to pay of buyers (see , p. ). Consumers and producers in any given market enjoy maximum *surplusses*: all consumers pay prices (at least incrementally) below the utility they receive (by area $A$), all producers receive prices (at least incrementally) above the costs they incur (by area $F$). In this *competitive equilibrium* (see column 1, row 4 in , p. ), no one can be made better off without making someone else worse off: it is *Pareto optimal*. [^22] [^23] &lt;!-- ![Market Equilibrium of Supply and Demand[]{label=&quot;fig:supply-demand&quot;}](supply-demand){#fig:supply-demand width=&quot;100%&quot;} --&gt; 2. The *price system* (see column 1, row 4 in , p. ) makes markets into supreme &quot;information processors&quot; [@Hayek1931]. [^24] Because decisions are decentralized, markets can elegantly aggregate dispersed information where a central planner would have to gather them by bureaucratic means (such as a [cba]{acronym-label=&quot;cba&quot; acronym-form=&quot;singular+short&quot;}). Because market decisions are always backed by private costs, the market price system can reveal and communicate (some!) private information, where a central planner may face distorted (inflated) information about cost and utility. ##### Conditions for Perfect Competition. {#sec:perfect-competition} The above properties of market equilibria hold only under the strict assumptions of perfect or atomistic competition. In one formulation, this entails [@McDowell2006 157f.]: 1. *Infinite buyers and sellers*. [\\[itm:infinite-buyers-sellers\\]]{#itm:infinite-buyers-sellers label=&quot;itm:infinite-buyers-sellers&quot;} There are so many consumers and producers in the market that an offer or bid by any one of them will have a negligible impact on prices. Everyone is a price taker. 2. *Zero barriers to entry and exit*. [\\[itm:easy-entry-exit\\]]{#itm:easy-entry-exit label=&quot;itm:easy-entry-exit&quot;} Firms can start or cease to produce a good at relatively little cost and effort. All markets are wide open. 3. *Perfect factor mobility.* [\\[itm:perfect-factor-mobility\\]]{#itm:perfect-factor-mobility label=&quot;itm:perfect-factor-mobility&quot;} In the long run, labor, capital and other inputs to production can move to wherever they earn the highest rents. It is hire and fire. 4. *Perfect information*. [\\[itm:perfect-information\\]]{#itm:perfect-information label=&quot;itm:perfect-information&quot;} Consumers know all prices and qualities of goods, producers know all prices and qualities of factor inputs. People are omniscient and powerful calculators of utility. 5. *Profit-maximizing firms*. [\\[itm:profit-maximizing-firms\\]]{#itm:profit-maximizing-firms label=&quot;itm:profit-maximizing-firms&quot;} Firms sell at the price and quantity that maximizes their profit. [^25] Other, exogenous criteria are not part of firm decision making. Given the other assumptions of perfect competition, firms sell where marginal cost equals marginal revenue. [^26] 6. *Homogeneous products.* [\\[itm:homogeneous-products\\]]{#itm:homogeneous-products label=&quot;itm:homogeneous-products&quot;} Goods produced by one supplier are the same as those produced by another supplier of the same category. Inputs provided by one factor owner are the same as those provided by another owner of the same factor. All goods and factor inputs are completely commodified. To this, one might add, according to @Wikipedia2012: 7. *Zero transaction costs.* [\\[itm:zero-transaction-costs\\]]{#itm:zero-transaction-costs label=&quot;itm:zero-transaction-costs&quot;} Buyers and sellers can exchange goods, services and make contracts at zero cost. Search, information, bargaining, policing and enforcement costs are assumed away. There is no friction. 8. *Constant returns to scale.* [\\[itm:constant-returns-to-scale\\]]{#itm:constant-returns-to-scale label=&quot;itm:constant-returns-to-scale&quot;} For any additional unit produced, costs rise by the same amount, no matter how much units are produced. The cost function is *linear*, marginal costs are constant. There are no (dis)economies of scale or scope. [^27] Constant returns to scale are related to, but distinct from assumption [\\[itm:easy-entry-exit\\]](#itm:easy-entry-exit){reference-type=&quot;ref&quot; reference=&quot;itm:easy-entry-exit&quot;} on . Excessive economies of scale imply difficult entry. Conversely, difficult entry implies varying marginal costs at low output. 9. *Property rights* [\\[itm:property-rights\\]]{#itm:property-rights label=&quot;itm:property-rights&quot;} are well established. Lastly, --- if somewhat redundant, because axiomatically assumed away by neoclassical welfare economics --- I would add: 10. *Utility equals willingness to pay.* [\\[itm:same-budgets\\]]{#itm:same-budgets label=&quot;itm:same-budgets&quot;} The first theorem of welfare economics --- that above specified perfect markets equilibrate at pareto optimality over *initial* distributions --- is often misrepresented to imply that individual *utility* is adequately expressed in willingness to pay, on which free market exchanges operate. This shortcut works only under the most heroic assumption of all: that all market participants have the same budget constraint. If everyone invoking the first theorem were to represent it in full, including the crucial, qualifying &quot;...over *given* allocations&quot;, this condition for perfect competition would be unnecessary. Alas, many do not. Moreover, equating individual utility with willingness to pay constitutes what might be called the of welfare economics (p. , because it *both* assumes equal *and* unequal budget constraints: the former to maximize utility, and the latter as incentive. For a thesis so thoroughly grounded in neoclassical orthodoxy as this, it seems appropriate to feature this contradiction prominently. I note wherever I relax some of these strict (and rarely plausible) assumptions in the following sections (summarized in , p. ). It should be clear then that I make the above, conservative or neoclassical assumptions less out of conviction, but for their logical elegance and to &quot;economize on moral disagreement&quot;, as [@GutmannThompson-2004-aa K226] have suggested. I hope these positions will be widely acceptable to readers on the political right, and provisionally tolerable to readers on the left. If I can show taxation to be wasteful, unfair and unsustainable and democracy to be outmatched, assuming even such liberal orthodoxy, sweeping reform should be all the more obvious. #### Market Failures. \\phantomsection [\\[sec:market-failures\\]]{#sec:market-failures label=&quot;sec:market-failures&quot;} Markets operate efficiently on private goods: people can be *excluded* from their *rival* use (see ). In this case, social costs and utility match private cost and utility. From some goods, people cannot be effectively excluded and/or are not rivals in their consumption. [^28] In these cases, social and private costs and utility diverge and markets may fail. Goods are overproduced when social cost is higher than private cost (*negative externality*), and goods are underproduced when social benefit is higher than private benefit (*positive externality*). This problem holds more broadly for *public goods*, *common goods* and *natural monopolies* (see column 1, row 5 in ), [^29] summarized in according to @Samuelson-1954-eu&#39;s typology of goods [-@Samuelson-1954-eu]. ##### Failure: Public Goods \\phantomsection [\\[sec:public-good\\]]{#sec:public-good label=&quot;sec:public-good&quot;} are *underprovided* by markets, because no one can be prevented from using them (non-exclusion), and they do not get used up (non-rivalry). Potential buyers can always free-ride on other&#39;s purchase and are therefore unwilling to pay producers adequately. Defense is a public good and fireworks are canonical examples of public goods. ##### Failure: Common Goods \\phantomsection [\\[sec:common-good\\]]{#sec:common-good label=&quot;sec:common-good&quot;} are *overused* on markets, because they are rival but again, no one can be prevented from using them [@Hardin-1968-aa]. Potential buyers can free-ride without paying the adequate price for exploiting the rival commons [@Hardin-1968-aa]. [^30] [^31] Clean air is a common good, and so may be the enlightened understanding of the electorate [@Caplan2007]. ##### Failure: Natural Monopolies \\phantomsection [\\[sec:natural-monopoly\\]]{#sec:natural-monopoly label=&quot;sec:natural-monopoly&quot;} arise where economies of scale abound in the production or distribution of goods or services, such that only a single or very few suppliers can profitably exist. Natural monopolies are *mispriced at the margin* because after initial fixed costs --- which few single consumers would be willing or able to pay --- marginal cost become negligible. Excessive economies of scale often occur in businesses dominated by fixed, rather than variable cost. Sewer systems, electricity grids or search engines can be natural monopolies with prohibitively high entry costs [^32] for basic infrastructure (sewers, electricity masts, web indices) and later, negligibly small marginal costs for adding an additional consumer. Natural monopolies can incur welfare losses in two ways: 1. If only one market supplier exists, it may charge monopoly prices causing a deadweight loss of underconsumption. 2. In the extreme, given the high marginal cost for the first buyer, no first buyer may come forth and the otherwise [^33] pareto-improving natural monopoly may not be provided at all. ##### Failure: Principal-Agent Problems. \\phantomsection [\\[sec:principal-agent-problem\\]]{#sec:principal-agent-problem label=&quot;sec:principal-agent-problem&quot;} Principal-agent problems are one market failure of the broader class of information asymmetry problems (row 5, column 2 in , p. ), where at least one party to a trade knows less about the service, good or risk being exchanged (pioneered by Nobel laureates @Akerlof-1970-aa, Stiglitz [-@Stiglitz1976] and @Spence1974). In principal-agent problems, the asymmetrically known quality is the effort exerted by the agent on behalf of the principal. When agent (say, manager) effort cannot be fully observed by principals (say, owners), and principals have interests (say, long-term returns) diverging from those of agents (say, a pet project), agents may be able to cheat on their contracts. When agents shirk successfully, they will exert less (or ill-directed) effort than would be pareto-optimal. In the extreme, the market between principals and agents breaks down completely, as principals anticipate shirking agents and forego the transaction altogether. Applied game theory and related disciplines offer a host of incentive designs to realign interests of principals and agents, which I need not discuss here comprehensively (but see @Tirole2006). Solutions include (p. ) --- which creates unemployment --- or deferred compensation and tournaments --- which invites risk-seeking (Holt 1995). Efficiency wages and tournaments try to alter the probabilistic calculus of would-be shirkers by promising outsized instead of *marginal* rewards and punishment for whichever effort *is* (randomly) observed. Deferring (part of) the compensation to later may make agents more far-sighted, but will still strictly cap their downside risk, especially when only bonuses are deferred (for example, stock options). The worst that can happen to an agent in any of these schemes is to loose their job, the tournament promotion or their bonus. By contrast, the worst that can happen to a principal, is to lose everything. In addition, if the observations of effort on which such schemes are based are spotty or isolated --- as they often are --- agents can &quot;game the system&quot; and incentives may turn ineffective, or even perverse. Both (p. ) and tournament compensation also increase economic inequality: instead of marginal productivity, they reward (p. ) and threaten losers with unemployment. All these incentive design schemes fall short of the one genuine solution to realign principal and agent interests: to either sell agents stock or charge them a substantial sign-up fee [@Tirole2006], effectively making agents into principals, too. Only then can they bear both the full upside and downside risk of the enterprise. To be able to take on such risk, of course, agents must own substantial assets, which they may not have in an unequal economy. Principal-agent problems fail markets if, and to the extent that, inequality in assets prevents people from taking equal risks in otherwise welfare-enhancing joint projects. They are one of the cases, where inequity makes for inefficiency, too. This is not merely a theoretical conumdrum, but a very real problem for postindustrial and knowledge-based economies (for example, Lisbon Strategy, EU 2020, @Bell-1973-aa). [^34] Almost by definition, an economy based on knowledge and innovation will display information asymmetries. The effort a knowledge worker (say, a programmer) puts in, cannot easily be observed, because that would require the principal to acquire the exact same specialized knowledge (say, a programming language). Similarly, a would-be innovator (say, an [ict]{acronym-label=&quot;ict&quot; acronym-form=&quot;singular+short&quot;} entrepreneur) will always know more about her nascent and uncertain innovation than any possible investor, because otherwise, the investor would have done the project herself, already. In short, for competitive markets to do their magical &quot;stochastic tinkering&quot; [@Taleb2007 211], people need to be equipped and incentivized to act on their local and uncertain ideas, and to specialize. For a *homo economicus* at least, there can be no *entrepreneurship* without some *ownership*, too. #### State Responses to Market Failures \\phantomsection [\\[sec:state-responses\\]]{#sec:state-responses label=&quot;sec:state-responses&quot;} States can respond to market failures by fiscal and regulatory interventions. I discuss them for each type of good (rows 6--8, column 1 in ). ##### Fixing Public Goods. \\phantomsection [\\[sec:public-good-response\\]]{#sec:public-good-response label=&quot;sec:public-good-response&quot;} States can step in to *provide public goods* or subsidize their private provision, both out of the public purse (fiscal policy). There is no regulatory or monetary response to public goods failure. [^35] ##### Fixing Common Goods. \\phantomsection [\\[sec:common-good-response\\]]{#sec:common-good-response label=&quot;sec:common-good-response&quot;} States can protect overused commons by a regulatory policy of &quot;fencing in the commons&quot;. [^36] By doing so, governments follow the [@Coase1960] theorem. It holds that markets can pareto-optimally resolve externalities if transaction costs are sufficiently low, and if property rights are well-defined. [^37] The Coase theorem is often erroneously cited to argue against state intervention. Maintaining and *issuing new property rights* are, of course, *regulatory* state interventions. [^38] Alternatively, states can resolve &quot;Tragedies of the Commons&quot; [@Hardin-1968-aa] fiscally by slapping a *Pigouvian tax* (@Pigou1912, popularized by @Baumol1972) on using the commons. [^39] The Pigouvian tax prices in the externality of using a common good. [^40] There is no monetary response to overused commons (see , p. ). &lt;!-- %here are two important notes from prospect theory, and Kahnemann 2012 %I think insurance requires only the weaker (contested by Kahnemann) expected utility hypothesis (bernoulli), who says that risk aversion can be explained merely by diminishing marginal utility (specifically, argues Bernoulli, utility is a logarithmic function of wealth). %Maybe I can do here with risk aversion and don&#39;t need LOSS aversion (based on reference points), which is what Kahnemann is out to explain, and with him prospect theory. %Consider a weird graph about that in Kahnemann. % The mixed economy can be understood as those institutional crutches that help system-one humans to behave as system-two humans, because system-two can&#39;t be relied upon in the long run, it&#39;s too effortful to use. %So we use institutions to get us there -- because system two is the better. % Kahnemann raises a big stink with indifference curves: %he says neoclassical economics is wrong there, because prospect theory shows that people have reference points from which they are loss averse, suggesting that there is no such thing as an indifference curve (I am not sure that is formally right, the indifference curve just wouldn&#39;t be linear, as it might be). % my problem with Kahnemann is that he meanders between positive and normative theory. %I would think prospect theory is a positive finding, but it need change normative theory. %Of course, Kahnemann might say, well if utility is experienced according to prospect theory, than maybe THAT is the kind of utility that we should maximize. %I disagree: %I think we should let system two reign, and therefore also need not abandon neoclassical economics, we just must make sure that there is always good and enough crutches around. --&gt; &lt;!-- %\\subparagraph{Fixing PCA-Problems} %\\subparagraph{Entrepreneurship Needs Broad-Based Ownership.} \\phantomsection \\label{sec:Ownership} More deductively, market economies can be thought of as welfare-maximizing because they capture decentralized and/or privately known information, and let diverse solutions compete. %Nassim Nicholas \\cite{Taleb2007} has put this succinctly by praising ``aggressive trial and error&#39;&#39; (\\emph{ibid.}: %xxi) in free markets that ``allow people to be lucky&#39;&#39; (\\emph{ibid.}: %xxi)\\footnote{Being a quantitative trader by profession, \\cite{Taleb2007} actually abandons rational choice when faulting Karl \\cite{Marx-1867-aa} and Adam \\cite{Smith-1776-lq} for believing that free markets work because of rewards.}. %This basic impetus for capitalist creativity is expressed in desideratum \\ref{des:Entrepreneurship}: %\\begin{desideratum}[Entrepreneurship] % A desirable tax will allow entrepreneurs to make their own production decisions according to their independent judgement of private and/or local information. % \\label{des:Entrepreneurship} %\\end{desideratum} %But what is required for this ``stochastic tinkering&#39;&#39; (\\citealt{Taleb2007}: %211) to work? %Principal-agent theory suggests that maximum effort may not be exercised when effort of agents is imperfectly or non-observable, or other information asymmetries prevail. %This of course --- unobservable effort and information asymmetries --- are likely features of highly differentiated knowledge economies\\footnote{Calls for more startups, patents and research spin-offs, particularly in Germany, may serve as evidence for suboptimal incentive design under the status quo.}, where people produce an idea, not a piece of welded metal (cf.~\\citealt{Bell-1973-aa}). %Game theoretic incentive design suggests that information asymmetry problems can be resolved either by making agents shareholders or by charging them substantial sign-up fees, both of which require substantial assets to begin with \\citep{Tirole2006}. %In short, for competitive markets to work their magic, people need to be equipped and incentivized to act on their local and diverse ideas. %For \\emph{homo economicus}, there can be no \\hyperref[des:Entrepreneurship]{entrepreneurship} without some \\hyperref[des:BroadOwnership]{ownership}, too. %This ties in with the simple capitalism desideratum presented earlier, but it also adds a specification: %\\begin{desideratum}[Broad-Based Ownership] % A desirable tax allows for or promotes a broad-based ownership of the means of production. % \\label{des:BroadOwnership} %\\end{desideratum} %include footnote for all desideratum&#39;s of where they are leading. %reference the same footnote %\\subsection[Welfare Gains]{Welfare Gains: %How Taxes Can Make the Pie Larger} \\label{sec:PurposesOfTaxation} %The burden of proof is on the state for interventions in the market. %Witnesses for the defense are summarized in \\autoref{tab:ends-mixed-economy} and are entertained in the below. %Taxes and other state interventions in the economy can in fact enhance market outcomes in several ways: %this figure is obsolete, it is now \\label{tab:ends-mixed-economy} %\\begin{description} % \\item[Redistribution.] \\phantomsection \\label{sec:fiscal-redistribution} Governments may respond to excessive \\emph{inequality} by taxing people proportionally or progressively to redistribute resources. %While inequality is further discussed in \\autoref{sec:tax-justice} on \\hyperref[sec:tax-justice]{equity}, it does bear on efficiency, too, as is argued in \\autoref{sec:InequalityIsInefficient}. % \\item[Risk Pooling.] \\phantomsection \\label{sec:state-insurance} People can possess \\emph{asymmetric information} about things of uncertain quality they exchange on the marketplace. %Insurance of unemployment, health or disability are principal examples, where sellers of risk (insurants) typically know more about their own risks than buyers of risk (insurers)\\footnote{This relaxes perfect competition condition {itm:PerfectInformation} (\\hyperref[itm:PerfectInformation]{perfect information}).}. %Less-informed buyers (insurers) of risks may expect bad risks for \\emph{all} insurants, causing premiums to rise and driving low-risk sellers out of the market entirely. %This mechanism may repeat until no exchanges are made at all, defeating the purpose of insurance. % Governments can avoid these \\emph{lemons markets} by forcing everyone to take out insurance \\citep{Akerlof-1970-aa}. %When risks are universal --- as is arguably the case for unemployment, health and disability --- the premiums for these insurances are effectively taxes. % \\item[Public Goods] \\phantomsection \\label{sec:public-good} can be enjoyed by an arbitrary number of people without exhaustion (non-rivalrous) and no one can be excluded (non-excludable) from its using (\\citealt{Samuelson-1954-eu}, summarized in \\autoref{tab:Types-Of-Goods}). %National defense is one example. %Because people in larger groups can always free-ride on the provision of public goods by others, they are likely to be \\emph{underprovided} by self-seeking individuals or markets \\citep{Olson-1971-aa}. % Governments can improve welfare by providing public goods out of tax revenue. %the types of good table happens earlier, as \\label{tab:types-of-goods} % \\item[Common Goods] \\phantomsection \\label{sec:common-good} are rival in their consumption but do not allow exclusion: %everyone can benefit, but they can be exhausted \\citep{Samuelson-1954-eu}. %A \\emph{Tragedy of the Commons} occurs when people overuse the common good \\citep{Hardin-1968-aa}\\footnote{Elinor \\cite{Ostrom1990} criticizes the canonically assumed failure of commons in social science and provides an empirically grounded account of their successful, non-coercive governing.}. %\\begin{quote} % \\emph{What is common to the greatest number has the least care bestowed upon it.}\\\\*\\\\* % Aristotle, Politics, Book II, Chapter 3 (384 b.c.-322 b.c.) %\\end{quote} % Government can avoid the \\emph{negative externality} of exhaustion of commons by \\emph{pricing in} the costs of its use, an approach also known as Pigouvian taxation (\\citealt{Pigou1912}, popularized by \\citealt{Baumol1972})\\footnote{The alternative, non-tax solution of issuing property rights on the commons (for example through an Emissions Trading Scheme), thereby making it an ordinary private good is of course a ``government&#39;&#39; solution, too. %Markets cannot maintain, let alone introduce new property rights.}.%check definitively whether it is Pigovian our Pigouvian. %note from correction: %add coase theorem in here. %Note that some solutions are costly. % \\item[Natural Monopolies] \\phantomsection \\label{sec:natural-monopoly} arise where economies of scale abound in the production or distribution of goods or services, such that only a single supplier can profitably exist\\footnote{This relaxes perfect competition condition \\ref{itm:infinite-buyers-sellers} (\\hyperref[itm:infinite-buyers-sellers]{price taking}) and {itm:easy-entry-exit} (\\hyperref[itm:easy-entry-exit]{easy entry and exit}).}. %Excessive economies of scale often occur in businesses dominated by fixed, rather than variable costs. %Sewer systems, electricity grids or search engines can be natural monopolies with prohibitively high entry costs for basic infrastructure (sewers, electricity masts, web indices) and later, negligibly small costs for adding an additional consumer. % Natural monopolies can incur welfare losses in two ways: %if only one market supplier exists, it may charge monopoly prices causing a deadweight loss of underconsumption. %Conversely, if several suppliers exist in one market, each of them may be unable to invest at optimal levels (underprovision). %%This ain&#39;t quite right. %I&#39;m missing the marginal vs.\\ average cost problem. % Governments can avoid the deadweight losses of natural monopolies by regulating them (for example, last mile ICT in Germany), franchising or outsourcing them (for example, local railway in Germany), enforcing common carriage (for example, electricity in Germany) or by nationalizing them (for example, public ownership of motorways in Germany). % \\item[Easy Market Entry.] Problems of prohibitive entry costs are not limited to natural monopolies\\footnote{This relaxes perfect competition condition {itm:easy-entry-exit} (\\hyperref[itm:easy-entry-exit]{easy entry and exit}).}. %Competition can also be hampered by market players who enjoy excessive economies of scale by sheer size or past learning curves. %Aside from regulatory responses (antitrust), governments can react proactively by means of infant industry protection or other industrial policy, the contested (de)merits of which are not under further consideration here\\footnote{If infant industry protection \\emph{is} considered welfare-enhancing, its medium-term welfare losses to the polity (either in the form of a DWL of subsidizing or protectionism) become a public good to the extent that a successful infant industry generates positive externalities for the rest of the economy. %As such, infant industry protection should be partially financed out of general revenue.}. %\\end{description} --&gt; &lt;!-- %\\paragraph{Diminishing Utility is a Fact (Hard) to Observe} Diminishing utility is a plausible intuition grounded in our very nature\\footnote{Some findings suggest that we are neurologically hard-wired to display diminishing utility in our feelings \\citep{Ng-1997-aa}.}. %Our quintessential evolutionary features, both our metabolism and propagation display starkly diminishing returns: %you can only eat so much and rear so many children. %Survey measures of self-reported happiness also support diminishing utility of wealth and income \\citep{Veenhoven-2000-aa, Nickell2008}\\footnote{If \\hyperref[sec:positional-race]{positional consumption} is, in fact, rampant, survey measures may yet underestimate the diminishing utility of wealth and income. %When people extract utility from levels of consumption \\emph{relative} to others, their valuation of \\emph{absolute} wealth and income is probably inflated.}. %add positional cascades, Frank %this is a great argument, look for empirical evidence --&gt; &lt;!-- Some theories for self-reinforcing inequality %\\paragraph{Path Dependence or Cumulative Causation.} The third dynamic of \\emph{path-dependent or cumulative causation} refers to situations where small initial state differences in performance lead to additional opportunities, reinforcing initial inequality. %These additional, scarce opportunities may be awarded to individuals (or firms, or regions) based on easy but imperfect measures (think SAT scores). %They may also be awarded based on probabilistic predictions on future performance (think past scholarships), further increasing a self-reinforcing dynamic. %In the worst, most inequitable (and inefficient case), they are awarded based on meaningless, randomly occurring differences (think mental state on day of testing), haphazard selections (think first come, first serve) or systematic measurement bias (think habitus expectations by assessors). %Malcolm \\cite{Gladwell} illustrates this dynamic in his account of \\emph{Outlier} hockey stars in Canada, whose birthdays are significantly more often in the early months of the year. %\\citeauthor{Gladwell} attributes this to very early elite selection in Canadian hockey and a cut-off point between the different leagues on each December 31st, giving children hockey players born early in the year a slight developmental advantages over their peers, amplified by the additional training they receive if initially selected. %Path-dependent or cumulative causation of inequality are frequently observed in educational systems, particularly in those which track students early (as in Germany). %These dynamics also apply elsewhere, where initially only slight differences lead to divergent experiences, reinforcing inequality and leading to further opportunities, for example when individuals or firms benefit from learning curves or economies of scale after initial jobs. %\\paragraph{Self-reinforcing Network Effects in Scale-Free Distributions.} The fourth dynamic are self-reinforcing network effects in homopholous networks with a scale-free graph distribution. %Networks are formalized as \\emph{graphs} comprising of \\emph{nodes} (individuals, firms) interconnected with (directed, undirected and/or weighted) \\emph{edges} (aquaintance, contracts) \\citep{Kleinberg-2009-oz}. %The \\emph{degree} of a node is given by the number of edges emerging from it. %I will first illustrate the graph theory of innovation diffusion \\citep{Bass1969} with a simple example, the diffusion of fax machines. %Assume that individuals (nodes) decide on whether to purchase a fax machine based on the technology&#39;s inherent value (the \\emph{innovation coefficient}) and the value they realize from the number of their peers (degree) also owning a fax machine (the \\emph{imitation coefficient}). %Consider first what would happen in a grid network, where every individual is connected to each adjacent individual (all nodes have the same degree). %Assume next that initial adopters are randomly distributed. %Fax machines would, largely determined by their inherent value, proliferate (or not) relatively homogenously over the entire network. %It will be unlikely that any given individual (node) has many more peers with fax machines than any other individual, rendering the imitation coefficient relatively inconsequential. %illustrate this --&gt; ##### Fixing Natural Monopolies. \\phantomsection [\\[sec:natural-monopoly-response\\]]{#sec:natural-monopoly-response label=&quot;sec:natural-monopoly-response&quot;} Governments can avoid the deadweight losses of natural monopolies fiscally, by nationalizing them (for example, public ownership of motorways in Germany) and ideally charging users a fee at *average* cost (for example, trucks and coaches on federal motorways in Germany). [^41] Governments can also procure natural monopoly goods and services from private firms and charge users a fee at average cost (for example, local railway in Germany). Alternatively, governments can allow privately held natural monopolies but tightly regulate them to enforce pricing at average cost and avoid a monopoly [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} (for example, *last mile* [ict]{acronym-label=&quot;ict&quot; acronym-form=&quot;singular+short&quot;} or electricity in Germany). There is no monetary response to natural monopoly problems (see footnote [\\[fn:monetary-commons\\]](#fn:monetary-commons){reference-type=&quot;ref&quot; reference=&quot;fn:monetary-commons&quot;}). #### Monetary Policy for Price Stability. \\phantomsection [\\[sec:price-stability\\]]{#sec:price-stability label=&quot;sec:price-stability&quot;} Monetary policy contributes best to efficient production --- and almost all other dimensions of material human need --- by staying out of the way of markets with stable prices to allow efficient exchanges in the first place. [^42] [^43] When prices rise (inflation) or fall (deflation) overall, otherwise pareto-optimal exchanges may be hampered. The welfare losses of inflation include hoarding (of real assets), drowning out relative price changes (noise), increasing transaction costs (such as shoeleather costs [^44] and menu costs [^45] ), as well as general uncertainty and possibly, unrest. Cost-wage spirals [^46] make inflation self-reinforcing, potentially escalating into hyperinflation. Inflation is also believed to set off the business cycle [@Friedman1970]. Deflation, while scarcely observed in the western world in the post-Bretton-Woods regime, [^47] is similarly damaging. It also makes transactions more costly and heightens uncertainty. Deflation can also cause the hoarding of cash and trap liquidity [^48] and self-reinforce into a deflationary spiral. In the long run, monetary expansion (or contraction) should follow the output of the economy $G$, so that price levels stay stable. [^49] Whether inflation and deflation are, as the monetarists would have it, &quot;always and everywhere a monetary phenomenon&quot; [@Friedman1970] and therefore caused by an over-expansion or contraction of the money supply in the first place, or whether it has it roots in the real economy as the Keynesians would argue, is a very complicated empirical question but --- luckily for the author --- irrelevant to the state job of ensuring an efficient market place. No matter &quot;who dunnit&quot;, monetary policy should aim at price stability, and, perhaps, counteract real price shocks --- if and to the extent that they occur --- with (p. .). Pooled Risks: Saving the Pie {#sec:risk} ---------------------------- [^50] #### The Human Condition of Risk. \\phantomsection [\\[sec:human-nature-of-risk\\]]{#sec:human-nature-of-risk label=&quot;sec:human-nature-of-risk&quot;} Humans inhabit a volatile environment, marred by low probability, but high impact events (*black swans*, according to @Taleb2007), for example a serious work accident. Unfortunately, humans also tend to ignore precisely such low probability, but high impact events [@Taleb2007], and overestimate the probabilities of favorable outcomes [@Baron2000 44], especially when they have few cognitive resources available. When they give it some thought, most people *avoid grave downside risks*: for example, most will prefer the certain but low cost of car liability insurance over the rare but high cost of paying for a car accident (column 2, rows 1--3 in ). [^51] #### Market Solutions to Risk: Insurance. \\phantomsection [\\[sec:insurance\\]]{#sec:insurance label=&quot;sec:insurance&quot;} Markets provide *insurance* as a ready-made institution to address this human need for down-side risk aversion (column 2, row 4). Insurants can buy protection from their grave downside risk, say, a car crash, by pooling their individual risks. An insurance deal stipulates that all insurants will regularly chip in a small amount to cover the few unlucky car wreckers, in return for the promise that they, too, will receive a payout if they crash their cars. [^52] Aside from car insurance, markets do sometimes, to some extent, provide insurance against the four big life risks commonly associated with the welfare state: unemployment, sickness, accident and disability. [^53] Market insurance of these life risks *does not* involve a redistributive component, though that is easily assumed for unemployment insurance. The *voluntary* exchange of premiums for coverage, in car and all other insurance, is a pareto-improvement: all risk-averse insurants are better off, by hedging against downside risks. Insurance is not obligatory and whoever finds it unnecessary (as may be the case for rich individuals who face limited downside risks) is not affected. #### Market Failure in Insurance: Asymmetric Information. {#sec:asymmetric-information} Insurance markets may fail when buyers and sellers possess asymmetric information about the risks to be insured. [^54] ##### Ex ante, \\phantomsection [\\[sec:adverse-selection\\]]{#sec:adverse-selection label=&quot;sec:adverse-selection&quot;} insurants with privately known high risk may disproportionately take out insurance. Insurers, anticipating such *adverse selection*, but unable to tell high-risk (&quot;lemons&quot;) from low-risk (&quot;cherries&quot;) applicants, [^55] may expect bad risks for *all* buyers, causing premiums to rise and further driving low-risk insurants out of the market. This *lemons market* mechanism may repeat until no exchanges are made at all, defeating the purpose of insurance [@Akerlof-1970-aa]. Adverse selection abounds in the insurance of the big life risks. Ex ante, insurants know more about their likelihood to become unemployed, sick, to be in an accident or become disabled than their insurers. [^56] ##### Ex post, \\phantomsection [\\[sec:moral-hazard\\]]{#sec:moral-hazard label=&quot;sec:moral-hazard&quot;} sellers of risk (insurants) might take on more risk than they otherwise would have, causing moral hazard and in turn drive up premiums. Moral hazard, too, may occur in the insurance of big life risks. Ex post, insurants may be willing to live less healthily, or more recklessly than they would without insurance. #### State Responses to Asymmetric Information in Insurance. \\phantomsection [\\[sec:state-insurance\\]]{#sec:state-insurance label=&quot;sec:state-insurance&quot;} ##### Ex ante, states can resolve lemons markets by *regulatory* means if they force everyone at risk to take out insurance (@Akerlof-1970-aa, @Barr) (column 2, row 8). [^57] States can also resolve lemons markets by *providing benefits* out of the treasury (column 2, row 9). As the risks of unemployment, health, accident and disability are near-universal, [^58] the contributions for such state-run insurance are effectively taxes. Some states (such as Germany) outsource insurance to quasi-fiscal organizations. This &quot;social insurance&quot; may make a difference in administrative, legal or rhetorical terms, but its economics are that of a state-run insurance and its revenues are taxes. Financing public insurance out of dedicated &quot;social contributions&quot; (usually regressive payroll taxes) instead of general revenue only adds a specific distributive component (a regressive tax on labor) to the overall tax schedule. [^59] [^60] Mandating insurance or, equivalently, providing benefits out of the public purse need not redistribute resources over and above the Kaldor-Hicks improvement from resolving a lemons markets. Properly understood, state interventions to hedge *individual* risks are meant to improve the efficiency, not equity of outcomes. Real policy often differs from this (p. ) and layers distributive components on top of the Kaldor-Hicks improvement. In Germany, for instance, social contributions are proportional, but capped and public health insurance covers an insurant&#39;s children at no additional cost. These additions to schedule and benefits may or may not be desirable, but they properly belong in the realm of (p. ). ##### Ex post, states and markets have essentially the same, clumsy method to reduce moral hazard: [^61] they re-individualize some of the risk by asking for co-payments or provide incentives for prudent behavior. Exploiting moral hazard is a (p. ), and can therefore be resolved either by (partial) property rights (co-payments) or by Pigouvian taxation (incentives). Equitable Distribution: Slicing the Pie Fairly {#sec:distribution} ---------------------------------------------- #### The Human Condition of Inequality \\phantomsection [\\[sec:human-nature-of-inequality\\]]{#sec:human-nature-of-inequality label=&quot;sec:human-nature-of-inequality&quot;} Humans, the social animals, can deal with material scarcity in two ways [@Pickett-2009-kx]: &quot;because members of the same species have the same needs as each other, they have the potential to be each other&#39;s worst rival&quot; [-@Pickett-2009-kx 197] but also, &quot;the potential to be each other&#39;s best source of cooperation, learning, love and assistance of every kind&quot; [-@Pickett-2009-kx 198]. Dominance Strategies. : The first, @Hobbes-1651-aaian [-@Hobbes-1651-aa] strategy is one of dominance: *homo homini lupus est*, man is a wolf to his fellow man. [^62] Following *dominance strategies* (column 3, row 1 in ), humans --- much like their primate cousins, the chimps --- maximize their classical evolutionary fitness (@Darwin1859, recently @Dawkins1976) by relying on individual power to secure access to scarce resources (food, shelter, females). Affiliative Strategies. : The second strategy is one of *affiliation* (column 3, row 3) and mutuality: to be your &quot;brothers keeper&quot; ([kjv]{acronym-label=&quot;kjv&quot; acronym-form=&quot;singular+short&quot;} Bible, Genesis 4:9). [^63] Following affiliative strategies, humans ---as their other primate cousins, the bonobos --- maximize inclusive fitness (@Hamilton1964, popularized in @Wilson1975), or fitness emerging at the group-level [@Wilson2012] by cooperation, trust and reciprocal altruism [-@Pickett-2009-kx 202ff]. *Dominance hierarchies* (column 3, row 2) arise as dominance strategies prevail and (usually male) members of a species fight for higher status to successfully monopolize resources. All but the highest status individuals are held to suffer from such heightened *relative* inequality (*ibid.*). Instinct does not determine us to follow dominance strategies (as the chimps, according to *ibid.*) or affiliative strategies (as the bonobos, according to *ibid.*), and so we need culture and institutions to strike that balance. *Both* markets and states can strike that balance, and reign in on dominance hierarchies. #### Market Equity \\phantomsection [\\[sec:market-equity\\]]{#sec:market-equity label=&quot;sec:market-equity&quot;} On competitive markets, people enter into voluntary exchanges that, at least, make everyone better off (if not necessarily by the same amount). [^64] Interactions under dominance hierarchies are *no such* pareto improvements; the (physically) stronger will extract from the weaker all she can, possibly short of killing the weaker party if prolonged extraction is in the interest of the stronger party. [^65] *Real* existing market economies have sometimes --- but not always --- created sharp inequality. Still, against the backdrop of dominance hierarchies, *ideal* market economies with , institutionalizing pareto-improving, voluntary exchange must be considered a civilizing achievement. [^66] #### Excessive Inequality \\phantomsection [\\[sec:inequality-dynamics\\]]{#sec:inequality-dynamics label=&quot;sec:inequality-dynamics&quot;} Real existing, imperfect, modern markets display some excessively inequitable dynamics that may compromise this ability to at least somewhat compress dominance hierarchies. In the following, I discuss some of these dynamics and further relax some assumptions of (p. ). ##### Efficiency Wages. \\phantomsection [\\[sec:efficiency-wages\\]]{#sec:efficiency-wages label=&quot;sec:efficiency-wages&quot;} Efficiency wages are, counterintuitively, wages *above* the market-clearing rate. At efficiency level, wages are so high, that some people cannot find gainful employment and must remain unemployed. [^67] Wages may be above market clearing rate because employers want to attract more applicants to choose from, because local traditions demand it, or even to feed malnourished workers. Two other suggested reasons stand out: 1. *Reducing turnover.* Employers may pay above-clearing wages because they want to avoid costly turnover. When faced with both potential unemployment, attractive, and, especially, seniority-graded wages, workers may not look for a job elsewhere (for example, @Salop1979, on [ldcs]{acronym-label=&quot;ldc&quot; acronym-form=&quot;plural+short&quot;} @Stiglitz1974a). 2. *Avoiding shirking.* Employers may also pay above-clearing wages because they want to deter incompletely contracted and incompletely observed workers from shirking. Because employers (principals) can observe effort only sporadically and imperfectly, they will catch shirking workers (agents) only some of the time. Would-be shirkers face some probability of getting caught, a reward for continued shirking, and a punishment for getting caught and may optimize their behavior accordingly. Efficiency wages can thereby solve this (p. ): by increasing both wage and unemployment rate, employers widen the spread between the two probabilistic outcomes for would-be shirkers (getting caught, not getting caught). Risk-averse employees may then work hard to avoid the inflated downside risk of unemployment and loss of a generous wage [@Stiglitz1984]. Both when they increase wages to reduce turnover, and to avoid shirking, employers move the labor market out of equilibrium. Otherwise pareto-improving employment is lost, and economic welfare foregone. Still, efficiency wages may persist, because they are efficient --- individually utility optimizing --- for employers, if inefficient for the economy as a whole. Employers enjoy lower turnover and shirking, at some price of higher wages, but they also push some of the social cost of above-clearing wages to everyone else. If and to the extent that principal-agent problems are otherwise unavoidable, its socially costly remedy may not be considered a market failure: there may be no way to make anyone better off (the unemployed) without making someone else worse off (well-paid employed and satisfied employers). But even if efficiency wages were to destroy no welfare, they certainly redistribute it. Here again, as in (p. ) or (p. ), people will be rewarded and punished *probabilistically* (not deterministically) and *out of proportion* to the marginal contributions they made --- or could make --- to the market economy. ##### Winner-Take-All. \\phantomsection [\\[sec:winner-take-all\\]]{#sec:winner-take-all label=&quot;sec:winner-take-all&quot;} Five paradigms and stylized dynamics of today&#39;s economy point to the possibility of runaway social inequalities that may result in distributions much akin to dominance hierarchies, where whoever is at the top reaps most or all of the benefits [@Frank1996]. 1. \\phantomsection [\\[itm:non-linear-returns\\]]{#itm:non-linear-returns label=&quot;itm:non-linear-returns&quot;} *Non-linear returns to scale* in indivisible human capital may disproportionately reward highly-skilled workers, as demand for their skills increases in the knowledge economy and they cannot be replaced by several less-skilled workers. [^68] [^69] Similarly, some highly-skilled work can easily by scaled up (an algorithm can easily be deployed millions of time), but many other occupations cannot be easily scaled as production reaches a physical limit (a hairdresser can only do so many haircuts a day) (originally @Rosen1981, recently @Taleb2007). Given the [eu]{acronym-label=&quot;eu&quot; acronym-form=&quot;singular+short&quot;}&#39;s proclaimed goal to become the leading knowledge economy in the world, it is to be expected that non-linear returns to scale in indivisible human capital will further increase [@Commission2007]. *Baumol&#39;s cost disease* is a related, but inverse concept. According to [@Baumol1965], some sectors (such as manufacturing) enjoy faster productivity growth than others (such as nursing), but, competing for the same laborers, both sectors must raise salaries. In violation of (neo)classical dictum, the wages of, for example, nurses rise, even though they have --- supposedly --- not concomitantly gained in productivity. Much of these sectoral productivity gains are reflected in similarly increasing non-linear returns to scale in human capital: as the indivisible innovations (say, laser welding) of engineers are easily scaled up in manufacturing, such innovations are absent in nursing and would resist scaling. If labor is, as [@Baumol1965] assume, in fact, (p. ) workers will be free to choose jobs with above-linear returns to scale until pay equilibrates at the same level across scalable and non-scalable work. In this scenario, diverging productivities are priced into *sectoral costs*, not *worker pay*. As a flip-side to diverging pay from above-linear returns, unscalable sectors will either disappear or become *relatively more* expensive to *consumers* (as seems to be the case with nursing). is, of course, an implausible assumption. If workers cannot, in fact, freely choose to work in scalable (computer engineer) or unscalable occupations (hair stylist), those in unscalable occupations will bear the brunt of diverging productivities in lower relative pay. The truth, as often, will lie somewhere in between, and greatly depend on circumstance. Some of the divergence between scalable and unscalable occupations will fall on workers, some on consumers and much will be split. [^70] 2. [\\[sec:cumulative-causation\\]]{#sec:cumulative-causation label=&quot;sec:cumulative-causation&quot;} *Path-dependent rewards and cumulative causation* may also let winners take all or most. If small initial state differences in performance lead to additional opportunities, initial inequality will be reinforced (@Jackson1968 [@Merton1988] recently popularized by @Gladwell). [^71] This pattern of path-dependent or cumulative causation is often observed in educational systems, particularly in those which track students early (as in Germany) or where social permeability is low (as in much of Europe) [@OECD2006]. 3. [\\[sec:network-effects\\]]{#sec:network-effects label=&quot;sec:network-effects&quot;} *Self-reinforcing network effects* occur where economic activity occurs along homopholous networks with a scale-free graph distribution (for an introduction to graph theory, see @Kleinberg-2009-oz). [^72] As actors (nodes) opt to interact with similar people (homopholy, for example, @Mcpherson2001), and opportunity (innovation) spreads (cascades) only among tightly nit groups (clusters) [@Bass1969] resulting utility (degree) distributions will be decidedly non-normal (scale-free, or fractal [@Mandelbrot2004]). Whenever features of economic consequence [^73] permeate through these networks of tightly clustered, self-similar nodes, opportunities and rewards will be a function of that same power-law distribution. Inequality, by the very structure of modern society, will be excessive and self-reinforcing [@Cozzi2009; @Keller2005; @Andriani2007]. A similar dynamic, applied to economic activity in space, is implied in the agglomeration and scale effects of (p. ). ##### Different Budget Constraints. \\phantomsection [\\[sec:different-budget-constraints\\]]{#sec:different-budget-constraints label=&quot;sec:different-budget-constraints&quot;} The magic of the (p. ) works over *given* allocations. In the real world, wealth and income disparities cause market participants to have different budget constraints. A higher budget constraint will inflate their willingness to pay and a lower budget constraint will deflate their willingness to pay, both at constant levels of utility. As distributions in the real world are not a blank, egalitarian slate, the demand and supply curves are distorted by differential budget constraints. Voluntary exchange no longer necessarily equilibrates at the pareto-optimum of *utility*, but at the pareto-optimum of *willingness*, and thereby *ability to pay*, a very imperfect and distorted proxy. If anything, this glossed-over difference between absolute *utility* and budget-dependent *willingness to pay* is the original, logical sin of neoclassical welfare economics. On the one hand, different budget constraints distort prices without any informational gain for @Hayek1931&#39;s superior information processing system: budget-distorted willingnesses to pay are *misinformation*. The old computer science adage applies here, too: *garbage in*, *garbage out* (GIGO). A market that equilibrates at multi-million yachts for people with outsized budgets and at malnutrition for others with very small budgets may, be formally pareto-optimal, but it may --- among other things --- not be utility-efficient in any meaningful way. [^74] On the other hand, such distortions of individual utility are precisely the sticks and carrots that incentivize homo economicus in market economies. Market economies reward individuals by letting them amplify their utility signals with a larger budget constraint and they punish individuals by forcing them to subdue their signals with a smaller budget constraint. These informationally useless spillovers from one exchange (Steve Job&#39;s inventions) to other, unrelated exchanges (Steve Job&#39;s consumption of yachts) are not merely a side-effect of market economies, they are their motivational essence. As original sins go, they can never be redeemed in full --- at least not in this world. So it is with the dirty little secret of neoclassical welfare economics: to equalize all budgets at all times would be to abandon a market economy to the fullest. Still, neoclassical economists and everyone else who relies on the first theorem must at least repent this sin by always confessing to it, and by highlighting its normative and policy implications. Else may not await purgatory, but lies ideology. We must not seal off, but open up an intellectual edifice to its criticism, we must not assume away but render transparent its inherent contradictions. ##### Diminishing Marginal Utility. \\phantomsection [\\[sec:diminishing-marginal-utility\\]]{#sec:diminishing-marginal-utility label=&quot;sec:diminishing-marginal-utility&quot;} With each additional unit of goods and that people gain, the added utility may fall (theoretically by @Lerner1944 [23], recent empirical support from @Ng-1997-aa [@Veenhoven-2000-aa; @Nickell2008]). Highly inequitable market outcomes will still be formally pareto optimal, [^75] but may leave great Kaldor-Hicks improvements unrealized as the poor stand to gain greater marginal utility than the rich would have to give up at their higher levels of consumption. In short utilitarian slogan, given diminishing marginal utility, perfect markets do *not* yield &quot;the greatest good for the greatest many&quot; [@Mill1863]. ##### Positional Externality. \\phantomsection [\\[sec:positional-race\\]]{#sec:positional-race label=&quot;sec:positional-race&quot;} Thorstein [@Veblen1899] has suggested that people consume excessively not merely to fulfill some manifest function inherent to the good purchased, but that they buy expensive things to &quot;heighten or reaffirm social status&quot; [@Merton-1968-aa 123]. Veblen goods are bought not *in spite of*, but *because* of their price, which is ideally publicly known and serves to communicate wealth and status to others. People consume conspicuously based on a *relational* rationale: what matters is what *other* people think about the cost (or sophistication) of a Veblen good. If people consume conspicuously --- at least partly --- to display status, the rationale is also *relative*: what matters is how much you spend in relation to what other people spend. Conspicuous consumption can then --- at least partly --- be understood as *positional* consumption: people can elevate their relative status if, and to the extent that they spend *more* than their peers. For this positional motivation for consumption, only *relative* prices (and qualities) matter, not absolute cost or utility. Positional consumption can be modeled as a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;}, as in . [^76] +:------------+:-+:-------+:--------+:-+ | | | | | | +-------------+--+--------+---------+--+ | | | Buy VW | Buy BMW | | +-------------+--+--------+---------+--+ | \\cline{3-4} | | | | | +-------------+--+--------+---------+--+ | | | | | | +-------------+--+--------+---------+--+ | \\cline{3-4} | | | | | +-------------+--+--------+---------+--+ | | | | | | +-------------+--+--------+---------+--+ | \\cline{3-4} | | | | | +-------------+--+--------+---------+--+ : Positional Consumption as a Prisoner&#39;s Dilemma[]{label=&quot;tab:pd-positional&quot;} \\scriptsize{The Joneses and the Does are peers, positional consumers and in the market for a new car. Payoffs are the net of cost of car ($VW=0$, $BMW=-5$) and status gain from driving the \\emph{relatively} more expensive car ($+10$ for superior, $-10$ for inferior, else $0$). Larger payoffs are better.\\\\ Prices of the BMWs are inflated so as to include the societal costs of wasteful consumption, here, as in an ideal world, accruing only to the buyer.} In a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;} of positional consumption, *more* excess will always be a strictly dominant strategy, and the unique Nash equilibrium. The social welfare optimum of mutual moderation will not be reached. People would derive same utility at collectively lower levels of consumption, but cannot do so for fear and anticipation of other players cheating by unilaterally consuming more. [^77] Consumers of Veblen goods may get stuck in a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;}, racing to keep up with others [@Frank1987]. Positional consumption wastes resources because it imparts no additional utility and exerts a negative externality on others by devaluing their purchases (a process known as *expenditure cascades* *ibid.*). ##### Monopsony Employers. \\phantomsection [\\[sec:monopsony-employers\\]]{#sec:monopsony-employers label=&quot;sec:monopsony-employers&quot;} When few, big firms face many, unorganized workers, labor markets may turn monopsonistic and cause both a welfare and a distributive effect. [^78] Welfare : is lost as some workers, facing lower wages stay home, who might otherwise have been gainfully employed at a higher, but still profitable wage. Under this monopsony [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}, some otherwise pareto-improving exchanges are not undertaken. Redistribution : . Further, compared to a competitive labor market, economic surplus is redistributed from workers to employers. This power imbalance between labor and capital is arguably one of the deepest contradictions that capitalism created. The free market, universalizing the commodity form, treats capital and labor as equal factors of production. Capital and labor, are fundamentally *not* equal factors of production and have vastly different bargaining power for at least three reasons: 1. Historically --- and to date --- capital tends to be *concentrated*: few have capital. By contrast, labor was historically, and is to date, greatly dispersed: many are workers. Larger groups with widely dispersed interests are harder to organize [@Olson-1971-aa]. Industrial production requires the cooperation and coordination of many, often thousands of workers who face collective action problems as well as colossal transaction costs. In the absence of institutions fostering collective action, employers may be able to permanently suppress worker cooperation by attracting individually rational defection. Collusion here, as always in atomistic markets --- for better or for worse --- is a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;}. 2. Capital is *easily cooperated and coordinated* into production, if pooling of resources over several capitalists is necessary at all. Almost by definition, [^79] any market economy will provide the institutions to pool capital (such as a public company) and match small owners with large projects and large projects with small owners (for example *convenience denomination* by financial intermediaries). By contrast, labor is (again increasingly) unorganized. 3. Capital can afford to lie idle and forgo a rent. Owners can instead convert it into the goods and services of a modern economy. Labor cannot afford to be unemployed, as consumption possibilities would then be reduced to the outputs of autonomous subsistence production, which are far below those produced under separation of labor. Not trained in, and often prevented from subsistence production, the worker faces the very real possibility of starvation if she does not enter into an agreement with capital. In economic terms, labor supply will always be *relatively less price elastic*, compared to capital supply. As a result, the free market, treating labor and capital equally as commodity, instituted a systemic power imbalance. #### Redistributive Policy. \\phantomsection [\\[sec:redistributive-policy\\]]{#sec:redistributive-policy label=&quot;sec:redistributive-policy&quot;} Government has a number of tools to redistribute market outcomes. These tools differ in effectiveness and efficiency: some alleviate only some kinds of market inequality (for example, , p. ), and some are very costly for any given increment of resource redistributed (for example, p. ). ##### Price Controls. \\phantomsection [\\[sec:price-controls\\]]{#sec:price-controls label=&quot;sec:price-controls&quot;} Government can respond to inequities by *regulating* price ceilings (such as rent control, in ) or price floors (such as minimum wages, in ). As intended, binding price controls shift welfare: consumers gain in surplus from price ceilings, producers gain in surplus from price floors (column 3, row 6 in ). But they gravely lack in efficiency and effectiveness. ![The [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} and distributive effects of a price ceiling with unit-elastic demand and supply (for example, of housing)](price-ceiling){width=&quot;100%&quot;} \\scriptsize{Compare with \\nameref{fig:supply-demand} in \\autoref{fig:supply-demand} (p.~\\pageref{fig:supply-demand}).} [\\[fig:price-ceiling\\]]{#fig:price-ceiling label=&quot;fig:price-ceiling&quot;} ![The [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} and distributive effects of a price floor with unit-elastic demand and supply (for example, of labor)](price-floor){width=&quot;100%&quot;} \\scriptsize{Compare with \\nameref{fig:supply-demand} in \\autoref{fig:supply-demand} (p.~\\pageref{fig:supply-demand}).} [\\[fig:price-floor\\]]{#fig:price-floor label=&quot;fig:price-floor&quot;} Efficiency. : In addition to the desired, redistributive zero-sum component, price controls also destroy welfare: much like taxes (), they cause a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}. Price controls are a negative-sum allocation. Ceilings push prices (say, rents) below equilibrium levels, decrease supply (of flats) and increase demand (for flats), causing excess demand. Some potential tenants cannot find good housing, even though they would be willing to pay a higher rent or some potential landlords will not built or improve housing, even though willing tenants could be found. Conversely, floors push prices (say, wages) above equilibrium levels and increase supply (of labor) and decreases demand (for labor), causing excess supply (unemployment). Some workers cannot find gainful employment, even though they would be willing to work for lower pay or some potential employer will not hire an available worker, because her cost (wage) outweighs her utility (productivity). Under binding price ceilings or floors, some otherwise pareto-improving exchanges will not be undertaken, causing decrepit real estate [^80] or unemployment, respectively. Such [dwls]{acronym-label=&quot;dwl&quot; acronym-form=&quot;plural+short&quot;} are the opposite of gains from trade, they invariably reduce to total consumer (producer) surplus of people able to buy (sell) goods at prices lower (higher) than their reservation price. The size of the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of price controls depends on the relative price elasticities of supply and demand (just like the , p. ). The more price inelastic demand, the smaller is the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of price floors: even at pushed-up prices, few buyers will be able to exit the market and almost all sellers will find buyers. Conversely, the more price inelastic supply, the smaller is the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} or price ceilings: even at pushed-down prices, few sellers will be able to forego sales and almost all buyers will find sellers. In the real world, and depends on many factors (p. ). In housing markets, supply (landlords) may be inelastic in the short run (for existing housing), but very elastic in the long run (for improved, or new housing). [dwls]{acronym-label=&quot;dwl&quot; acronym-form=&quot;plural+short&quot;} (or decrepit housing) are probably sizeable. In labor markets, too, demand (employers) will likely not be price inelastic across the board. Often, labor can be substituted by capital. Here, too, [dwls]{acronym-label=&quot;dwl&quot; acronym-form=&quot;plural+short&quot;} (or unemployment) will be sizeable. In any case, it will be very hard for states to gauge elasticities and calibrate price controls accordingly to specific industries, time periods and regions. [^81] Effectiveness. : Price controls are also limited in their effectiveness. They may, in principle --- if at colossal cost and complexity --- redress some of the inequities of markets (p. ), (p. ) and (p. ). Short of a near planned economy, they do little to dampen s (p. ): consumers can simply buy different (more extravagant) categories of goods, or more of the same to fulfill positional impulses. They can also hardly reign in on (p. ). By definition, their demand for labor is highly price elastic, given plenty of alternatives. Worker supply of labor, absent another employer, is almost perfectly price inelastic. Governments should, however, respond to monopsony employers in labor markets either by anti-trust action or by leveling the playing field. Regulating a right to strike and the right to form trade unions can put monopoly employees opposite monopsony employers in collective bargaining. [^82] Employment protection and labor contract regulation can also alleviate the power imbalance in labor markets. ##### Affirmative Action. \\phantomsection [\\[sec:affirmative-action\\]]{#sec:affirmative-action label=&quot;sec:affirmative-action&quot;} To a limited extent, governments can also respond to winner-take-all markets with affirmative action or equal opportunity legislation. ##### Redistribution by Fiscal Policy \\phantomsection [\\[sec:fiscal-redistribution\\]]{#sec:fiscal-redistribution label=&quot;sec:fiscal-redistribution&quot;} The principal redistributive tool of government is a progressive tax, by which government collects more from the rich to pay for public expenditures, and/or finances handouts to the poor. [^83] ##### Redistribution by Monetary Policy \\phantomsection [\\[sec:distributive-effects-of-inflation\\]]{#sec:distributive-effects-of-inflation label=&quot;sec:distributive-effects-of-inflation&quot;} Monetary policy cannot effectively mitigate inequitable market outcomes. Inflation and deflation redistribute wealth, but not between the rich and the poor. Instead, inflation redistributes wealth between credits denominated in real terms (for example, house ownership), debts denominated in nominal terms (for example, a fixed-rate mortgage) and credits denominated in nominal terms (for example, fixed-rate pension) and debts denominated in debts denominated in real terms (for example, a futures contract) as summarized in . \\centering ![The distributive effects of inflation, with some examples[]{label=&quot;fig:distributive-effects-of-inflation&quot;}](distributive-effects-of-inflation){#fig:distributive-effects-of-inflation width=&quot;100%&quot;} Such arbitrary redistribution does not correspond to of markets (p.) and cannot be defended on normative grounds. [^84] Additionally, redistributing by monetary policy causes severe (p. . Today, as financial products have diversified, redistributing based on denomination and between creditors and debtors will have increasingly arbitrary results. [^85] Monetary policy on distributive goals, as on (p.), shines by staying neutral. It must strive to maintain price stability to avoid any arbitrary and undue distributive effects that would otherwise interfere with the distributive dynamics of state and market. Consistency Over Time: Saving Tomorrow&#39;s Pie {#sec:time} -------------------------------------------- #### The Human Condition in Time Human beings are myopic planners [@KahnemanTversky1979]. They discount the future hyberbolically; the more remote a future event is, the more humans discount its rewards (@Ainslie1975, @Thaler1981). Human beings also succumb to herding: they do as others around them do. Both these frailties can lead to time inconsistency, where we simultaneously hold incompatible preferences for our present and future selves, for example when we would like to retire somewhere warm and mild, but also want a new car every three years. To be time consistent, we have to do unto future generations (or selves) as we would have them do unto us (the Golden Rule of reciprocity). Applied to the material world, we have to save --- at least --- at that level, which allows the highest constant level of current and future consumption (@Phelps1966a, @Solow1956). If we are more generous to our children, or consider technological innovation to be endogenous, we might have to save even more. Time consistency bears both on equity and efficiency. An *equitable* policy ensures same (discounted) [^86] utility for people living today vs. people living in the future. An *efficient* policy ensures maximum (discounted) growth over all periods (along the long-term growth path of the economy). #### Interest \\phantomsection [\\[sec:interest\\]]{#sec:interest label=&quot;sec:interest&quot;} In the exchange economy, capital markets align preferences of present and future generations (and selves) by compensating savers with an interest. Ideally, risk-free interest rates, [^87] and other (hedged) returns on capital (for example, stock market indices) equilibrate at that level, where future marginal utility equals present marginal cost of saving. At this intertemporal optimum, [^88] we follow the Golden Rule: present generations (and selves) *do* unto future generations (and selves) as vice versa. Under the assumptions of neoclassical orthodoxy, efficient capital markets should guarantee *some* degree of time consistency. ##### Garbage In, Garbage Out. But because markets are ultimately *aimless* and operate on (assumed!) *exogenous* preferences, the *formal* ability of capital markets for intertemporal optimization is strictly limited. Interest rates --- as all other prices --- are elegantly aggregated (time) preferences of people. If these preferences are inconsistent, or amoral, so, too, will be the resulting market &quot;optimum&quot;. As in statistical analyses, no matter the algorithm, the quality of the output depends strictly on the quality of the inputs. People may be relatively consistent --- or at least our best bet --- to gauge some components of observed discount rates, including the likelihood of exogenous growth (innovation), or continued human existence (compare footnote [\\[fn:3components\\]](#fn:3components){reference-type=&quot;ref&quot; reference=&quot;fn:3components&quot;} (p. ). For these components, people may input reasonably good &#39;data&#39;, and markets will output reasonably good prices. By contrast, the &#39;data quality&#39; of *pure discount component rate of the future* --- discounting future utility simply because it lies in the future --- is deeply questionable for reasons (compare footnote [\\[fn:3components\\]](#fn:3components){reference-type=&quot;ref&quot; reference=&quot;fn:3components&quot;} (p. ): 1. *Inconsistency.* People are, as cognitive psychology has shown, *time inconsistent* even in their own lives, and the short- to medium term (@Ainslie1975, @Thaler1981). If they are similarly inconsistent when deciding how much saving to supply at any given interest rate, *overall* interest rates will be distorted. Myopic individuals will *cause* the very inflated interest rates meant to keep their short-sightedness in check, and overall saving will be too low. [^89] 2. *Morality.* As [@Samuelson2005 54] has recently reminded us, *term uncertainty*, especially pure discounting of the future is a *normative concern*. Just how much we value the welfare of future generations and selves is a *moral* judgement. There is nothing in our nature or our environment that could positively tell us what a *true* rate would be, there are only normative judgments of what a *right* rate should be. Once people have agreed on such a rate, markets can do the @Hayek1931ian number crunching. But markets, because they will accept *any* rate as input, cannot solve this moral problem for us. Instead, only a democratic polity --- and its government --- can decide the pure discount rate. People often say they want their children to live a *better* life. If similar sentiments are deeply-held and widespread, people might democratically decide to save *more* than intertemporal *equity* demands. If indeed, we shall give to our children *better*, than we ourselves received, we are at least looking for a *Kaldor-Hicks* (not Pareto!) improvement without and above the equilibrium capital return. Even ignoring such substantive concerns, real world capital markets are also plagued by *formal* intertemporal failures, causing their own costly and inequitable time inconsistency. The capitalist algorithm not only receives bad inputs, but it is also ridden with bugs, that fail both in the short and the long term. #### Short-Term Intertemporal Failures {#sec:short-term-inconsistency} Economic activity tends to fluctuate in the short-term. ##### Business cycles are periodic fluctuations, typically relatively mild in amplitude and slope. They are endogenously caused by lumpy, lagged decision-making of market participants, for example in inventory cycles. [^90] According to the dissenting, minority view, business cycles are *true* shocks that may or may not be endogenously amplified, but are exogenous in origin (@Kydland1982). [^91] ##### Bubbles and panics can lead to abrupt and dramatic fluctuations in economic activity, with great slope and amplitude. Bubbles and panics envelope market actors when they make their decisions based on the anticipated decisions of others, in a beauty-contest type game [@Keynes1936]. Similar rationales also emerge when making privately informed decisions creates a positive externality for other participants to free-ride on your inadvertently disclosed information [@Banerjee-1992-aa]. [^92] ##### Fluctuations are Costly. Both types of fluctuations in economic activity are inefficient because they temporarily leave factors of production idle (overused) and increase periodic transaction costs. Fluctuations leave factors of production (for example, an assembly line) idle during the downturn or building up excess capacities during the upturn (for example, a real estate overhang). Additionally, fluctuations cause unnecessary adjustment costs (for example, storing equipment). The costs of short-term fluctuations are particularly virulent in labor markets, where hiring is expensive and unemployment can degrade worker morale. Aside from these material costs, short-term and uncertain employment (and economic prospects in general) cause stress and hardship. Such idleness or mania diverts the economy away from its (exogenously) long-term growth path and thereby slows down the economies progress along the growth path. By contrast, &quot;true&quot; exogenous shocks (such as an earthquake) or genuinely new information about future utility of present savings (for example, because of a new technology) is not a deviation from, but a *shift* of the long-term growth path. It is, for better or for worse, unavoidable. #### Short-term: State responses States can respond by regulatory, fiscal and monetary means to reduce the frequency, depth and duration of economic fluctuations. In the following, I discuss the state responses to a downward deviation from the long-term growth path of the economy. States should likewise, if with opposite sign, respond to upward deviations from long-term growth. ##### Regulatory States can reign in on short-term downturns simply by outlawing respective actions of market participants. [epl]{acronym-label=&quot;epl&quot; acronym-form=&quot;singular+short&quot;}, for example, can include lay-off protection, making it harder for employers to fire workers in a downturn. Other regulatory interventions include the bans on short-selling of recent 2007ff fame. Regulatory responses can be effective, but blunt instruments as they also affect economic activity outside of endogenous downturns. Generous lay-off protection, for instance, can cause a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of unemployment during upturns, keeping the economy under its long-term growth path. Regulatory interventions can also fail to distinguish between endogenous and exogenous fluctuations in the economy overall and some of its markets. [^93] As collateral damage, generous lay-off protection may prevent workers from moving to new, more productive firms and sectors (for example services rather than mining). Similarly, bans on short-selling may prevent or defer necessary market readjustment in case of real, exogenous price shocks (for example, a war in an oil-exporting country). ##### Fiscal Stimulus. \\phantomsection [\\[sec:fiscal-stimulus\\]]{#sec:fiscal-stimulus label=&quot;sec:fiscal-stimulus&quot;} States can also respond to short-term downturns by propping up aggregate demand, following @Keynes1936ian demand management. [^94] By definition, to push up aggregate demand, government must *dissave*. Fiscal stimulus must be paid out of newly issued debt, or prior savings --- but it cannot be paid out of increased *current* period tax revenues, for such hikes would cut private demand, as public demand expands. [^95] Still, even *deficit spending* will *crowd out* some private demand, as it soaks up savings in capital markets. Government bonds must be sold to someone, and that someone will not be able to spend or invest that same money elsewhere. Luckily for government (and all of us), it can target its spending wisely to those areas where it will have a maximum *multiplier effect*, that is, where demand begets as much more demand as possible. For example, when government procures a new railroad line, these construction workers will have more resources to spend on entertainment, or invest in a family home. Of course, government demand is not the only demand that multiplies; private demand also multiplies. However, because government (ideally) aims *not* to maximize profit, but overall economic recovery, it can seek out that spending that maximizes multiplication. Private demand, by contrast, will seek maximum profit at low risk, which, in economic downturns, is often found in a &quot;flight to safety&quot;, hoarding of cash or equivalents. A good stabilizer should go entirely into immediate consumption or investment, and possibly incentivize further dissaving. [^96] While easier to understand for the often maligned deficit spending type of stimulus, governments must trade off the multiplication effect versus the crowding out effect during *all* fiscal expansions. Even when (or rather, hypothetically, *if*) governments were to pay for stimulus &quot;out of pocket&quot;, these now dispersed savings were collected at an earlier time, ever since they crowded out that same amount in private demand. Maximizing multiplied demand, for every increment of demand crowded out is a challenge that government must always face in fiscal expansion. Where to best direct resources is an empirical question for econometricians, statistical physicists and other experts that I happily need not, and cannot engage here. *Automatic stabilizers* can support demand without legislative intervention. They include unemployment insurance [^97] and short-time working benefits, both of which smooth out (consumer) demand by substituting the market income of laid-off, or shorted workers. [^98] *Discretionary stabilizers* are often called for when bubbles, panics or amplified shocks cause output fluctuations too large and fast to be captured by automatic stabilizers. They come in the form of tax breaks, stimulus subsidies or public investment. They, too, rely on dissaving, and the incurred debt must be repaid in good times. *Side effects.* As all potent drugs, fiscal stimulus must be prescribed with great care. It has at least two dangerous side effects: 1. *Structural painkiller.* All stabilizers can disguise real needs for adjustment as cyclical problems, further diverting an economy from its equilibrium and long-term growth path. 2. *Playing favors.* Especially discretionary stabilizers are also prone to clientelism, that easily creeps in when legislatures pass often very targeted stimulus packages. When specific decisions are made, rather than general rules passed [@Weber-1918-aa], as discretionary stabilizers require, special interests are more likely to gain the upper hand. [^99] ##### Monetary Stimulus. [\\[sec:monetary-stimulus\\]]{#sec:monetary-stimulus label=&quot;sec:monetary-stimulus&quot;} Government can also prop up the economy in the short run by monetary stimulus. [^100]. Government injects more cash into the economy, to make up for liquidity frozen up during the downturn, thereby (p. )[^101] The monetary contraction in the course of an endogenous downturn can best be described as a general loss of confidence in the profitability of future projects (such as investments). As market participants become squeamish about economic prospects in general, they hoard cash (or equivalents) or flee to safety (such as gold, or --- before 2009 --- government bonds). [^102] Less funding is available even for the most profitable and unrisky projects and the lack of liquidity can render (balance sheet) solvent firms (cash flow) insolvent. The money supply contracts. When these defaults further feed the market panic and cause more debts to sour, a self-reinforcing debt-deflation crises may ensue. A monetary stimulus bolsters market confidence by essentially placing an optimistic, massively government-backed bet. When central banks resort to quantitative or qualitative easing, they declare whichever assets they buy or collateral they accept as worthwhile (profitable) investments by fiat. When, if and to the extent that markets buy into this stunt of optimism, they will be increasingly willing to lend the injected (and their own) money again, too. Monetary stimulus has some drawbacks, too. Monitoring the money supply in the economy is very difficult and imprecise, and related policy imperatives are incompletely understood. To avoid inflation, central banks must contract their money supply again as soon as the monetary stimulus has worked, and market liquidity has recovered. The timing and calibration of interventions to maintain a constant money supply are crucial, but difficult to get right. Monetary stimulus, too, can disguise and defer unavoidable realignment when the cause of the downturn is exogenous, such as a true price shock (as for example when revised Greek fiscal numbers became available in 2008). When the long-term growth path is shifted (downward), the money supply *must* contract in absolute terms as the underlying economy, too, will contract, or at least grow slower than previously expected. Monetary stimulus does not allow economies to live beyond their real, exogenous means for long; it just defers the pain to a later contraction or risks inflation, which in turn (p. . #### Long-Term Inconsistency {#sec:long-term-inconsistency} ##### Market problems The market institution of equilibrium interest rates may fail to guide humans to temporal consistency in the long run for three reasons. 1. (Exogenous) growth theory discussed so far assumes technology to be constant, or exogenous: as if the rate at which new technology is discovered cannot be altered by humans. Accordingly, once a maximum steady-state output is achieved, further *capital deepening* or other policy will be inconsequential for growth. Exogenous technological innovation appears unreasonable in a knowledge-based economy (as recently endorsed in @Communities2009). When we *can* endogenously improve our creativity, we might have to save more to achieve intertemporal Kaldor-Hicks improvements. Additionally, markets may fail to adequately save for and invest in fundamental [rnd]{acronym-label=&quot;rnd&quot; acronym-form=&quot;singular+short&quot;}, because innovation is often a (p. ). Market innovation may also be hampered by under asymmetric information (p. ). 2. Ancillary conditions frequently observed in post-industrial economies imply *real* (public or private) and require offsetting, greater *nominal* savings (p. ). They include aging, [^103] structural unemployment, [^104] or, equivalently, (p. ), underprovided public goods (basic research?) and overused common goods (@Stern-2006-aa: climate change!). Markets can, by definition, not provide these goods or solve these problems, and to the extent that government (or lovers) have neglected to do so in the past, these failures constitutes real dissavings. 3. Market return rates on capital set a saving rate by equilibrating *self-interest*. Savings rates, however, are a public good, which cannot be provided out of self-interested exchange. One person&#39;s saving bestows a positive externality on other people in two ways: 1. When technological change is held to be *exogenous*, other people will benefit from capital deepening and approximating of steady-state growth. 2. When technological change is allowed to be endogenous, these benefits include greater economy-wide productivity and innovation and will be even larger. Markets, when left to their own devices, will save less than optimal because not all of the societal benefit can be recouped in interest payments. ##### Government Saving. \\phantomsection [\\[sec:government-saves\\]]{#sec:government-saves label=&quot;sec:government-saves&quot;} If the polity has democratically prescribed a savings rate and/or the market fails to save at positively optimal levels, government can prop up saving by regulatory and fiscal means. In *regulatory* policy, government can mandate citizens to save into a pension or other saving scheme. [^105] Mandatory pensions and similar financial products solve time inconsistency both at the individual level (present and future self) and the societal level (present and future generations). The difference between fiscal and regulatory policy I draw in (p.  is a bit murky and overdrawn, when it comes to saving. Here is my attempt to draw it anyway: Regulatory : I treat policy as regulatory, when it ties made contributions to *entitled benefits* later in life. Much like when it is fencing the commons, government guarantees legally enforceable, quasi-property in both private and public pension schemes. Such a regulatory pension scheme, tying benefits to contributions, organises intertemporal production (saving) and distribution (dissaving) in a *single* institution --- much like the market economy does. [^106] Fiscal : I treat a policy as fiscal, when contributions do *not* entitle payers to specific benefits. By definition, this is what taxes do. In fiscal policy, government can encourage private saving with clever subsidies or save resources itself by raising more taxes than spending in the current period. Government can invest budget surpluses in publicly run projects such as education or infrastructure, or privately run enterprises in [swfs]{acronym-label=&quot;swf&quot; acronym-form=&quot;plural+short&quot;} (here again, the public-private is irrelevant to the saving component (p. ). Such a fiscal intertemporal regime keeps intertemporal production (saving) and distribution (dissaving) apart. Each step in the intertemporal deal requires a separate political decision. For example, government will have to decide when to auction off [swfs]{acronym-label=&quot;swf&quot; acronym-form=&quot;plural+short&quot;} and what to use the revenue for. Convergence Over Space: Growing the Pie Smoothly {#sec:space} ------------------------------------------------ #### The Human Condition of Space Humans have evolved in small, intimately known groups of hunters and gatherers of no more than a few hundred individuals, covering a relatively small area of Earth in their lifetime (popularized, reviewed in @Diamond1997). In evolutionary terms, mass society and fast travel are recent developments. And so we may be ever disposed to parochially define our context in terms of locale (and ethny) [^107] (for example, @Van-den-Berghe-1981-aa). When we let our parochial sentiments make policy, economically costly autarky results. We are rich and prosperous, because at the national, regional and global level we are integrated by organic solidarity of *functional* differentiation, not the mechanical kind of locale and ethny [@Durkheim-1893-aa]. Under modernity, autarky is always regression. The cosmopolitan mindset transcends our parochial heritage, and defines our group and place in the most encompassing, radically modern way: species homo sapiens on planet Earth. This is the jurisdiction of human rights, the scope of science and the marketplace of economic liberalisation. However, modernity and prosperity have not arrived nor progressed simultaneously everywhere, but the have swollen very differently (p. ). Our world, today, is a very unequal place. #### Market Solutions \\phantomsection [\\[sec:trade\\]]{#sec:trade label=&quot;sec:trade&quot;} Markets offer the institution of trade to mediate between our parochial tendencies and the cosmopolitan demands of our modern world. Trade kindles our self-interest, binds us to strangers far away, increases our common welfare and, sometimes, makes us more equal, too. In the following I (very) briefly describe four mainstream trade theories and highlight their welfare and distributive effects, but will have to ignore much real world complexity and empirical refinement (for another review with empirical evidence, see @Beckfield2009). I here apply the paradigms of trade theory to commerce *within* an idealized, closed economy, even though trade is traditionally understood as *inter-state* exchanges between two or more open economies. However, some of the same basic dynamics also govern welfare and distribution *inside* one closed economy. Of course, those institutions that differ from one open economy to another --- including monetary policy, trade barriers and factor mobility --- do not apply to trade within the closed economy: there is only one money, no tariffs or quotas, and great (if not perfect) capital and labor mobility. But the overall dynamics of divergence and convergence are always the same. In fact, thinking, as we do, *differently* about commerce *within* a country, and trade *between* countries is an exercise in cognitive compartmentalization (Gabbard 2010), and it often borders on mercantilism or reeks off nationalist sentiment. ##### Four Trade Theories \\phantomsection [\\[sec:trade-theories\\]]{#sec:trade-theories label=&quot;sec:trade-theories&quot;} Trade is always and everywhere the voluntary exchange between people who differ in location, ability or something else of economic value. Trade is, in short, the mode of a market economy. We know of at least four different theories to explain growth, convergence and divergence under trade: Absolute Advantage : [\\[itm:absolute-advantage\\]]{#itm:absolute-advantage label=&quot;itm:absolute-advantage&quot;} Trade on absolute advantage occurs between at least two parties that can produce different goods at the cheapest cost [@Smith-1776-lq]. [^108] The parties will both specialize in whichever good they can produce more efficiently, and trade the surplus production (over their domestic demand) for goods other parties have an absolute advantage in producing. All parties benefit from greater overall productivity. Aggregate output increases: everyone gets richer. Crucially, when a party has no absolute advantage in producing any good, it will not trade at all. The benefits are divided according to productivity. In @Smith-1776-lqian trade, everybody gains, if not equally. It exploits exogenous productivity differences, but does not otherwise lead to convergence in productivities or prosperity. Comparative Advantage : [\\[itm:comparative-advantage\\]]{#itm:comparative-advantage label=&quot;itm:comparative-advantage&quot;} Trade on comparative advantage occurs between parties that produce different goods at *relatively* different productivities [@Ricardo1817]. [^109] A party will specialize in that good, for which they have to give up the least other production (opportunity costs). The party then trades the surplus production (over their domestic demand) for goods other parties have a comparative advantage in producing. Trade on comparative advantage increases overall output. A party will trade, even if it has no absolute advantage in producing any good. The benefits of trade are divided according to the terms of trade. In Ricardian trade, too, everybody gains, but not equally. It exploits exogenous productivity differences, but does not otherwise lead to convergence in productivities or prosperity. Factor Price Equalization : [\\[itm:FPE\\]]{#itm:FPE label=&quot;itm:FPE&quot;} According to the [fpe]{acronym-label=&quot;fpe&quot; acronym-form=&quot;singular+short&quot;} theorem, parties trade with parties that differ in their factor endowments (such as labor and capital) [@Stolper1941]. [^110] A party will specialize in producing goods intensive in their more abundant factor and trade the surplus production (over their domestic demand) for goods other parties have specialized in. Overall output increases, as factors are put to the most productive use. Within the party, the relative factor returns change as trade commences. The relatively more abundant factor (in rich countries, capital) is in higher demand and reaps a higher return. The relatively less abundant factor (in poor countries, low-skilled labor) is in higher demand and reaps a higher return. Over the long run, according to [@Stolper1941] specialization continues until all factors are equally abundant in all parties, and command equal returns (prices). Between the parties, according to Hekscher-Ohlin trade, everybody gains, but not equally. It equalizes factor prices, but does not further convergence of endowments or prosperity. Immediately, specializing according to factor endowments can reinforce exogenous, pre-existing inequality: as a (human) capital-poor party opens to trade, the return on capital (education) [^111] may fall. Economies of Scale : [\\[itm:NTT\\]]{#itm:NTT label=&quot;itm:NTT&quot;} According to *[ntt]{acronym-label=&quot;ntt&quot; acronym-form=&quot;singular+short&quot;}*, parties trade with one another not to exploit any exogenous, pre-existing difference in productivities or endowments, but because specialization itself pays [@Krugman-1980-aa]. [^112] A party will specialize in the goods in which it is already specialized, or at the least, in which no other party is specialized to produce at competitive prices. Similarly, a group of geographically (or otherwise) clustered parties will specialize in one category of goods (food in north-western France) or one industrial sector (automobile in southern Germany), to benefit from specific resources (specialized labor) and networks (for example, supply chain, trade fair). In [ntt]{acronym-label=&quot;ntt&quot; acronym-form=&quot;singular+short&quot;}, overall output increases, as more production happens at higher scale and dense networks are efficiently shared. [ntt]{acronym-label=&quot;ntt&quot; acronym-form=&quot;singular+short&quot;} also implies significant distributive effects: whoever specializes first and is closely clustered, wins. This *agglomeration* may increase spatial inequality and counteract convergence. In extreme cases, economies of scale and network effects may breed monopoly or oligopoly producers, causing additional distributive effects and welfare losses. ##### Balance of payments In the short and medium term parties can defer all of the distributive effects of trade by running *current account deficits*, including trade deficits. Current account deficits are offset by *capital account surpluses*, including sales of domestic assets and issued debt. In the long run current account deficits (and counterparty surpluses) built up untenable imbalances and can trigger *balance of payments crises*. At the end of the day, current account deficits cannot persist but must be (p. ). Running current account deficits may defer the distributive pain (at a cost), but cannot avoid it. In the final analysis, trade deficits, surpluses (and capital account deficits, surpluses respectively) are meaningful only net of trade with *all* other parties. If party A has a a trade deficit with party B, which has a trade deficit with party C, which has a trade deficit with party A, overall current accounts may be balanced. Only the net deficit trade distilled into capital account surpluses (debt or foreign ownership) matters for economic imbalances. This can be explained with reference to personal finance. All people except for suppliers and employees run a &quot;trade deficit&quot; with their local supermarket; yet, as long as they have offsetting surpluses with other parties (for example, their clients), they may be financially sound. Only if they spend more than they earn (or own), will they be in trouble. ##### Adjustment costs \\phantomsection [\\[sec:adjustment-costs\\]]{#sec:adjustment-costs label=&quot;sec:adjustment-costs&quot;} All of the above four trade models assume costless adjustment of the economy. As some sectors wax and others wane in the course of specialization, factors of production (capital, labor) are transferred from one use to another with no cost. This is an unrealistic assumption. Specialized capital (machinery) or (trained) labor may not be useful in another industry without some retooling or retraining, if at all. Adjustment costs must be subtracted from the *welfare* gains of trade. To the extent that adjustment costs are concentrated in one party or industry, as they are likely to be, adjustment also has &quot;domestic&quot; *distributive* effects. People who have a stake in the industry favored by trade, such as trained workers and owners win. Workers and owners in declining industries lose. ##### Factor Mobility \\phantomsection [\\[sec:factor-mobility-trade\\]]{#sec:factor-mobility-trade label=&quot;sec:factor-mobility-trade&quot;} Modeling the spatial dimension of a closed market economy on international trade theories may appear as a bit of a stretch. The analogy works only to the extent that factors of production stick to place and industry. By definition, labor and especially capital face no *formal*, spatial boundaries *within* the closed economy. In a perfectly mobile labor and capital market, none of the above distributive effects would apply: factories and workers would fluidly and without cost move to wherever they can earn most, counteracting any spatial factor rent differentials. In the real world, neither capital nor labor is perfectly mobile, and to that extent, trade theory applies. [^113] Domestic and international trade are subject to essentially the same economic dynamics. They differ not dichotomously, but gradually, depending on factor mobility. If I have stretched the concept of international trade here, it is to show that the closed, mixed economy experiences the same welfare and distributive dynamics, and has, as I explain in the following section, found policy responses to mitigate them. #### Government Solutions Government can forge spatial convergence to counteract divergent trade dynamics by fiscal, and to a lesser extent, by regulatory means. ##### Regulatory Policy According to (neo)classical economic doctrine, to allow everyone to partake in and gain from trade government should &quot;fight factor market rigidities&quot; (a euphemism for letting wages fall and consumer prices rise). Two welfare state institutions are frequently blamed for wage rigidities: 1. Formal minimum wages or income-substitution benefits can establish effective price floors in labor markets and cause a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of unemployment, or, in the terms of trade theory, a group or region of people with no absolute advantage. [^114] 2. Powerful trade unions and corporatist style collective bargaining can make wages (especially downward) rigid. To further trade, or, equivalently, clear labor markets, government should avoid price floors and, if found downwardly rigid, tamper or bust trade unions. [^115] Both these measures will have adverse (vertical) distributive consequences, which government may counteract with fiscal transfers. Again, as I argued earlier, to distribute vertically (p. ). Equity can best be achieved by fiscal measures. In international trade, government can protect &quot;infant industry&quot; to remedy the distributive dynamics of Hekscher-Ohlin trade and to check agglomeration by setting import tariffs or quotas. Under the shield of protection, the economies accumulate capital (or high-skilled labor) until it becomes the more abundant factor and join international trade to participate (in arguably more value-adding) capital-intensive production. Alternatively, protection allows domestic firms or sectors to grow to a scale and density that allows them to reap returns to scale and network effects before they enter international competition. By definition, there are no tariffs or quotas *within* the closed economy, so government cannot *protect* infant industries. ##### Fiscal Policy Government can nurse infant industries, by pursuing industrial or structural policy. [^116] In industrial policy, government forges capital deepening, builds forward-looking infrastructure and favors &quot;picked winners&quot; in sectors and firms. Similarly, structural policy supports underperforming regions or sectors by granting subsidies or tax breaks. The toolset of industrial and structural policy is varied and complex, often centered on fiscal means, but also including a banking regime, ownership structure, corporate governance and institutions in education and training (for an impressive survey of different configurations of these interlinked institutions, see @HallSoskice-2001-aa). Government intervention in the economy is also rightfully controversial. Instead of nurturing infant, but promising industry, it can protect sclerotic, inefficient sectors doomed for &quot;creative destruction&quot; [@SchumpeterSwedberg-1942-aa]. In the worst case, industrial and structural policy succumbs to clientelism and turns into crony capitalism. Famously, *picking winners*, in industries and regions, is very hard and government may be particularly bad at it. Both the targeting and timing of government support for regions, industries and firms are tricky. Subsidies must be *targeted* at industries that later can successfully compete in an open market. Government often has limited knowledge or ability to make these calls, and clientelist politics easily creeps in. Subsidies must also be *timed* to reliably recede, to put sufficient pressure on industry to become competitive and to avoid windfalls. Phase out a subsidy too soon, and the industry dies. End it too late, or never, and you invite rent-seekers. More broadly and less targeted, government can address spatial inequities of trade by transfers payments. As subsidies, tax breaks or funding for local government, central government can compensate for adjustment costs, give side-payments to losers from trade and generally smooth structural change. Progressive transfers can also dampen self-reinforcing agglomeration and counteract suppressed factor returns. ##### Monetary Policy By definition, a closed economy only has one currency, and thereby, one monetary policy. Consequently, monetary policy cannot affect the spatial dimension of economic activity *within* that closed economy. ## The Means of a Mixed Economy &gt; *&quot;The revenue of the state *is* the state.&quot;*\\ &gt; --- Edmund @Burke1790 [111, emphasis added] To pursue the (p. ) of (p. ), (p. ), (p. ), (p. ) and (p. ), the mixed economy relies on an intact set of (p. ), (p. ) and (p. ) means. Effectively commanding regulation, taxation and fiat money is not trivial. To be effective, these institutions of the mixed economy must be designed to *anticipate* and *minimize* adverse interactions with the independent market economy. As is the defining feature of the mixed economy, (p. ), both in its policy *ends* and *means*. The Means of Regulatory Policy {#sec:regulatory} ------------------------------ Effective regulatory policy is (p. ) but (p. ), and always (p. ) with the reach of economic activity. Capability. : [\\[itm:capability\\]]{#itm:capability label=&quot;itm:capability&quot;} To regulate economic activity, government must command an effective monopoly on the use of force and possess the administrative capability to pass, monitor and enforce regulation. Regulation will often be exceedingly complex and require frequent modification and government must be equipped to keep up with the creativity and dynamism of private business. [^117] Restraint. : [\\[itm:restraint\\]]{#itm:restraint label=&quot;itm:restraint&quot;} On the other hand, the regulatory power of government must also be strictly limited by the rule of law, particularly the right to property, and the norms of good governance. Markets must be protected from arbitrary or unpredictable regulation of the economy that would disrupt their smooth functioning. Restrained government maximizes planning reliability for market participants, protects their confidence and avoids retroactive effects. [^118] Congruence. : [\\[itm:congruence\\]]{#itm:congruence label=&quot;itm:congruence&quot;} Lastly, the scope of regulation must match that of the economic activity in question. [^119] When regulation covers an area smaller than the respective market, *regulatory arbitrage* ensues. For example, when health and safety in manufacturing are regulated at the county level, factories will relocate to wherever the rules are laxest. Because manufactured goods can easily be moved, consumers in the strictly regulated county will buy (cheaper) goods produced under lax conditions. Regulation at the county level will be ineffective. When local jurisdiction have an incentive to attract manufacturing such as employment or tax revenue, standards may equilibrate at a the (low) Nash equilibrium in a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;}-type game. When jurisdiction and the mobility of factors and goods does not match, a *race to the bottom* may ensue. [^120] Conversely, when the scope of regulation far exceeds that of markets, government may overreach and violate norms of *subsidiarity*. For example, when health and safety of hairdressers are regulated at the national level, hypothetical regional or local differences in customer preferences would be negated for no compelling reason. People will usually not travel to get a haircut, and haircuts cannot be transported. Regulatory arbitrage is hence unlikely to occur. Local government is free to set training standards at whichever level its electorate sees fit, without fear of hairdressers relocating or customers fleeing to other, laxer jurisdictions. [^121] As @Bordo2011 put it, *this* is the &quot;*raison d&#39;être*&quot; [-@Bordo2011 4] for fiscal federalism, as defined by [@Oates1972]. In the closed economy, government can always regulate at the largest possible scope of markets, the national level. By definition, no relocation or trade beyond the closed economy are possible. Central government can devolve jurisdiction to local government according to market scope, as it sees fit. The Means Fiscal Policy {#sec:fiscal} ----------------------- To redistribute market outcomes, fund public goods and natural monopolies or to change market incentives, government relies on the treasury. These fiscal activities in (p. ) differ by and , summarized in (p. ): Base. : \\phantomsection [\\[itm:base\\]]{#itm:base label=&quot;itm:base&quot;} Fiscal revenue generation differs in its *nominal* base, or *what is taxed*. [^122] Schedule. : \\phantomsection [\\[itm:schedule\\]]{#itm:schedule label=&quot;itm:schedule&quot;} Obligatory transfers differ in their desired schedule. They can apply to everyone at a *flat* rate, *proportional*, or *progressive*, charging the fortunate above a linear tariff on their ability to pay (or , p. ). Differing in base and schedule, fiscal activities fall in three broad categories: \\phantomsection [\\[sec:levies\\]]{#sec:levies label=&quot;sec:levies&quot;} fund (p. ), including suboptimal saving. [^123] In english, they are also known as Pigouvian *taxes*, which is imprecise, because they are no proper taxes. In german, they are known as literally &quot;steering taxes&quot; (*Lenkungssteuern*), which is apt, because they are meant to change market activity. Pigouvian levies are *specific* in base: only whoever uses common good pays. Pigouvian levies --- ideally --- are scheduled at *marginal cost*: everyone pays for the increment by which she has exhausted the common good. Equivalently, everyone pays according to the marginal social cost they inflict on everyone else. &lt;!-- % \\subparagraph{\\hyperref[sec:common-good]{Common Goods} and \\hyperref[sec:natural-monopoly]{Natural Monopolies}} should both be financed out of specific levies. %For common goods, these take the form of Pigouvian levies, pricing in the costs of using the non-excludable, but rival commons. %For natural monopolies, these take the form of fees, ideally at average (not marginal) costs of production collected by the state or a publicy-owned firm. %As the rate of the levy is determined only by the costs of producing the respective good or service, a schedule does not apply. --&gt; \\phantomsection [\\[sec:fees\\]]{#sec:fees label=&quot;sec:fees&quot;} fund (p. ). Fees are *specific* in base: only whoever benefits from a natural monopoly pays. Fees are scheduled at *average* cost: everyone pays the same share of the overall cost, no matter how much she added in incremental cost (see footnote [\\[fn:why-ac-fees\\]](#fn:why-ac-fees){reference-type=&quot;ref&quot; reference=&quot;fn:why-ac-fees&quot;}, p. ). \\phantomsection [\\[sec:taxes\\]]{#sec:taxes label=&quot;sec:taxes&quot;} fund all remaining government outlays, including those for . Taxes, no matter their use, are *general* in base: everyone pays unconditionally, and without individualized return. [^124] Taxes can be scheduled to achieve *any* desired redistribution, even though most would understand redistribution to imply at least a proportional or progressive schedule. &lt;!-- % \\subparagraph{\\hyperref[sec:fiscal-redistribution]{Redistributive} taxes} should, by definition, be general in incidence and proportional or progressive on ability to pay, depending on the chosen equity norm. %A \\emph{specific} redistributive tax is a contradiction in terms: %redistributing only between some entities in turn constitutes a redistribution between the included and the exempted entities. %Moreover, specific inclusion or exemption of entities based on something other than the accepted equity norm (for example, ability to pay) conflicts with liberal-democratic norms of non-discrimination. --&gt; Taxes meant to raise general revenue to finance all remaining outlays (see , p. ) imply only a flat, or per-capita schedule. Funding, for example, , [^125] , [^126] \\hyperref[sec:fiscal-stimulus]{stimulus} [^127] or \\hyperref[sec:government-saves]{government saving} imply flat schedules: these programs are all about maximizing welfare, have universal coverage and justify no redistribution. However, [\\[sec:fiscal-redistribution-and-revenue-are-one\\]]{#sec:fiscal-redistribution-and-revenue-are-one label=&quot;sec:fiscal-redistribution-and-revenue-are-one&quot;} because redistributive taxes for equity and general-revenue taxes for welfare share the *same base*, they can also be rolled up in one combined schedule. In addition to cutting redundant administration, taxing only according to one schedule makes more sense. When two taxes have overlapping objectives, such as a general base, their parallel implementation may conflict. This is also the case for general revenue and redistributive taxes, simply because a flat financing of, for example, public goods, already embodies *one* --- possibly contested --- equity norm. #### A Norm of Ordoliberal Hygiene. \\phantomsection [\\[sec:ordoliberal-hygiene\\]]{#sec:ordoliberal-hygiene label=&quot;sec:ordoliberal-hygiene&quot;} These three categories of obligatory payments to the treasury --- Pigouvian levies, fees and taxes --- are different public policy beasts entirely, and must not be confused. &lt;!-- % \\cite{Dwyer2009} 340: %Indeed, Leicht and Fitzgerald (2006) argue that the middle class has been ‘lent what it should have been paid’ and that a middle-class lifestyle was maintained during a time of stagnant income growth for many at the cost of financial security. --&gt; &lt;!-- %\\paragraph{Redistributive Taxes and General Revenue are One} The remaining fiscal instruments with the goals to redistribute and raise general revenue are treated as one. %There is no meaningful distinction between taxes that are meant to redistribute (or its progressive element) and those (or that component), which is meant to finance public/common goods or collectivize risk. % %A separate treatment of the merely redistributive component of taxation and its revenue generating element is unnecessary because the population concerned, is, in theses cases, everyone (by definition), and hence same. % %We can then implement the degree of redistribution that is required over the entire range of general revenue generating taxes. %\\paragraph{A Norm of Ordoliberal Hygiene.} This typology of fiscally relevant state interventions into the market carries implications for the efficient design of a tax. %General and specific fiscal interventions follow, in fact, two \\emph{opposite} logics. %A Pigouvian tax or fee is meant to fall on those very market interactions they are raised to discourage or finance. %Pigouvian taxes \\emph{should} change relative costs in the market vis-a-vis the status quo to reduce otherwise overconsumption of commons. %An efficient local sewage system fee \\emph{will} fall only on actual private or business use of the system, and not on other activity, say, a sewage-free server farm. %General taxes to redistribute, pool risks or finance public goods, by contrast, should, to \\hyperref[sec:DWL]{minimize their deadweight loss} (desideratum \\ref{des:minimal-DWL}) on otherwise Pareto-optimal markets leave relative prices unchanged. %From this follows a norm of ordoliberal hygiene to keep these two types of fiscally relevant state interventions into the market apart. %Because I am not concerned with Pigouvian levies and fees here, I exclude them from the following in desideratum \\ref{des:ordoliberal-hygiene}. --&gt; [\\[des:ordoliberal-hygiene\\]]{#des:ordoliberal-hygiene label=&quot;des:ordoliberal-hygiene&quot;} A desirable tax should not include any Pigouvian or fee-like components. Neither Pigouvian levies nor fees should be designed to redistribute or raise *any* general revenue. Conversely a redistributive or general revenue tax should not alter behavior. General and specific fiscal interventions follow, in fact, two *opposing* logics: A Pigouvian levy or fee is meant to fall on those very market interactions they are raised to discourage or finance. Pigouvian levies *should* change relative costs in the market vis-a-vis the status quo to reduce otherwise overconsumption of commons. General revenue or redistributive taxes, by contrast, should leave relative costs and levels of market activity unaffected, otherwise, a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} ensues. An efficient local sewage system fee *will* fall only on actual private or business use of the system, and not on other activity, say, a sewage-free server farm. [^128] Of course, Pigouvian levies and fees sometimes cause undesired distributive effects. For example, taxes on fuel consumption often hit lower and middle class people hardest. Still, government should not alter a Pigouvian schedule, let alone issue offsetting subsidies, such as the German commuter tax relief (&quot;Pendlerpauschale&quot;). Instead, government should use general-base redistribution for equity. #### Redistribution and Revenue-Generation are One. \\phantomsection [\\[sec:fiscal-redistributionAndRevenueAreOne\\]]{#sec:fiscal-redistributionAndRevenueAreOne label=&quot;sec:fiscal-redistributionAndRevenueAreOne&quot;} On the other hand, this typology of fiscally relevant state interventions also suggests that redistributive and revenue-generating taxes for and provision of can be consolidated into one. The desired general incidence in both these classes of taxes implies that the universe of taxable entities is the same. Redistributive and general revenue taxes *should* be rolled into one for two reasons. economicus Administrative Ease. : The collection of any tax will be administratively costly. Implementing fewer taxes will be cheaper. The flat or proportional schedule of revenue generation can be easily included in a redistributive (proportional or progressive) schedule as a base rate. General Revenue and Redistribution Interact. : When two taxes have overlapping objectives, such as a general base, their parallel implementation may conflict. This is also the case for general revenue and redistributive taxes, simply because a flat or proportional financing of pooled risks and public goods already embodies *one* --- possibly contested --- equity norm. A flat or proportional financing of general revenue, could, for instance, conflict with efficiency desideratum des:low-price-floor in . Conversely, *levels* of risk pooling and public good provision are shown to bear equity consequences in . A desirable tax exclusively finances general revenue and redistributes through a single schedule. [\\[des:redistribution-and-revenue-are-one\\]]{#des:redistribution-and-revenue-are-one label=&quot;des:redistribution-and-revenue-are-one&quot;} #### Congruence. In addition to a well-functioning tax administration, tax collection also requires congruence between the economic activity being taxed and jurisdiction of taxation. In @Oates1972&#39; seminal formulation: &gt; *&quot;The result of tax competition may well be a tendency toward less than efficient levels of output of local services. &gt; In an attempt to keep taxes lows to attract business investment, local officials may hold spending below those levels for which marginal benefits equal marginal cost, particularly for those programs that do not offer direct benefits to local business.&quot;*\\ &gt; --- @Oates1972 [143] &gt; [^129] For example, when Pigouvian levies on pollution in aluminum production are set at the local level, but aluminum is widely traded, production will relocate to wherever taxes are lowest. When factors and/or goods are sufficiently mobile, tax levels will race to the bottom. This cooperation problem of taxation at the local level also applies to redistributive taxation where the rich will relocate to avoid progressive taxation. Even fees for natural monopolies will be affected: when sewage fees are set at the local level, other municipalities can attract more establishments by pricing their sewers at (initially much lower) marginal cost, rather than otherwise optimal average cost. As more people and firms relocate to the cheaper municipality, it eventually becomes unable to provide the natural monopoly at marginal cost. In the closed economy, tax competition is usually not a problem, as most taxes and vulnerable fees are set at the national level. [^130] #### (Many) More Tax Desiderata. For general revenue and redistributive taxes, the list of desiderata goes on (see @Held2010a). I focus here on only two criteria that are important for the welfare state and regional integration: and (p. ). ##### Well-Determined Incidence on Natural Persons. \\phantomsection [\\[sec:well-determined-incidence\\]]{#sec:well-determined-incidence label=&quot;sec:well-determined-incidence&quot;} In @Vickrey1947&#39;s seminal clarification, &quot;genuinely progressive taxation is necessarily *personal* taxation&quot; [-@Vickrey1947 1, emphasis added]. Albeit often ignored, this is merely a definitional clarification: distributive norms only concern the relative utility of *natural persons*. Corporations, for example, are no moral subjects, only people are. Factually, corporations are also never the ultimate recipients of utility. The welfare of all non-natural, private juristic persons (say, a public company) ultimately accrues to natural persons as their owners (say, shareholders), workers or customers. Taxing a corporation --- as [cits]{acronym-label=&quot;cit&quot; acronym-form=&quot;plural+short&quot;} try to --- is as nonsensical as it is impossible. It is nonsensical because we do not, and *cannot* know what it would *mean* to tax, say, the income of Deutsche Bank AG, because Deutsche Bank AG is an institutionalized fiction, but not a moral subject of fairness. It is also impossible because taxing the income of Deutsche Bank AG *will* instead reduce the welfare of its owners, workers or customers (who *are* moral subjects). Crucially, natural personhood is a condition *only* for general revenue and redistributive taxation, because they share the same *general* base (all people). By contrast, pigouvian levies and fees *can* and *should* be levied on any merely formal market participants, including corporations, that use the respective common good or natural monopoly (a *specific* base). While a [cit]{acronym-label=&quot;cit&quot; acronym-form=&quot;singular+short&quot;} on the income of Deutsche Bank AG makes no sense, the corporation should still be billed for its sewage or carbon footprint. Not only will a nominal tax on the income of a corporation factually fall on its owners, workers and customers, but, to make matters worse, we cannot even know which of these stakeholders pays. In the language of economics, the *effective* incidence of a tax can be difficult to ascertain, and it often matters little who nominally pays the tax as shown in (p. ). \\centering ![The Incidence of a Tax on Suppliers with Same Elasticities for Producers and Consumers[]{label=&quot;fig:same-incidence&quot;}](same-incidence){#fig:same-incidence width=&quot;100%&quot;} Taxation is always an (p. ). The incidence of a tax is borne by all parties to these market exchanges in proportion to their relative price elasticities of demand and supply, respectively, as shown in (p. ). Whoever can cheaply exit or substitute the taxed market exchange dodges most of the tax: because these price elastic suppliers *would* quickly cut their quantity supplied at lower prices, they can extract relatively high prices from buyers. Buyers, in this example, cannot cheaply exit or substitute the taxed market exchange: they *would* hardly be able to cut their quantity demanded in response to higher prices. At the post-tax equilibrium, buyers foot most of the tax. \\centering ![The Incidence of a Tax on Suppliers with Relatively Less Elastic Demand[]{label=&quot;fig:different-incidence&quot;}](different-incidence){#fig:different-incidence width=&quot;100%&quot;} In the real world, relative price elasticities of demand and supply in many markets are unknown and would be difficult to ascertain. [^131] Elasticity of demand and supply depends on many factors, including the availability of substitutes (for example,, margarine for butter) as well as the mobility and specificity of factors (for example, immobile, specific typewriter factory vs mobile, unspecific car rental). Therefore, to fairly and effectively redistribute between natural persons, government must tax market interactions where the relative price elasticities of demand and supply can be easily known. This is often the case for very broad categories of market exchanges, such as labor ([payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;}) or consumption ([vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;}): [^132] people cannot substitute working *somewhere* or employing *someone*, they cannot avoid to buying or selling *something*. [^133] The price elasticities for demand and supply in these broadly defined markets are small and similar. In the closed economy, government can effectively tax broad categories of market exchanges, such as consumption. [^134] By definition, economic activity does not extent beyond the borders of the closed economy, and there can therefore be no substitution to, for example, spending at home. ##### Minimal Market Distortions, Welfare Losses. \\phantomsection [\\[sec:minimal-DWL\\]]{#sec:minimal-DWL label=&quot;sec:minimal-DWL&quot;} When government taxes a market exchange, buyers and sellers may react in ways that reduce overall welfare. When the post-tax price of the transaction is higher than the buyers utility or the sellers cost, they exit the market. Their otherwise pareto-improving exchange does not occur, as shown in (p. ). Government also gains no revenue from these non-occurring transactions. Everybody loses. The sum total of these losses is known as the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of taxation (areas C, E in figure [)](fig:DWL). An efficient tax minimizes [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} for maximum revenue (areas B, D). \\centering ![The [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of a Tax with Unit-Elastic Supply and Demand[]{label=&quot;fig:DWL&quot;}](dwl.pdf){#fig:DWL width=&quot;100%&quot;} This ratio of tax revenue to [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} --- much like the (p. ) --- depends on the price elasticities of supply and demand. The more inelastic supply and demand, the smaller the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} for any given tax rate, as shown in (p. ). If buyers and sellers cannot easily cease their welfare-improving exchanges, they will continue to trade when taxed. A tax on such a market will not distort or depress market activities. \\centering ![The Deadweight-Loss of a Tax with Inelastic Supply and Demand[]{label=&quot;fig:smaller-DWL&quot;}](smaller-DWL){#fig:smaller-DWL width=&quot;100%&quot;} The price elasticities of demand and supply are, again, a hard, empirical question. As discussed above, supply and demand in broad categories of market exchanges, such as all domestic consumption ([vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;}) or all domestic labor ([payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;}) are relatively inelastic, and the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of such taxes is likely to be small in a closed economy. Still, [dwls]{acronym-label=&quot;dwl&quot; acronym-form=&quot;plural+short&quot;} of taxation may abound, even in closed economies. In mixed economies, the market for low-wage labor is particularly vulnerable to large [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}s. Welfare state governments to people who earn too little to maintain a socially acceptable standard of living (p. ). In this scenario, taxation of low-wage labor has two related, negative effects. [^135] 1. Taxation raises the effective cost of living for low-wage earners: for each euro they earn on the market, they can afford a lower, real standard of living. 2. If people can apply for handouts, their supply of labor may become perfectly price elastic, when market incomes are close to welfare incomes. If they could earn less on the market, than by collecting welfare, low-wage workers may be forced to exit the labor market: a large [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} ensues. [^136] Any tax on low-wage labor will only increase the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}: for every euro of wages taxed, more people will be pushed outside the (official) labor market. [^137] In the closed economy, government can minimize the tax burden on low-wage labor by relying more on progressive taxes, including a [pit]{acronym-label=&quot;pit&quot; acronym-form=&quot;singular+short&quot;}, instead of proportional taxes ([vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;}, [payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;}). The Means of Monetary Policy {#sec:monetary} ---------------------------- Government provides the economy with legal tender to serve as a medium of exchange, a unit of account and a store of value. Only government can do this, because supplying money is a (p. ). To avoid the (p. ) and (p. ) of de- or inflation, governments should be devoted to price stability. They set that quantity of money where it equilibrates with variable money *demand* at stable prices. The demand for money, in turn, is determined by the output of the economy. [^138] #### Tools. Government extends or contracts the supply of money to meet a quantity, interest rate (US) or inflation rate (Bundesbank) target. Fiat money is supplied in one of two ways: [^139] Government creates *base money* by buying government bonds ([omo]{acronym-label=&quot;omo&quot; acronym-form=&quot;singular+short&quot;}) or other financial assets ([qe]{acronym-label=&quot;qe&quot; acronym-form=&quot;singular+short&quot;}) from private holders who receive legal tender out of thin air [^140] in return. [^141] Central banks can also lend money to financial institutions at a set rate (discount window rate). Government stimulates the multiplication of *broad money* in the fractional reserve banking system by lowering the reserve requirements for private banks. \\centering ![The Policy Trilemma of Central Banks[]{label=&quot;fig:triangle-cb&quot;}](triangle-cb){#fig:triangle-cb width=&quot;1\\linewidth&quot;} #### Role. The impact of monetary policy on the real economy is, of course, incompletely understood and highly controversial. I do not need to recapitulate these controversies here (for a recent review, see @Wapshott2011). I note two hopefully uncontroversial points: 1. *Monetary Neutrality.* [\\[itm:monetary-neutrality\\]]{#itm:monetary-neutrality label=&quot;itm:monetary-neutrality&quot;} Following the classical dichotomy, *nominal* economic variables, such as the price level, do not matter in the long run. The prosperity of an economy is determined by its level of technology, human capital and physical capital --- not by the amount of intrinsically worthless currency in circulation. On this ultimate end of monetary policy, Monetarists and Keynesians can probably agree: good money stays out of way of the real economy to let actual output track the long-term growth path of the economy. 2. *An Empirical Question.* [\\[itm:empirical-macroeconomics\\]]{#itm:empirical-macroeconomics label=&quot;itm:empirical-macroeconomics&quot;} Different monetary theories follow from different assumptions about human behavior: Keynesians : believe that market participants are swayed by changes in nominal variables. As a result, the prices for goods and services and factor prices (especially wages) do not equilibrate promptly or perfectly. Self-reinforcing (debt-deflation) crises of excess supply and depressed demand may result [@Fisher1933]. The state can smooth out resulting aggregate fluctuations in output by altering the money supply. Monetarists : believe that market participants are fairly rational and will consider expectations of future inflation (and taxes) in their current decisions. As a result, prices for goods and services and factor prices, especially wages, equilibrate quickly and keep the economy on its long-term growth trajectory. State intervention in the business cycle or the money supply are unnecessary and ineffective. [^142] [^143] Determining our irrational *animal spirits* [@Keynes1936] is an empirical question best left to cognitive psychology, behavioral economics and related fields (recently @Akerlof2010, see , p. ) &quot;Keynes vs. Hayek&quot; [@Wapshott2011] need not be a normative struggle, but a pragmatic balance based on empirical evidence. Whichever policy stabilizes economic output along the long-term growth trajectory is best. If ever there was a policy that should be judged on consequentialist terms, it is monetary policy. Because monetary dynamics *do not* and *should not* matter to our &#39;household with a cast of billions&#39; I can refer the issue to empirical clarification and need not entertain it further here. Suffice it to remind us that government should set that quantity of money where it equilibrates with variable money demand at stable prices. The rest is empirical details. ## Trade-Offs of the Mixed Economy Given its conflicting (p. ) the mixed economy can arrange its (p. ) to make at least two basic trade-offs. 1. *Command vs. Exchange.* It can organize more or less production and distribution by command instead of exchange. 2. *Consumption vs. Saving.* It can defer more or less of current consumption to the future. We can look at these trade-offs by (p. ) and by (p. ) of the entire economy. #### By Expenditure. \\phantomsection [\\[sec:by-expenditure\\]]{#sec:by-expenditure label=&quot;sec:by-expenditure&quot;} A mixed economy can devote its resources (expenses) to public or private, investment and consumption goods. The four expenditure components and revenue flows are plotted on a two-dimensional coordinate space of the mixed economy in (p. ). I provide (very roughly) equivalent macroeconomic variables as in (p. ). [^144] \\centering ![Coordinate Space of the Mixed Economy[]{label=&quot;fig:coordinate-space&quot;}](coordinate-space){#fig:coordinate-space width=&quot;1\\linewidth&quot;} \\centering ##### Save/Consume More or Less. Government increases the private savings rate by taxing consumption (instead of income), encouraging investment (factory) through industrial policy or other fiscal stimulus and increases public saving by putting net tax revenue into durable public goods (basic research), natural monopolies (a bridge) or . Steering in the opposite direction, government increases private consumption by cash transfers, fiscal stimulus and public consumption by handing out in-kind benefits (school milk), or providing short-term public goods (fireworks) and natural monopolies. The saving-consumption trade-off roughly expressed in the gross savings rate in (p. ). [^145] ##### More or Less Government/Market Government sets the exchange-command mix of the economy by taxing more or less of economic output, and commissioning public investment and consumption from the revenue. The exchange-command mix is (roughly) expressed in the public expenditure quota in (p. ). ##### How to Strike the Best Balance. Welfare economics and the of a mixed economy (p. ) suggests that there are one (or more) *optima* in the balance between saving and consumption, between market and command. These ends also bind each of the government interventions, such as providing a public good, to *specific* justifications, [^146] such as a market failure in providing the public good. Normatively, government *should* not take an arbitrary position on (). However, I should point out that *positively*, government in a closed economy *could* employ its to choose any of the possible trade-offs between consumption and saving, market and command economy. ##### Tax is Key. Of these of the mixed economy, tax is the dominant tool. To redirect resources between the four quadrants, government relies primarily on (p. ) rather than on (p. ) which has limited applications and (p. ) which is neutral in the long run. \\small -------- -------------------------------------------------------- ------------------------------------------------------------------- -------------------------------------------------------------- --------------------------------------------------------------------- -- (r)3-4 *Private* *Public* *[I]{acronym-label=&quot;I&quot; acronym-form=&quot;singular+short&quot;}* Factory, [rnd]{acronym-label=&quot;rnd&quot; acronym-form=&quot;singular+short&quot;} Bridge, basic research *$\\sum=$[gfcf]{acronym-label=&quot;gfcf&quot; acronym-form=&quot;singular+short&quot;}* *[C]{acronym-label=&quot;C&quot; acronym-form=&quot;singular+short&quot;}* TV set, vacation School milk, fireworks *$\\sum=$[fce]{acronym-label=&quot;fce&quot; acronym-form=&quot;singular+short&quot;}* *$\\sum=$($\\gls{I}+\\gls{C}$)* *$\\sum=$[G]{acronym-label=&quot;G&quot; acronym-form=&quot;singular+short&quot;} [^147]* (r)3-4 -------- -------------------------------------------------------- ------------------------------------------------------------------- -------------------------------------------------------------- --------------------------------------------------------------------- -- : GDP Components by Expenditure in the Closed Economy[]{label=&quot;tab:GDP-Comp-Exp&quot;} #### By Changes in Net Worth \\phantomsection [\\[sec:delta-net-worth\\]]{#sec:delta-net-worth label=&quot;sec:delta-net-worth&quot;} The same trade-offs of the mixed economy can also be understood as different changes in net worth. As economies, firms and households save and consume, their net worth changes. This truism is expressed in the Haig-Simons income identity: $$\\label{eq:haig-simons} \\text{Income}=\\text{Consumption}+\\Delta\\text{Wealth}$$ Or, trivially transformed: $$\\label{eq:haig-simons-trade-off} \\text{Savings}-\\text{Dissavings}=\\text{Income}-\\text{Consumption}$$ applies to the savings trade-off of households, firms and government, including some examples. \\centering ![Individual and Collective Haig-Simons Identity of Income[]{label=&quot;fig:haig-simons-individual-collective&quot;}](haig-simons-individual-collective){#fig:haig-simons-individual-collective width=&quot;1\\linewidth&quot;} The logic of the Haig-Simons identity, and is easy: you can only have your cake *or* eat it. Any change in consumption must be matched by a change in income or net worth --- and vice versa. For instance, a household can afford to build a house either by taking out a mortgage, earning more, or consuming less. The identity can be balanced *across* households, firms and government, too. These transfers occur through different financial products and fiscal institutions. For instance, a household can also afford to build a house if it pays fewer taxes and government accepts less revenue. Government looses the amount in income that households earn. [^148] A comprehensive Haig-Simons identity also includes depreciation (for example, neglected roads) and depleted natural resources (for example, fossil carbohydrates) as real dissavings. For instance, an economy can consume more than it earns for some time by burning oil. [^149] Conversely, a comprehensive Haig-Simons identity also includes real savings such as a newly developed technology or public infrastructure, even when these are not (yet) market priced. [^150] For instance, an economy can consume less today at equal income and channel the surplus into basic research, [rnd]{acronym-label=&quot;rnd&quot; acronym-form=&quot;singular+short&quot;} or send more people to (costly) college. The Haig-Simons identity of income is a truism similar to the law of conservation of matter. Surprisingly, it is often ignored or misconstrued, even in (p. ). I offer two notes to further clarify: 1. *Saving $=$ Investment.* In the long run, when industry has adapted and monetary effects have neutralized, all savings are invested. Saving does *not* depress aggregate demand and choke the economy. True Keynesian shortfalls in aggregate demand result from people hording *cash* or equivalents, and not because of an increase in investment. In deflationary spirals, anticipating lower prices, people cut *both* consumption *and* investment: the contracted money supply freezes up *all* economic activity. This popular conflation of monetary dynamics with savings rates may be one of the most formidable obstacles to enlightened, democratic choice of economic policies and tax in particular. [^151] Instead, saving *changes*, but need not depress, aggregate demand; more capital goods and fewer consumer goods are in demand. If given enough time, the economy can transform from &quot;S-classes to school buildings&quot; without write-downs on unamortized capital investment. Well-regulated, competitive financial intermediaries will always channel saving into investment. An investment is a valuable transformation or improved understanding of our physical world. It requires labor. Be it a [swf]{acronym-label=&quot;swf&quot; acronym-form=&quot;singular+short&quot;} or a savings account, a blast furnace or a green tech patent --- in the final analysis, saving always means to build more things that last longer and/or that we will consume later, instead of things that last a short while and that we consume now. Of course, any *one* investment will *ultimately* be consumed or depreciate away. And we *can* save too much, when capital goods depreciate faster and have such decreased marginal returns that they outstrip our current utility from the same resources (This follows from [@Solow1956] theory of growth, p. ). But while that means we should not save endless amounts at any point in time, it does not mean that at some point in time, we should save no more. There is no economic reason why we could not roll over (limited) savings to our children in perpetuity. 2. *Dissaving $\\neq$ Debt $=$ Deposits $\\neq$ Saving.* [\\[itm:credits-debits-wash\\]]{#itm:credits-debits-wash label=&quot;itm:credits-debits-wash&quot;} Dissavings are not the same as debt. Dissaving is a decrease in net worth of households, firms and economies: we diminish some durable thing in its value. Conversely, saving is an increase in net worth: we add value to some durable thing. In contrast, going into debt does not affect net worth of households, firms or government: we temporarily gain access to an *already existing* valuable thing (construction man-hours), potentially transform it into something else (a house) and return the valuable thing later (with interest). Conversely, putting in a deposit (or other credit) also does not affect net worth: we temporarily grant access to an *already existing* valuable thing to others, for an interest. \\small *Households* *Government* -------------------------------------- ------------------------------------------- -------------- $Income - Spending&lt;0$ $Revenue - Spending&lt;0$ *$\\sum$* private debt public debt $=$ (for example, credit card, mortgage) (for example, government bonds) *All Debt* \\[20pt\\] $Revenue - Spending&gt;0$ *$\\sum=$* $Income-Spending&gt;0$ private credit public credit $=$ (for example, deposits, bonds) (for example, reserves, sovereign wealth) *All Credit* $\\sum=0$ : Debt and Credit in the Closed Economy[]{label=&quot;tab:Debt-Credit&quot;} Trivially, public and private debt will always equal public and private credits in the closed economy as summarized in . By definition, every debtor needs a creditor. Debt and credit define the short-term control of and long-term claims to valuable things and may have distributive consequences, but they do not affect net worth. Net saving, by definition, does. [^152] Smoke and Mirrors of the Mixed Economy &gt; *&quot;If it&#39;s too good to be true, it&#39;s too good to be true.&quot;*\\ &gt; --- The author&#39;s landlady, trained nurse, single mother of three and foreclosed homeowner in Irvine, CA (2007) A mixed economy can seemingly overcome its physical limitations and the (p. ) between different ends using a set of smoke and mirrors. \\centering ![Circular Flow of Income in the Economy, with Macroeconomic Imbalances](circular-flow-with-imbalances.pdf){width=&quot;100%&quot;} \\scriptsize{Compare \\autoref{fig:circular-flow-with-imbalances}.} [\\[fig:circular-flow-with-imbalances\\]]{#fig:circular-flow-with-imbalances label=&quot;fig:circular-flow-with-imbalances&quot;} I briefly explain how three such practices allow us to live beyond our means in the short term: 1. *Credit Bubbles.* [\\[itm:credit-bubbles\\]]{#itm:credit-bubbles label=&quot;itm:credit-bubbles&quot;} Within the closed economy, (p. ). Still, excessive debt and credit can serve to hide or defer economic trouble ahead. In efficient financial markets, credit is extended to households, firms and governments at an interest rate that fully reflects the risk of default. To assess credit risk, creditors make (and update) predictions about the future solvency (and liquidity) of debtors. In the real world, these guesses are sometimes overly optimistic given the available information and credit is extended at too low an interest rate to too many people and organizations. [^153] As long as these risks are not reappraised or do not materialize, an economy can seemingly live beyond its material means. Eventually, of course, credit bubbles will burst and debtors will partly (have to) renege on their promises to repay interest and principal. Now, the economy as a whole has to pay for the fat years. Credit bubbles are thus always an intertemporal redistribution of wealth from the future to the presence. [^154] When they burst, credit bubbles also jumble ownership rights as defaults spread [@Stiglitz2010]. [^155] Depending on this resulting, quintessential political-economic struggle between debtors and creditors, the brunt of the bursting bubble is allocated between different households, firms and government [@Coggan2011]. [^156] Similar to inflation and asset bubbles, bursting credit bubbles also redistribute somewhat arbitrarily based on financial product and timing: people who own equity or leave overheated credit markets early enough, win. The suckers and laggards loose. 2. *Asset Bubbles.* [\\[itm:asset-bubbles\\]]{#itm:asset-bubbles label=&quot;itm:asset-bubbles&quot;} Concomitant to credit bubbles, bubbles can arise in certain classes of overvalued assets, often including real estate (2007ff), stock (2000f) but also art, oldtimers or essentially useless gold. Assets are overvalued, when their capital gains are more than what returns can reasonably be expected, given all available information. As long as prices do not revert to intrinsic value, people and the economy as a whole can live off the virtual capital gains and beyond its material means. Asset bubbles, too, redistribute wealth from the future to the presence. In addition, as pyramid schemes, they redistribute between early investors and later, &quot;greater fools&quot; (see @Stiglitz2010 for a good discussion of the 2007ff crises). 3. *Inflationary Pressure.* [\\[itm:inflationary-pressure\\]]{#itm:inflationary-pressure label=&quot;itm:inflationary-pressure&quot;} Excessive monetary expansion, aside from fueling credit and asset bubbles can also cause demand-pull inflation. The onset of inflation and its costs, however, need not be instantaneous. As upcoming wage-price spirals and increasing inflationary expectations silently add to built-in inflation [@Gordon1988], an economy may enjoy temporarily heightened output. [^157] As the price level eventually creeps up, [^158] the economy pays the costs of inflation through depressed growth. Grandfathered inflation also redistributes from the future to the presence, and arbitrarily redistributes between cash-denominated and other ownership claims, between debtors and creditors. [^159] #### Too Good to be True. In summary, an economy cannot produce and consume above its long-run growth path of outward shifting aggregate supply. Trivially, the wealth of an economy is determined by its natural resources, technology, human and physical capital --- all *tangible things*. When these capacities are fully utilized (as they are *not* in a cyclical downturn or debt-deflation crisis), no financial or monetary charlatanism can take us beyond them in the short term. Any short-term gain that a bubble or excessive monetary expansion will bring only defers the day of this reckoning. ## Real Dissavings of the Mixed Economy {#sec:real-dissavings} &gt; *&quot;Give me chastity and continence, but not yet.&quot;*\\ &gt; --- @St.AugusteofHippo397 Confessions (VIII, 7) Our economies substantially dissave in ways that are not reflected in conventional macroeconomic data. These *real* dissavings, or &quot;off-budget fiscal activities&quot; [@Bonker2006 49] include population aging, [^160] depleted natural resources (land, oil, water), exhausted common goods (global warming) [^161] or failed public goods (immunization?), to name just a few. These processes all unambiguously degrade something of economic value, and should be recorded as consumption in our aggregate Haig-Simons accounts (as I have suggested in , p. ). [^1]: [\\[fn:tilly\\]]{#fn:tilly label=&quot;fn:tilly&quot;} This positive description does not imply normatively, as liberal entitlement theory would have it, &quot;that a person is entitled to those goods acquired in uncoerced exchanges with others&quot; [@Nozick1974; @Friedman1962 149]. Uncoerced *exchange* does not mean absence of coersion. At the very least, markets rely on a large-scale coercive power (aka. the state) for property rights and states and markets [@Tilly-1985-aa]. Naturalizing whatever allocative results the free market produces as entitled, inalienable, private property is as ahistorical as it is uncritical. [^2]: Because I am interested in economic abstractions, a materialist theory of the state (such as @Tilly-1985-aa) suits me better than, for example, contract theories. Again, this does not imply positive or normative claims. [^3]: I prefer the terms &quot;market&quot; and &quot;command&quot; economy, because the conventional *capital*ism-socialism dichotomy is misleading: real existing Soviet (especially Stalinist) socialism also accumulated capital (for example, electrification, railroads), just in different hands. Market exchange and planned command describe the different modes of production and distribution more precisely. [^4]: Mixed economies also alter market outcomes to improve *efficiency* in case of market failures. [^5]: Power resources theory [for example @Korpi2003], Marxist interpretations [for example @Offe1972] or recently, some advocates of a [big]{acronym-label=&quot;big&quot; acronym-form=&quot;singular+short&quot;} might disagree: for them, the purpose of the welfare state is to change power relations. These are reservations of the (p. ) and are discussed in (p. . [^6]: Conversely, a socialist welfare state is an oxymoron. Just as welfare states are defined by their coexistence with a market economy, there is no such thing as a socialist welfare state, no matter the (however corrupted) distributional goals of nominal socialisms. In a predominantly planned, or even command economy, the state directs much of both production and distribution and therefore does not require the (welfare state) institutions governing the mixed economy, including taxation, insurance and pension funds, business cycle smoothing and dedicated social service provision. Even where these institutions nominally existed, they never faced the (otherwise defining) condition of independent market prices. For example, where &quot;prices&quot; are *set* and &quot;profits&quot; owned by the state, taxes become a meaningless category: they are really just changes in the set prices [for example, @Bonker2006 23]. [^7]: A misnomer. Even the [nhs]{acronym-label=&quot;nhs&quot; acronym-form=&quot;singular+short&quot;} is not fully socialized medicine, it is just a tax-financed near state monopsony on health services. [^8]: In fact, social insurance contributions are a regressive [^9]: Individual citizens are incentivized to mis(over)-represent their marginal utility from additional doctors, absent individually accruing costs. This is equivalent to a [cpr]{acronym-label=&quot;cpr&quot; acronym-form=&quot;singular+short&quot;} problem or the cooperation problem of a [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;}. [^10]: ...or socialize parts of the labor market, which would further shift the frontier, but not alleviate the problem. [^11]: ...as the etymology of economics would suggest: the science of managing the *oikos*, greek for household. [^12]: [\\[fn:also-in-mpp\\]]{#fn:also-in-mpp label=&quot;fn:also-in-mpp&quot;} This section is based, in part, on earlier, unpublished work which I submitted to the Hertie School of Governance as my Master of Public Policy thesis in [@Held2010a]. I have since revised and expanded it. [^13]: [\\[fn:human-condition\\]]{#fn:human-condition label=&quot;fn:human-condition&quot;} I choose the term &quot;human condition&quot;, because it captures what might be (positively) inescapable in human existence, without specifying either a natural or cultural explanation. [^14]: Some of the frailties follow the heuristics and biases of human cognition identified by the prospect theory research program [@KahnemanTversky1979; @Kahneman2011] Prospect theory is appropriate to consider for any first-order theory on the material: it provides a positively descriptive model of human cognition and decision making as opposed to the normatively optimal (but rarely observed) expected utility hypothesis, on which neoclassical economics rest (first formulated by @Bernoulli1738, later specified by @VonNeumannMorgenstern1944). In part, the mixed economy and the welfare state (for example, mandatory pensions) are institutions to get prospect theory humans, easily mislead by their fast but erroneous &quot;system one&quot; to follow the slow, but accurate &quot;system two&quot; [@Kahneman2011]). Supposedly, when we give it some thought, we find that decisions based on expected utility [@Bernoulli1738] maximize our happiness, but ever the &quot;cognitive misers&quot; [@FiskeTaylor-1991-aa], exhausted and overwhelmed, we easily fall back to *system one* shortcuts and fail to rationally maximize utility. Institutions, characteristically enabling and restricting human behavior --- including those of the mixed economy --- can help prospect theory humans to approximate decisions based on expected utility. Prospect theory, alas, is still an emerging field (together with yet dejunct evolutionary psychology and cognitive neuroscience), and any link to the mixed economy must in the meantime remain tenuous. [^15]: Indeed, as Dr. Mildner of the Hertie School of Governance never tired of telling me: &quot;There is state failure, too&quot; [^16]: As suggested by the late [Kim-Jong Il&#39;s choice of grey jumpsuits](http://kimjongillookingatthings.tumblr.com/). [^17]: As in modernization theory, for example, [@InglehartWelzel-2005-aa]. [^18]: Specifically, resource-based &quot;rentier states&quot; may hinder liberal democracy [@Beblawi1990]. Generally, distributive decisions in planned economies appear easily as zero-sum games (as opposed to the per-definition non-zero-sum, pareto improvements in competitive markets), which may corrupt politics. [^19]: This broad definition of efficiency is more demanding than pareto efficiency. [^20]: Here, as always, it is important to keep up (p. ). The first theorem of welfare economics invoked here is an exercise in positive logic. It does not imply: 1. that *actual* markets display these properties --- that would be a first order *empirical*, not logical finding. The assumptions of are, in fact, quite heroic and may rarely be observed in the real world. 2. that market allocations are beyond normative reproach. The first theorem of welfare economics, crucially, operates only on *ex ante* distributions, which may or may not be distributively just. [^21]: (Neo)classical economics takes an oddly static view of the economy. In reality, equilibria are more often in flux, a matter of becoming, not being. [^22]: [\\[fn:1st-theorem\\]]{#fn:1st-theorem label=&quot;fn:1st-theorem&quot;} Formally, the first theorem of welfare economics states that over a *given* distribution, the competitive equilibrium will be a pareto optimum (demonstrated first graphically by [@Lerner1944], mathematically by [@Lange1934], [@Debreu1954] and others). [^23]: An allocation is pareto-improved if at least someone receives more, with others receiving (at least) the same. When all such improvements are exhausted, an allocation is pareto optimal. Crucially, pareto improvements and optimality are always in reference *only* to some ex-ante allocation. Also, pareto improvements do not imply that everyone would be better off by the same amount. [^24]: Nassim Nicholas @Taleb2007 has recently put this succinctly by praising &quot;aggressive trial and error&quot; [-@Taleb2007 xxi] in free markets that &quot;allow people to be lucky&quot;. Being a quantitative trader by profession, @Taleb2007 abandons rational choice when faulting Karl @Marx-1867-aa *and* Adam @Smith-1776-lq for believing that free markets work because of *rewards*. [^25]: @Friedman1970a&#39;s dictum applies: &quot;The Social Responsibility of Business is to Increase its Profits&quot; [-@Friedman1970a], both as commandment and promised salvation. [^26]: This assumption is politically consequential. If firms and their owners, in fact, merely maximized their profits, their property would *not* bestow power. By definition, to be a profit-maximizer in a competitive market is to *only* do things that pareto-improve everyone over a given distribution. For property to bestow power in the classic definition [@Geoff2002 8ff], owners must sacrifice some of their profit to make other people do something they would *not* otherwise (pareto-optimally) have done. Under microeconomic dictum, owners exert power by *transferring* or *relinquishing* some of their surplus production. Clearly, to assume profit maximization and thereby to effectively define away a power aspect of property is implausible, and it rests on some heroic assumptions of human rationality and utility-maximization. More importantly, profit maximization implies --- as all such static microeconomics --- that future profits, and thereby, future demand *can* be known, at least probabilistically. From a more dynamic or evolutionary perspective, capitalism is *path dependent*, and those rich enough to bet on future profits now will make the very paths on which that future depends. Property *does* bestow power, and the kind of axiomatical contortions to define that away probably constitute the kind of hegemonic tendencies of microeconomic thinking that so (rightly) infuriates its critics. However, any emancipatory or equity argument on the distribution of property should explicate its relaxation of profit maximization, and provide some preliminary account of the motivations of rich people, instead. I take up this argument when I discuss the \\[fairness of taxation\\] (p. ), especially of a . [^27]: Constant returns to scale contrast with the very idea of functional differentiation, economic modernization and division of labor discussed (p. ) and (p. ). [^28]: I discuss only a popular subset of market failures in this paper. I ignore many of the problems recently highlighted by the 2007ff financial and sovereign debt crises. While common and public good failures explain much (for example herding and information spill-overs in @Banerjee-1992-aa), the 2007ff crises require a dedicated first-order theory of financial capitalism, including an in-depth appreciation of its institutions (for example, derivatives), practices (for example, bank equity) and policies (for example, interest rate). [@Cassidy2010] provides an equally accessible and comprehensive account of &quot;How \\[financial\\] Markets Fail&quot;. [^29]: Providing a public good has a positive externality, using a common good has a negative externality. Underproviding a public good and overusing a common good can also be represented as [pds]{acronym-label=&quot;pd&quot; acronym-form=&quot;plural+short&quot;}, where underprovision or overuse is the Nash Equilibrium. Whoever first buys a natural monopoly at very high marginal cost, extends the positive externality of very low marginal costs to all subsequent buyers. [^30]: Elinor [@Ostrom1990] criticizes the canonically assumed failure of commons in social science and provides an empirically grounded account of their successful, non-coercive governing. [^31]: Public and common goods are often hard to distinguish in public choice. *Not* overusing a common good is a public good, and providing a public good is a common good. [^32]: This relaxes the perfect competition assumption [\\[itm:easy-entry-exit\\]](#itm:easy-entry-exit){reference-type=&quot;ref&quot; reference=&quot;itm:easy-entry-exit&quot;} on . [^33]: Otherwise, if *all* buyers could pay for the initial costs equally. [^34]: Calls for more startups, patents and research spin-offs, particularly in Germany, may serve as evidence for suboptimal incentive design under the status quo. [^35]: [\\[fn:monetary-commons\\]]{#fn:monetary-commons label=&quot;fn:monetary-commons&quot;} There are no monetary responses to any of these market failures, because (p. ) boosts or retards economic activity *overall* by extending or restricting credit and money creation. Markets fail in the provision of public goods, common goods and natural monopolies because these goods are ill-priced *relative* to other goods in the economy. States cannot deliberatively correct relative prices in the economy by monetary policy. Inflation, if and to the extent that it is caused by monetary overexpansion, may, however appear first in some goods, including long-term assets such as real estate, as appears to have happened in the US monetary expansion up until the 2007ff financial crisis. These relative price changes on the first impact of inflation cannot be effectively targeted by government and eventually propagate into an overall increase in price levels. [^36]: The expression stems from Ireland, where the British king passed enclosure legislation to privatize and built fences around previously communal pastures. [^37]: The Coase theorem further holds that it does not matter to whom the property rights are initially granted, say whether the polluters are granted a right to pollute, or citizens are granted a right to clean air (invariance thesis). While *welfare* neutral, this original granting of property rights does have distributive effects: whoever is granted the property right, enjoys a windfall. Economists often suggest to auction off new property rights at competitive prices, to minimize arbitrary distributive effects. [^38]: Here&#39;s what happens upon privatization: Enclosed public goods become natural monopolies. Enclosed common goods become private goods, as per (p. . [^39]: States can also regulate common goods by outlawing its overuse, as the US does by instituting minimum [cafe]{acronym-label=&quot;cafe&quot; acronym-form=&quot;singular+short&quot;} standards on automakers. Regulation by defining maximum acceptable use of commons is inefficient, because it does not incentivize market participants to save the commons wherever it is marginally cheapest to do so. Standards regulation cause a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} much like minimum wages or other price floors and ceilings. [^40]: The German term for a Pigouvian tax, helpfully, translates to &quot;steering tax&quot; (Lenkungssteuer). [^41]: [\\[fn:why-ac-fees\\]]{#fn:why-ac-fees label=&quot;fn:why-ac-fees&quot;} Recall that prohibitevely high initial marginal cost marred private markets in the first place. [^42]: Except for (p. ). [^43]: For two technical reasons, most economists and central banks prefer a positive, but moderate and stable rate of inflation of around 2% rather than perfect price stability at 0% inflation: 1. A positive inflation rate gives central banks room for maneuvre in fiscal stimulus. At moderate inflation, central banks can pursue a *negative* real interest rate (as was the case in 2011ff in many markets). 2. A positive inflation rate can mitigate the hypothesized downward stickiness of nominal labor costs. Real wages will fall even when nominal wages stay constant. [^44]: The cost of minimizing cash holdings, including many trips to the bank, during which shoe leather is supposedly worn down. [^45]: The costs of repricing for business, including the cost of changing restaurant menus. [^46]: Experiencing higher consumer prices, workers will demand higher wages, which in turn prompts producers to increase sale prices. [^47]: With the exception of Japan&#39;s lost decade. [^48]: Under deflation, holding (increasingly valuable) cash is relatively more attractive than investing. [^49]: This, if nothing else, is the reason why backing a paper currency with specie (such as the Gold standard) or pegging it to another currency (fixed exchange rate) are bad ideas: under such fixed or pegged regimes, the money supply expands and contracts *independent* of output. Under the gold standard, money supply tracks discovery and extraction of gold --- a process which is likely unrelated to economic output. This, in addition to the transaction costly lunacy of digging up gold in one place (a mine) only to bury it in another place (a vault). Similarly, under a pegged (or fixed) exchange rate, money supply follows the monetary command of *another* economy. [^50]: I concentrate here on *individual* risk. Governments and markets should also reduce *aggregate* risk and systemic uncertainty [@Knight1921]. They should avoid extreme negative risks, seek extreme positive risks [@Taleb2007] and avoid systems too complex and tightly coupled to be safely operated by humans [@Perrow-1999-aa]. To the extent that such precaution, requires fiscal, regulatory or monetary policy intervention, this end also depends on intact of the mixed economy (p. ). [^51]: If people were neutral between upside and downside risks, they would be indifferent between a lifetime of car insurance premiums and a (low) probability of accidental personal bankruptcy. [^52]: With sophisticated financial markets, insurants buy this protection not just from one another, but also from other (rich) people or financial intermediaries willing to stomach the risk for a premium. [^53]: I do not include old age here, because the risk component (a long life in retirement) of pensions is small compared to the saving component. Other insurances, notably health insurance, also include a saving component (for high morbidity in old age), but the risk component dominates. [^54]: This relaxes perfect competition assumption [\\[itm:perfect-information\\]](#itm:perfect-information){reference-type=&quot;ref&quot; reference=&quot;itm:perfect-information&quot;} on . [^55]: The conventional terminology stems from @Akerlof-1970-aa&#39;s original paper, in which he modelled quality uncertainty in the market for used cars. In American slang, a lemon is a car &quot;that is found to be defective only after it has been bought&quot; (Wikipedia). A cherry, conversely, is a good used car. [^56]: Prior the Obama administration&#39;s [ppaca]{acronym-label=&quot;ppaca&quot; acronym-form=&quot;singular+short&quot;}, in the US, *pre-existing conditions* were frequently exempted from private health insurance contracts, defeating the purpose of insurance for many chronically ill or disabled persons. Private insurers minimize the ex-ante problem of asymmetric information by excluding those risks about which the insurants may already know. In Germany, insurants experience a similar frustration if they seek to take out (recently privatized) occupational disablement insurance, facing greatly limited coverage (for example, no cardio-vascular conditions), eligibility (for example, no back-related conditions for construction workers) or premiums (for example, higher rates for burnout-prone teachers). [^57]: To the extent that government also regulates coverage and admittance, as it would have to do to avoid a lemons markets, the mandatory, nominally private insurance becomes a partly quasi-fiscal institution. When central qualities of the products (insurance) are regulated and buyers forced to buy, premiums are to a large extent pre-determined. Insurance firms will differ only in the few, administrative components of their business outside the reach of state command. The part of the premium that covers the mandated services is effectively a tax, and the insurance company but an outsourced service provider to the government. Ignoring, as I have here done, the (vexingly complicated) supply side of medical care, the difference between mandatory private and public health insurance appears small. [^58]: Even people with a privately known risk of zero should be included, as low-risk insurants may otherwise find it attractive to misrepresent their privately known risks. This extreme case of a privately known risk of (close to) zero cannot be justified under an efficiency norm of Pareto optimality, as everyone is made better off by making some people worse off. Instead, it requires the stronger Kaldor-Hicks efficiency norm [@Kaldor1939; @Hicks1939]. Resolving a lemons market may cost some people less than it will benefit others. &lt;!-- % The costs of risk pooling should incur generally to all entities, irrespective of their individually, asymmetrically known risk\\footnote{Moral hazard, the ex-post problem of asymmetric information, is ignored \\citep{Arrow1971}. %It is assumed here and in the following that the insured, privately known risks do not depend on the behavior of entities. %If risky behavior does in fact occur and create moral hazard, disincentivizing of such risky behavior may enhance efficiency. %\\emph{Specific} sources of revenue such as co-payments or variable premiums should then be extracted from some, risk-seeking entities only. %Such a specific revenue component of moral hazard is, in fact, a Pigouvian taxation of a commons problem. %It is further discussed below.}. %Even entities with a privately known risk of zero should be included, as low-risk entities may otherwise find it attractive to misrepresent their privately known risks\\footnote{This extreme case of a privately known risk of (close to) zero cannot be justified under an efficiency norm of \\hyperref[sec:pareto]{Pareto optimality}, as everyone is made better off by making some entities worse off. %Instead, it requires the stronger \\hyperref[sec:kaldor-hicks]{Kaldor-Hicks efficiency} norm described in \\autoref{sec:tax-optimality} \\citep{Kaldor1939,Hicks1939}. %Resolving a lemons market will cost some people less than it will benefit others.}. --&gt; [^59]: Because (p. ). [^60]: In perfect markets with sufficient price flexibility it also does not matter whether social contributions are levied from employers or employees. The incidence of a tax depends only on the relative price elasticity of supply and demand for labor. In labor markets, employees (supply) are typically less price elastic than employers (demand) who can substitute labor for capital, or move their capital. Employees end up paying for &quot;social insurance&quot; no matter the nominal burden. Only the matters (p. ). [^61]: Again, states and markets are not the only way to organise production and distribution of material goods, emphatically not in the mainstays of moral hazard, such as decisions about medical care. [@Schwartz2010] argue passionately for a patient-doctor relationship based on trust and &quot;Practical Wisdom&quot; and cogently argue how *any* system based on incentives can, and *will* be perverted. [^62]: According to [@Hobbes-1651-aa] &quot;life in a state of nature&quot; would be &quot;solitary, poor, nasty, brutish and short&quot;. [^63]: Whether a capacity for altruism really developed out of genetic nepotism, as both the book of genesis and the inclusive fitness hypothesis (@Hamilton1964 [@Wilson1975]) would have it has recently been questioned [@Wilson2012]. [^64]: This, again, is the first theorem of welfare economics (see footnote [\\[fn:1st-theorem\\]](#fn:1st-theorem){reference-type=&quot;ref&quot; reference=&quot;fn:1st-theorem&quot;}). [^65]: This, of course, is the very exploitation that Marx has criticized (Manchester) capitalism for ([-@MarxEngels-1848-aa; -@Marx-1867-aa]). By definition, *proletarians* --- those who only have their offspring --- only receive the minimum wage necessary to reproduce themselves, and their labor. [^66]: *Price discrimination* strategies are one often unacknowledged practice that redistributes utility from the rich to the poor in advanced market economies. To tap into different willingnesses to pay, firms often try to price same or similar goods to different market segments. Especially when the goods are same (toothpaste with or without coupon rebate), or similar in production cost (mid-priced sedan and luxury sedan), price discrimination can redistribute resources from (rich) consumers with a high willingness to pay to (poor) consumers with a lower willingness to pay --- an effective, if normatively imperfect way to redistribute. Compared to a no-trade scenario, price discrimination causes a pareto-improvement as per the (see footnote [\\[fn:1st-theorem\\]](#fn:1st-theorem){reference-type=&quot;ref&quot; reference=&quot;fn:1st-theorem&quot;}). Compared to a trade scenario *without* price discrimination, however, (poor) buyers with a low willingness to pay are better off compared to (rich) buyers with a high willingness to pay. With price discrimination, buyers of mid-priced sedans enjoy some of the innovations (airbags) first paid for by luxury car buyers. Pareto optimality makes everyone better off, but not by the same amount: a caveat that can have unexpected implications. If ever there was a way that wealth would &quot;trickle down&quot;, this must be it. In addition to this unexpected distributive effect between consumers, much of price discrimination also constitutes monopolistic competition, and as such redistributes resources from consumers to producers and causes a [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}. [^67]: Consequently, some part of real observed structural unemployment may stem from such heightened wages, and therefore, be &#39;natural&#39; [@Schlicht1978], as opposed to &#39;voluntary&#39;, as the Monetarists would have it. Keynesians may then conclude that an expanded money supply should continuously push against such natural, but suboptimal unemployment. [^68]: By contrast, in perfect capital markets one investor with \\$10 should be roughly replaceable by two investors with \\$5 each, ignoring transaction costs, information asymmetries, risk aversion and other complications. High-skill labor markets may display invisibilities: one computer scientist with a PhD in artificial intelligence may not be replaceable by two (or more) computer scientists with an undergraduate degree in the same area. [^69]: This relaxes assumption [\\[itm:constant-returns-to-scale\\]](#itm:constant-returns-to-scale){reference-type=&quot;ref&quot; reference=&quot;itm:constant-returns-to-scale&quot;} on . [^70]: Such flexibility does justice to [@Baumol1965] original insight, which started out as an empirical observation on the relative pay of the performing arts, and not as an(other) *iron* law of wages (cf. @Malthus1798). [^71]: These additional, scarce opportunities may be awarded to individuals (or firms, or regions) based on easy but imperfect measures (for example, standardized test scores). They may also be awarded based on probabilistic predictions on future performance (think past awards), further increasing a self-reinforcing dynamic. In the worst, most inequitable (and inefficient case), they are awarded based on meaningless, randomly occurring differences (for example, mental state on day of testing), haphazard selections (for example, first come, first serve) or systematic measurement bias (for example, habitus expectations by assessors). [^72]: This relaxes assumption [\\[itm:easy-entry-exit\\]](#itm:easy-entry-exit){reference-type=&quot;ref&quot; reference=&quot;itm:easy-entry-exit&quot;} on . [^73]: Such as job opportunities on professional networks [@Benkler2006], the marks of social distinction received at the dinner table (@Bourdieu-1984-aa, recently @Hartmann2002), academic peer citations [@Jackson1968; @Merton1988], or plain social influence [@Asch]. [^74]: This is precisely the kind of critique from which the first theorem of welfare economics is rightly immune: it assumes *given distributions* and it --- as pareto optimality in general --- have nothing to say about the virtue or vice of these original allocations. This is, however, the kind of critique that must be waged against any neoclassical, or otherwise account that swiftly jumps from formal logic to normative judgement to policy implication without taking due note of these limitations. [^75]: Marginal utility diminishes, but does not turn negative. Consequently, the rich will still enjoy greater utility, if only slightly more. [^76]: The canonical version can be found in (p. . [^77]: When positional consumption is a pure [pd]{acronym-label=&quot;pd&quot; acronym-form=&quot;singular+short&quot;} cooperation problem, it becomes not just inequitable, but inefficient, too. When defection (buying BMWs) is widespread, curbing positional consumption is even . Economist [Robert H. Frank](http://www.robert-h-frank.com) has long ([-@Frank1987]) argued provocatively that positional taxation may benefit the rich and recently testified for a [pct]{acronym-label=&quot;pct&quot; acronym-form=&quot;singular+short&quot;} in front of the US [House Financial Services Committee](http://financialservices.house.gov/) on May 16, 2007. [^78]: This violates the assumption [\\[itm:infinite-buyers-sellers\\]](#itm:infinite-buyers-sellers){reference-type=&quot;ref&quot; reference=&quot;itm:infinite-buyers-sellers&quot;} of . [^79]: Historically, at least, public companies (the Dutch East India Company in 1602) and banks (Venice-based Medici bank in 1397) predated capitalism, and to this day, [ifis]{acronym-label=&quot;ifi&quot; acronym-form=&quot;plural+short&quot;} urge nascent &quot;capitalisms&quot; to legislate corporate law and build financial intermediaries. [^80]: Rent control is famously derided as &quot;the next best way to destroy a city&quot; aside from carpet bombing. [^81]: In addition to the mere complexity of such regulatory intervention, price controls are also prone to rent-seeking clients. As the gains (and losses) will often be obvious to, and concentrated in some citizens, they may lobby their government to affect changes [@Peltzman1976; @Posner1975; @Krueger1974]. As in oil-rich rentier states [@Beblawi1990], widespread zero-sum games may corrupt politics. [^82]: Such as in Article 9, Section 3 of the German Basic Law. [^83]: A cash-flow based, post-paid [pct]{acronym-label=&quot;pct&quot; acronym-form=&quot;singular+short&quot;} (recently @McCaffery2002 [@McCaffery2005]), a [lvt]{acronym-label=&quot;lvt&quot; acronym-form=&quot;singular+short&quot;} [@George1879], and, if necessary, a [nit]{acronym-label=&quot;nit&quot; acronym-form=&quot;singular+short&quot;} to fight structural unemployment or working poverty (first suggested by Milton @Friedman1962), and a [wt]{acronym-label=&quot;wt&quot; acronym-form=&quot;singular+short&quot;} on net worth to rein in extreme inequity are to respond to these inequities (p. ). Notably, even such sharply progressive taxation, may be unable to mitigate the inequities (and welfare losses) of monopsony employer labor markets. Potentially, monopsony employers will counteract progressive schedules by further lowering their wages. Some regulatory interventions, especially a right to strike and to unionize, may be essential. [^84]: Rich people are more likely to be creditors, and poor people more likely to be debtors. Telling, as [@Coggan2011] does, the history of mankind as the struggle between debtors and creditors [paraphrasing @Marx-1867-aa] may be roughly adequate and enlightening, but the simplification carries only so far. As [@Coggan2011 K6-24-04] himself points out, German hyperinflation wiped out the (often nominally denominated) savings of the middle class, but left relatively unaffected the (often real denominated) wealth of the upper class and the subsisting lower class with neither debt nor savings. [^85]: There is a case to be made for deliberately sharing the burden of systemic and unsustainably high levels of debt, as may be case with Greek sovereign debt in the 2007ff financial crisis [@Coggan2011]. As @Coggan2011 reminds us, we do not need inflation to achieve that; a (partial) default will do the trick, colloquially referred to as a &quot;haircut&quot; in the 2007ff financial crisis. [^86]: [\\[fn:3components\\]]{#fn:3components label=&quot;fn:3components&quot;} In his treatise on the economic costs of global warming, @Stern-2006-aa lists three components to discount to future utility: future people may value an additional unit of consumption less (or more) if they have more (or less) output available overall (elasticity of marginal utility). expected but *exogenous* growth, such as a chance discovery of a new technology, may increase future output without requiring present people to give up anything. people will discount future utility simply because it is in the future and uncertain (the pure discount rate of the future) [@Stern-2006-aa 52]. [^87]: Idiosyncratic risk of specific investments are diversified or hedged away. Risk-free interest rates still include a premium for *systematic* (not systemic) or aggregate risk, such as the continued existence of life on earth. The remaining interest component, the pure discounting of future utility, represents the hedonic loss of present, myopic individuals. [^88]: Specifically, saving (below @Solow1956&#39;s optimal rate) without interest is a @Kaldor1939-@Hicks1939 improvement: future utility is greater than present cost. Interest can be considered an intertemporal sidepayment from the future to the present. With interest, sub-optimal level saving can be pareto improved: upon saving more, both present and future generations (and selves) are at least equally well off. [^89]: Myopia implies a *shift* in the supply curve of saving. For any given interest rate, people will supply less. [^90]: *Endogenous* business cycle durations range from 3--5 years for lagged inventory decisions [@Kitchin1923], 7--11 for fixed investment [@Juglar1862], 15--25 for infrastructural investment [@Kuznets1930] and 45-60 for technological revolutions [@Kondratiev1925]. For a recent empirical test of all four business cycle theories, see [@Korotayev2010]. The latter two of these are probably beyond reliable anticipation and certainly beyond the short-term. They are also contested as empirical artifacts [@Howrey1968] or restricted to heterodox, evolutionary economics [@Modelski2010]. [^91]: [rbct]{acronym-label=&quot;rbct&quot; acronym-form=&quot;singular+short&quot;} proponents consequently argue against state interventions to counteract these exogenous shocks. [^92]: For a fully-fledged account, involving the debt, equity and asset market as well as monetary and regulatory correlates of *Maniacs, Panics and Crashes*, see [@KindlebergerAliber-2005-aa]. Herding has also been complemented and expanded into the (heterodox) debt-deflation theory of economic cycles, where credit cycles magnify initial bubbles, panics or shocks [@Fisher1933]. [^93]: Generous lay-off protection has been hypothesized to contribute to &quot;eurosclerosis&quot;. [^94]: According to the opposing, now minority view of [ret]{acronym-label=&quot;ret&quot; acronym-form=&quot;singular+short&quot;}, fiscal expansion will be ineffective, because market participants will correctly anticipate that current deficit spending (expansion) will be offset by future tax hikes (contraction). Anticipating such future losses, they will hoard more cash and equivalents today, than they would otherwise, thereby negating any current period effect. Even though controlled experiments are not readily available at the level of entire economies, deficit spending seems to work, at least a little. Here, for once, human ignorance (of [ret]{acronym-label=&quot;ret&quot; acronym-form=&quot;singular+short&quot;}) is not only bliss, but also a blessing for all of us. [^95]: A democratic polity can public for private demand as it wishes (p. ), but any such balance will not affect aggregate (public *and* private) demand. Democracies can call this shot, but it will not alleviate slumps. [^96]: The 2009 German *cash-for-clunkers* scheme --- for all its infamous waste, cannibalizing effects and blatant clientelism --- included such an incentive: car buyers had to top-up the subsidy with own savings to buy a new car. [^97]: To the extent that it serves as an automatic stabilizer and/or pools the near-universal risk of unemployment, &quot;unemployment insurance&quot; is, again, a misnomer. The demand stabilization bought by unemployment benefits are a public good, idiosyncratically financed out of a specific tax in many countries. [^98]: (German) automatic stabilizers have recently faired well during the 2007ff financial crisis and received great praise by [ifis]{acronym-label=&quot;ifi&quot; acronym-form=&quot;plural+short&quot;} (@IMF-2008-ab [20], @WorldBank2008 [19]) and experts [@BofingerFranz-2007-aa 8]. [^99]: The 2009 [vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;} break handed out to the hospitality sector by the new liberal-conservative coalition in Germany is a recent, brazen example. [^100]: Technically, central banks can lower the (federal funds) interest rate by buying (back) government bonds (*[omo]{acronym-label=&quot;omo&quot; acronym-form=&quot;singular+short&quot;}*), pump cash in the economy by buying assets () or accept riskier collateral from commercial banks (*qualitative easing* [@Buiter2008]). [^101]: &quot;Monetary expansion&quot; is thereby a misnomer. [^102]: This is the (Post-)Keynesian consensus. Strict monetarists argue that the money supply *always* remains at equilibrium without government intervention. [^103]: The second demographic transition delivered low, often below-replacement level [tfr]{acronym-label=&quot;tfr&quot; acronym-form=&quot;singular+short&quot;} and low mortality (@Davis1945, restated by @Caldwell-1976-aa). US 2009 estimated [tfr]{acronym-label=&quot;tfr&quot; acronym-form=&quot;singular+short&quot;}: 2.05 [@CIA2009], Germany 2009 estimated [tfr]{acronym-label=&quot;tfr&quot; acronym-form=&quot;singular+short&quot;}: 1.41 [@CIA2009], EU-25 2002 [tfr]{acronym-label=&quot;tfr&quot; acronym-form=&quot;singular+short&quot;}: 1.37 [@Demeny-2003-aa 2]. Life expectancy at birth for EU-25 is 69 years for males and 78 years for females [@Demeny-2003-aa 2] [^104]: When understood endogenously, strata of underqualified workers are also a real (if not nominal) dissaving in human capital. Saving, then, means to (publicly) educate most everyone to be competitively productive in a skill-based economy that outsources or offshores much of manual labor. [^105]: It does not matter for the economy-wide savings rate whether citizens save into privately offered financial products or state-guaranteed (often [paygo]{acronym-label=&quot;paygo&quot; acronym-form=&quot;singular+short&quot;}) schemes. I discuss the (somewhat epiphenomenal and misconstrued) later (p. ). [^106]: Real existing (public) pension schemes are a lot murkier and less (p. . Frequently, pension schedules feature equity or other policy goals such as child-rearing. In these, more realistic cases, the link between contributions and benefits is distorted. [^107]: A neologism suggested by @Van-den-Berghe-1981-aa to replace the clumsy &quot;ethnic group&quot;. [^108]: Formally, the trade theory of absolute advantage posits two countries, one factor of production (labor), perfect factor mobility within party, no factor mobility between parties, and constant returns to scale. [^109]: Formally, the trade theory of comparative advantage posits two countries, one factor of production (labor), perfect factor mobility within party, no factor mobility between parties, and constant returns to scale. [^110]: Formally, Hekscher-Ohlin trade theory posits two countries, two factors of production, perfect factor mobility within party, no capital mobility between parties, and constant returns to scale. [^111]: An effect known as *brain drain*. [^112]: Formally, [ntt]{acronym-label=&quot;ntt&quot; acronym-form=&quot;singular+short&quot;} relaxes perfect competition assumption [\\[itm:constant-returns-to-scale\\]](#itm:constant-returns-to-scale){reference-type=&quot;ref&quot; reference=&quot;itm:constant-returns-to-scale&quot;} of (p. ). Specialization pays, because marginal costs of production fall (positive returns to scale). Clustering pays, because network effects (such as in the transmission of innovation, according to @Bass1969) reward and reinforce tightly-nit networks (technically a group of nodes with some degree of heightened interconnectedness). For a brilliant introduction to network theory, see [@Kleinberg-2009-oz]. [^113]: Such divergence between *formal* and *real* factor mobility and market flexibility is crucial not only to trade theory, but also economic policy making. I return to this issue when I discuss the theory of (p. ). [^114]: German reunification provides a realistic example for this scenario. Eastern workers were less productive than their western compatriots, but soon expected to be paid equally. Arguably, labor costs in the East and the West converged too far and too soon causing structural unemployment. [^115]: This does not contradict a (p. ). Trade unions --- supply cartels by another name --- are justified if and to the extent that they help to *balance* the playing field between (monopsony) employers and atomized workers. By definition, unemployment suggests that the field has tilted too far: there are surplus workers that cannot find a willing employer at the price imposed by the supply cartel. Unemployment resulting from wage rigidity is the welfare loss equivalent to the reduced output of a monopoly provider. Recognizing a level playing field in the real world will be very difficult. Employers will always (and sometimes truthfully) argue that they would hire more workers, if only the wages were lower. Unions will always (and sometines truthfully) argue that employers could pay more without laying off workers. [^116]: I use the term more narrowly and do not, as others, include market failures or protection. are adressed by government to improve (p. , not to improve convergence. Protection, such as [isi]{acronym-label=&quot;isi&quot; acronym-form=&quot;singular+short&quot;} is not available within the closed economy. By industrial policy, I here mean the discretionary and deliberative intervention in the economy by government, maybe best captured in the french term &quot;gouvernement economique&quot;. [^117]: The failure of bank regulation in the run-up to the 2007ff financial crisis has recently shown that government is easily ill-equipped to prevail in this &quot;cat-and-mouse-game&quot;. [^118]: I cannot provide here, but merely refer readers to a thorough discussion of these and related legal norms of the rule of law in market regulation. [^119]: [@Zurn-2000-aa] speaks of a broader incongruence between the popular inputs and (economically constrained) outputs of EU-level democracy. I return to in the conclusion. [^120]: For a more formal treatment of *systems competition* see [@Sinn2004]. [^121]: This is a hypothetical and admittedly implausible example. There may be other compelling reasons to legislate health and safety, at the national level, for hairdressers, too, including fairness and simplicity. [^122]: Nominal base, sadly, differs from the effective (p. ). [^123]: As I explained in , the economy-wide savings rate is a commons, too. A pigouvian levy on dissaving such as a [pct]{acronym-label=&quot;pct&quot; acronym-form=&quot;singular+short&quot;} internalizes the social cost of consuming over the government-set optimal savings rate [@Held2010a]. [^124]: One might think that a redistributive tax is *specific* in base, but that is a misunderstanding. Redistribution favors some, and disfavors others, but it always affects *all* people somehow. Indeed, a *specific* redistributive tax would be a contradiction in terms: redistributing only between some entities in turn constitutes a redistribution between the included and the exempted entities. Moreover, specific inclusion or exemption of entities based on something other than the accepted equity norm (for example, ability to pay) conflicts with liberal-democratic norms of non-discrimination [^125]: Even entities with a privately known risk of zero should be included, as low-risk entities may otherwise find it attractive to misrepresent their privately known risks. This extreme case of a privately known risk of (close to) zero cannot be justified under an efficiency norm of , as everyone is made better off by making some entities worse off. Instead, it requires the stronger norm (p. )[@Kaldor1939; @Hicks1939]. Resolving a lemons market will cost some people less than it will benefit others. [^126]: Even those who receive *no* utility from the public good should pay, both because their non-utilizing cannot be observed (non-exclusion) and because any additional utilization does not create costs (non-rivalry). This case of little or no utility from public good again requires , not [@Kaldor1939; @Hicks1939]. &lt;!-- %The costs for providing public goods should likewise incur generally to all people. %Even those who receive \\emph{no} utility from the public good should pay, both because their non-utilizing cannot be observed (non-exclusion) and because any additional utilization does not create costs (non-rivalry)\\footnote{This case of little or no utility from public good again requires \\hyperref[sec:kaldor-hicks]{Kaldor-Hicks}, not \\hyperref[sec:pareto]{Pareto efficiency} \\citep{Kaldor1939,Hicks1939}.}. %Schedules for pooled risks should be flat, or at maximum proportional where a flat tax would create undue market distortions between entities of different size\\footnote{The complication of a possibly desirable proportional schedule applies only when entities \\emph{are} in fact of different size. %This is the case for firms, where the fixed cost of a flat tax would place smaller firms at a disadvantage versus larger firms. %I resolve this problem of different sizes later when introducing desiderata \\ref{des:personal-taxation} of \\hyperref[sec:PersonalTaxation]{taxing only natural persons}. %Natural persons are, in an economic sense, entities of identical size. %weight, impact? %}. %Proportional taxation, in this case, is not based on an equity consideration. --&gt; [^127]: Fiscal stimulus occurs only government spending *exceeds* tax revenue in bad times. Resulting debt has to be paid back in good times with increased tax receipts. [^128]: Confusion, alas, *is* everywhere. For example, a [ftt]{acronym-label=&quot;ftt&quot; acronym-form=&quot;singular+short&quot;} was frequently touted by the left to pay back sovereign debt, or to redistribute between rich and poor. However, a [ftt]{acronym-label=&quot;ftt&quot; acronym-form=&quot;singular+short&quot;}, as originally conceived by [@Tobin1970], is a *Pigouvian levy*, meant to disincentivize short-term trading. Accordingly, it should be judged on *those* merits, and on those *only*: can it curb the commons of &quot;speculation&quot;, or, more precisely beauty-contest [@Keynes1936] herding [@Banerjee-1992-aa]? Whether or not a [ftt]{acronym-label=&quot;ftt&quot; acronym-form=&quot;singular+short&quot;} is, in fact, able to also raise revenue, let alone *redistribute* according to some norm of fairness is dubious, at best. Traders may be able to push the additional cost of a [ftt]{acronym-label=&quot;ftt&quot; acronym-form=&quot;singular+short&quot;} down their customers, including small-time bank customers. For a Pigouvian levy, this is a good sign: the price signal of an exhausted commons *should* perculate through the entire economy. For a *redistributive* tax, however, such dubious incidence is arbitrary: redistributive taxes must fall only on those intended by legislators. [^129]: Political economists such as [@Dehejia1999] have recently added more nuance to the spectre of tax competition. It might, for example, not quite lead to the bottom, as attracted investment also has diminishing returns [-@Dehejia1999 416], and may further favor smaller economies more than larger ones, where the ratio between attracted investment and foregone public spending is higher. For a succinct overview of tax competition theories, also see [@Wilson1999]. [^130]: The German [lbt]{acronym-label=&quot;lbt&quot; acronym-form=&quot;singular+short&quot;} is a sad exception. While it is ostensibly meant to raise general, not specific revenue for municipalities, it is set by local government. Predictably, many municipalities find themselves forced to lower [lbt]{acronym-label=&quot;lbt&quot; acronym-form=&quot;singular+short&quot;} rates. [^131]: It is usually assumed that incidence applies only to *indirect* taxes such as [vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;} and [cit]{acronym-label=&quot;cit&quot; acronym-form=&quot;singular+short&quot;}. I find this limited application of incidence implausible. The concept remains applicable for *direct* taxes, including [pit]{acronym-label=&quot;pit&quot; acronym-form=&quot;singular+short&quot;} and [wt]{acronym-label=&quot;wt&quot; acronym-form=&quot;singular+short&quot;}, too. A [pit]{acronym-label=&quot;pit&quot; acronym-form=&quot;singular+short&quot;} on labor incomes, for instance, may partially fall on employers when labor markets are very tight: tax-depressed worker supply may cause employers to pay more. Even a [wt]{acronym-label=&quot;wt&quot; acronym-form=&quot;singular+short&quot;} may fall on people other than the owners when demand for their collateral is sufficiently inelastic: interest rates may rise when private capital becomes less abundant. [^132]: A proportional [payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;} is actually equivalent to a [vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;} in its incidence. A [payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;} taxes (only) labor income before it is spent (prepaid), a [vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;} taxes (only) labor income after it is spent (postpaid). [^133]: Saving is a third alternative if you subscribe to a *[y2c]{acronym-label=&quot;y2c&quot; acronym-form=&quot;singular+short&quot;}*: when you consider returns on capital a genuine *change* in welfare, you can substitute present spending for *more* future spending. If, instead, you believe in a *[osn]{acronym-label=&quot;osn&quot; acronym-form=&quot;singular+short&quot;}*, returns on capital are just compensation to risk, uncertainty and delayed pleasure: welfare is *postponed*, but not changed. Consequently, under a [osn]{acronym-label=&quot;osn&quot; acronym-form=&quot;singular+short&quot;}, you can only substitute current consumption for same future consumption, and have no alternative but to spend your money at some point. For a summary of the savings norm debate see [@Held2010a] or, in the brilliant original, [@McCaffery2005 819]. [^134]: I use consumption and, broadly equivalently, labor as examples here because unlike income taxation ([pit]{acronym-label=&quot;pit&quot; acronym-form=&quot;singular+short&quot;}), they do not require me to decide on a savings norm ([y2c]{acronym-label=&quot;y2c&quot; acronym-form=&quot;singular+short&quot;} or [osn]{acronym-label=&quot;osn&quot; acronym-form=&quot;singular+short&quot;}). Both norms are unattractive in the extremes and only a [pct]{acronym-label=&quot;pct&quot; acronym-form=&quot;singular+short&quot;} can resolve the tension (@Held2010a). Still, it is important to point out that, no matter the savings norm, savers in a closed economy have limited or no alternative to investing at home. Therefore, the incidence of a [pit]{acronym-label=&quot;pit&quot; acronym-form=&quot;singular+short&quot;} will also be relatively well-defined in a closed economy. [^135]: As explained in the above, both a [payroll]{acronym-label=&quot;payroll&quot; acronym-form=&quot;singular+short&quot;} and a [vat]{acronym-label=&quot;vat&quot; acronym-form=&quot;singular+short&quot;} burden low-wage labor. [^136]: In this extreme, unlikely case, the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of a welfare handout is equivalent to the [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;} of a price floor on labor at the same level. [^137]: People leaving the labor market in this scenario has nothing to do with laziness. The legislated minimum standard of living is after all considered minimally acceptable. People who opt for welfare instead of poorly paid work may not have a choice. [^138]: This very brief account follows the *quantity* theory of money. Competing theories are discussed in the below. [^139]: If for no other reason, fiat money would be preferable to intrinsically valuable moneys *because* governments can adjust its supply to output in the long run and at little cost. By contrast, specie moneys are costly to extract and their supply changes arbitarily on new discovery. The futility of a gold-based economy is aptly summarized by investor Warren Buffett: &gt; *&quot;\\[Gold\\] gets dug out of the ground in Africa, or someplace. &gt; Then we melt it down, dig another hole, bury it again and pay people to stand around guarding it. &gt; (...) &gt; Anyone watching from Mars would be scratching their head.&quot;*\\ &gt; --- Warren Buffett [^140]: Actually, the balance sheet of the central bank records the acquired bond as an asset, and the currency as a liability. [^141]: Central banks are customarily barred from buying government bonds directly from government to increase transparency. By buying government bonds from a middlewoman, central banks can still &quot;print money&quot;. [^142]: They are unnecessary according to . [^143]: They are ineffective according to the policy ineffectiveness proposition. [^144]: [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} and its macroeconomic components, while commonly reported, falls short of a comprehensive account of the economy. Rather than a measure of prosperity, it indicates market value level of economic *activity*. Roughly, [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} is to prosperity, as a corporation&#39;s cash flow statement is to its income statement. [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} growth and positive cash flows are a necessary, but not a sufficient condition to prosperity. [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} does not measure changes in net worth as an income statement would: for example, an earthquake (or nuclear power disaster, or both) can raise [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} because of reconstruction *activities*, even if it actually wiped out *assets*. [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} also excludes environmental degredation or use of natural resources. Lastly, [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} falls short, because it records only activity with a market value; economic activity in the household or family are not included, as are social costs or utility of market activities (externalities). [^145]: GDP-related metrics fall short, again: [gfcf]{acronym-label=&quot;gfcf&quot; acronym-form=&quot;singular+short&quot;} and consequently the gross savings rate does not include capital depreciation. It also does not include real dissavings on goods without a market value, such as the population growth rate or environmental quality. Consequently, gross savings rates may be overstated in existing accounts. [^146]: Justifying a choice of the mixed economy does not mean that it can be reduced to a positive question. Ends and means such as redistribution may, and maybe should, remain contested. [^147]: [gdp]{acronym-label=&quot;gdp&quot; acronym-form=&quot;singular+short&quot;} also does not, as implied in distinguish between public consumption and public investment, but lumps both together in [G]{acronym-label=&quot;G&quot; acronym-form=&quot;singular+short&quot;}. [^148]: Strictly speaking, this would be the case only for a perfect tax with a zero [dwl]{acronym-label=&quot;dwl&quot; acronym-form=&quot;singular+short&quot;}. [^149]: Arguably, the past two centuries of growth in the West are to some extent due to the exploitation of fossil fuels. [^150]: Preparing a Haig-Simons account on illiquid, public or intangible assets will be difficult. As far as possible, accounting should rely on market prices, but will also have to rely on some planned [cba]{acronym-label=&quot;cba&quot; acronym-form=&quot;singular+short&quot;}. [^151]: In my ongoing dissertation at [bigsss]{acronym-label=&quot;bigsss&quot; acronym-form=&quot;singular+short&quot;} on the pluralist and deliberative politics of taxation, I call this misunderstanding *Bastard Keynesianism*, gone badly awry. I hypothesize that because people do not properly understand the Haig-Simons identity (and the economic cycle), they will erroneously assume that any tax on consumption will hurt the economy. In truth, as I argue here, an economy can take on any trade-off between present and future consumption, if only it is phased in slowly enough. This confusion, I argue, diverts people away from optimal and fair taxation and effectively curtails the democratic sovereign in pluralism. [^152]: This distinction seems straightforward. Yet, similar to bastard Keynesianism, much of public debate of finance and economicsis is marred by great confusion about these basic terms. @McCaffery2005 explains how an inconsistent treatment of debt and capital gains opens up income taxation to &quot;tax evasion 101&quot;. [^153]: Credit and concomitant asset bubbles can arise for several reasons and according to competing theories, including beauty-contest-type [@Keynes1936], herding [@Banerjee-1992-aa] and excessive monetary expansion [@Stiglitz2010]. An authoritative history of financial crises is [@KindlebergerAliber-2005-aa]. [^154]: Conversely, credit crunches, in part, redistribute wealth from presence to the future. By keeping the economy below its potential aggregate supply, credit crunches also waste capacity [^155]: Conventionally, as @Stiglitz2010 points out, a default wipes out shareholders (equity finance) and makes creditors (debt finance) into the new shareholders. In the 2007ff financial crises, as elsewhen, economic and political power often intervened to distribute the pain of default differently [@Stiglitz2010]. [^156]: @Coggan2011 recently told the history of all hitherto existing society as the struggle between debtors and creditors, to paraphrase [@MarxEngels-1848-aa]. According to both @Coggan2011 and [@Stiglitz2010], household debtors and governments assumed much of the burden during the 2007ff financial crises, causing, in part, the pursuant sovereign debt crises. [^157]: Traditionally described as an (inward) move along the [@Phillips1958] curve. [^158]: An upward shift of the [@Phillips1958] curve. [^159]: In addition, the costs of later disinflation may be large. Prescriptions for disinflation and expected costs differ. [^160]: The second demographic transition (@Davis1945, restated by @Caldwell-1976-aa) delivered low, often below-replacement level [tfr]{acronym-label=&quot;tfr&quot; acronym-form=&quot;singular+short&quot;}. [^161]: The greatest, widest-ranging market failure in the history of mankind, according to [@Stern-2006-aa]. 3.0.3 Efficiency and Equity Equity and efficiency are not a zero-sum trade-off. Sometimes, when the cake is sliced up more equitably, it may grow. Formulations of such positive-sum relationship between the two vary, and only some examples can be given here. Extreme inequality in post-tax market outcomes may also conflict with the efficiency goal of full factor employment. Most (post)industrial societies have legislated socially accepted minimal incomes. This legislation takes the form of statutory minimum wages or unemployment benefits. Either way, an effective or de-facto price floor for wages is established.63 People with productivities below this price floor will not find employment, leading to a DWL of structural unemployment.64 Crucially, the level of minimal incomes depends on the broader configuration of tax incidence, indirect taxes in particular. When taxes are relatively flat and fall on labor or consumption, the real costs of living for low-income earners rise. This tax wedge can take the form of (flat or proportional, payroll-style) social contributions, VAT or a relatively flat or proportional personal income tax: either way, more money has to be made on the market for the same living standard (in Purchasing Power Parities, PPP). The implications for tax design are twofold: 3.0.3.1 Structural Unemployment as Endogenous. The only genuine fix to structural unemployment is of course more education to lift people, or at least their children, out of their low standards of productivity. Better education, to be sure, will require fresh ideas — but also lots of public resources. Well-funded public education, in turn, is endogenous to a fiscal configuration. 3.0.3.2 Structural Unemployment as Exogenous. Even from a static perspective, assuming low-productivity workers to be exogenous, some measure of tax relief for would-be working poor may enhance welfare. An efficient tax system should reduce the burden on low-productivity earners, to keep their post-tax socially acceptable minimum wages as low as possible. \\[des:low-price-floor\\] A desirable tax has a minimal incidence on low-productivity earners to lower their cost of living. References "],
["tax.html", "Chapter 4 Tax Choice 4.1 Tax Criteria 4.2 Base, Schedule and Timing 4.3 Taxes on Income 4.4 Taxes on Consumption 4.5 Taxes on Wealth 4.6 Non-Taxes 4.7 The Scores 4.8 The Old Hand — Direct, Progressive Taxes: PIT, CIT &amp; Dual PIT 4.9 The Streaker — Local Business Tax (LBT) 4.10 Impotent Redistribution 4.11 Durables 4.12 Introducing the PCT 4.13 Investment vs. Consumption 4.14 Gifts and Bequests 4.15 Imputed Income 4.16 Cross-Border Transactions 4.17 Family 4.18 The Formula for the PCT 4.19 Work/Play, A Constructed and Diffuse Border On Which Taxation Depends 4.20 Stratification, Elsewhere. 4.21 An Angel in Tax", " Chapter 4 Tax Choice 4.1 Tax Criteria 4.1.1 Optimality A system is efficient if it maximizes outputs for given inputs. But exactly what should be maximized? Efficiency norms vary and can conflict. I present here three popular definitions in increasing order of strength.65 Earlier entries are subsets of later entries, as visualized in . Pareto Efficiency. \\[sec:pareto\\] An allocation is pareto-optimal, when no one can be made better off without making someone else worse off. Pareto optimality, is, in fact not a pure efficiency norm but includes an equity component, too. Kaldor-Hicks Efficiency. \\[sec:kaldor-hicks\\] An allocation is Kaldor-Hicks efficient, when no one can be made better off without making someone else worse off by the same or a greater amount of disutility (Kaldor 1939; Hicks 1939). It can also be described as a pareto-optimal outcome where sufficient compensatory sidepayments can be made from the winners to the loosers. Social Welfare Optimum. \\[sec:swo\\] The pure efficiency norm is given by the utilitarian slogan “The greatest good for the greatest many” (Mill 1863). It knows no equity at all, but is concerned only with the sum total. It is also known as a social welfare optimum in game theory (for example, Oye (1985)). Selected Efficiency Norms, Where Area Equals Utility I choose as the weakest of these efficiency norms and note whenever a stronger efficiency norm is required for an argument. Pareto efficiency is also widespread efficiency norm that is convenient for its link to (page ). 4.1.1.1 In Dubio Pro Mercatus. The above posited perfect markets can be shown to work Adam Smith (1776)’s Invisible Hand (1776). More formally, the first theorem of welfare economics states that any competitive equilibrium will be Pareto optimal: no one could be made better off, without making someone else worse off66. For () efficiency, the burden of proof then lies on state interventions in . \\[sec:perfect-competition\\] It follows that a tax should affect market interactions as little as possible. 4.1.1.2 Minimize Deadweight Losses The efficiency loss that does occur when taxes alter market interactions is called a deadweight loss (DWL), illustrated in . It arises as follows: consumers and producers both reduce their buying and selling, as consumers face higher and producers lower prices. This reduces the welfare, or surplus of consumers and producers. Some of this loss is transferred to government. The net that is not recouped as tax revenue is the deadweight-loss of taxation. A desirable tax should have a minimal deadweight loss. \\[des:minimal-DWL\\] A desirable tax will maintain and strengthen incentives to maximize private, unobserved effort in work and investment. \\[des:Incentives\\] 4.1.1.2.1 Reformulations of DWL. The idea of the DWL of taxation can be reformulated in a number of ways. Factor Unemployment refers to DWLs in markets of factors of production, when tax wedges cause incomplete employment of labor or capital. This problem is particularly dire in the indivisible supply of low-productivity labor, where entire strata of workers may be unable to find gainful employment at post-tax wages. This problem of is further discussed in and addressed in desideratum \\[des:low-price-floor\\]. Taxes are Anti-Growth. A sophisticated version of this sentiment will also argue that widespread DWLs waste economic resources in the medium and long run. Neutrality. The DWL of a tax can also be described as its impact on a hypothetical, before-tax, pareto optimal market. An efficient, desirable tax is neutral in the sense of leaving pre-tax relative prices unchanged (McCaffery 2005, 849). 4.1.1.3 Responsiveness is Elasticity. This responsiveness is called the price elasticity of demand and supply respectively. It is defined as the percent change in quantity demanded (supplied) over a percent change in price (???). Where \\(E-{d}\\) (\\(E-{s}\\)) is the price elasticity of demand (supply), \\(Q-{d}\\) (\\(Q-{s}\\)) the quantity demanded (supplied) and \\(P\\) the price, it holds that: \\[\\label{eq:PED} E-{d}=\\frac{\\%\\ \\mbox{change in quantity demanded}}{\\%\\ \\mbox{change in price}}=\\frac{\\Delta Q-{d}/{Q-{d}}}{{\\Delta P}/{P}}\\] and \\[\\label{eq:PES} E-{s}=\\frac{\\%\\ \\mbox{change in quantity supplied}}{\\%\\ \\mbox{change in price}}=\\frac{\\Delta Q-{s}/{Q-{s}}}{{\\Delta P}/{P}}\\]. 4.1.1.4 Elasticities of Supply and Demand become more intuitive when applied in polar cases of perfect inelasticity () and perfect elasticity (). 4.1.1.4.1 Perfectly elastic supply (demand) does not respond at all to changes in price. Whatever the price, suppliers (buyers) will always produce (buy) the same quantity. It is displayed by a perpendicular supply (demand) curve in . 4.1.1.4.2 Perfectly inelastic supply (demand) responds maximally to minimal changes in price. When the price drops (rises) below a certain point, suppliers (buyers) will cease stop producing (buying) any quantity. Below (above) the horizontal supply (demand) curve in , no production (buying) occurs at all. Perfectly Inelastic Supply and Demand Perfectly Elastic Supply and Demand Unit Elastic Supply and Demand 4.1.1.4.3 Unit elastic supply (demand) responds proportionally to changes in price. Along a supply (demand) curve of slope \\(1\\) (\\(-1\\)), producers (buyers) produce (buy) one percent more quantity for each percent increase (decrease) in price67. 4.1.1.5 Elasticities and DWLs. Because DWLs stem from market participants tax-depressed activity, less price responsive activities will be more efficient to tax. Relatively inelastic supply and demand will have relatively smaller DWLs (Ramsey (1927), Piatkowski and Jarmuzek (2008, 16)). This is illustrated in figures \\[fig:DWL\\] and \\[fig:smaller-DWL\\]. displays the same market with the same tax, only with relatively less elastic (= more inelastic) supply and demand. Compared to the original DWL-scenario in , the DWL in is absolutely smaller and smaller relative to the recouped tax revenue. Because both producers and buyers are less responsive to the tax-induced price change, they each loose less of their surplus. A doable specification of desideratum \\[des:minimal-DWL\\] on minimal DWLs is: A doable tax falls on inelastic bases. \\[des:tax-inelastic\\] 4.1.1.6 Determinants of Elasticity. Elasticities are hard to quantify: if prices change, they typically change only marginally. Elasticities are typically known only locally, that is, in the vicinity of equilibrium prices. Elasticities are determined by a host of factors. The price elasticity of supply depends, amongst other things, on the mobility of labor and capital employed in production, the availability of substitute consumers or the ability to substitute current for future sales (a.k.a. liquidity). The price elasticity of demand depends, amongt other things, on the availability of substitute goods, brand loyalty or ability to substitute current for future consumption. 4.1.2 Incidence 4.1.2.1 Flypaper Theory is False. To illustrate this dynamic, first consider . In this market (actually the same as in ), a tax is placed on the supplier. For any given market price, the producer is now willing to produce less. The supply curve shifts upwards by exactly the amount of the tax. Assuming constant demand, the curtailed supply will raise the price above the prior equilibrium. Both consumers and producers end up paying for the tax. Consumers have to buy at higher prices. Producers have to sell at lower prices. In this case, consumers and producers carry an equal share of the tax burden.68 The naive “flypaper theory of tax incidence” is false: the state cannot easily legislate who should pay a tax. Instead it must anticipate market reactions to the tax and gauge the effective incidence, irrespective of where the tax was originally levied.69 4.1.2.2 The Relatively Less Elastic Bears Relatively More. The incidence is not only different from where a tax is levied, it can also fall asymmetrically on demand and supply. More specifically, the incidence of a tax is determined by the relative price elasticities of supply and demand: whoever is less elastic (more inelastic) bears a relatively greater share of the burden. illustrates the incidence of a tax on suppliers in a market with relatively less elastic (more inelastic) demand. It is the same market as in and \\[fig:DWL\\], only the elasticity of demand is different: buyers react relatively less to price changes than sellers. As a result, buyers are willing to pay quite a lot to maintain a similar quantity. Sellers, by contrast, will curtail their production substantially when prices drop. The market reaches a post-tax equilibrium at a price much greater than pre-tax equilibrium for consumers, and a little lower for producers. Consumers end up bearing most of the burden. 4.1.2.3 Incidence is Hard. As argued in the above, relative price elasticities of demand and supply can be hard to ascertain ex-ante, and they are affected by many factors. To reliably gauge the effective incidence of a tax, government would have to know the elasticities in a myriad of markets and constellations with great precision. This will be very costly and likely unsuccessful. To effectively redistribute between natural persons only, as per desideratum \\[des:personal-taxation\\], a doable tax will fall on market interactions where relative elasticities of demand and supply can be easily and reliably ascertained. Government can achieve non-random, normatively justifiable redistribution only when: A doable tax has a well-determined incidence. \\[des:determined-incidence\\] 4.1.3 Natural Personhood 4.1.3.1 Corporations Corporate bodies have rightfully been characterized as fictions. The welfare of all these non-natural, private juristic persons ultimately accrues to natural persons as their owners, workers or consumers70. Equity considerations do not apply at the corporate level71. As per desideratum \\[des:redistribution-and-revenue-are-one\\], . \\[sec:fiscal-redistribution-and-revenue-are-one\\] \\[des:redistribution-and-revenue-are-one\\] It follows that for both these components: A doable tax falls only on natural persons. \\[des:personal-taxation\\] 4.1.3.2 Families \\[sec:love-marriage\\] “Love and marriage, love and marriage Go together like horse and carriage (…) Try, try, try to separate them: It’s an illusion.” — Frank Sinatra (1915-1998) Personal taxation becomes impossible when common usage, transfers and mutual obligations become so intensive, informal and intangible as to render individual accounts meaningless. In modern, functionally differentiated societies such mechanical solidarity is — for better or for worse — largely replaced by extensive, formalized and tangible organic solidarity (Durkheim 1893). Romantic cohabitation, marriage and the nuclear family alone remain as refuges of mechanical solidarity. Personal taxation of (married) couples and families becomes impracticable. They are therefore taxed as one entity, or at least under one rate. A problem arises: you have to add (two) potentially (very) different incomes72 together and tax them under a single rate. There are two ways to do this. First, you can apply the same schedule as for single households, which would punish people just for getting married or having children. For example, two people earning $50,000 each would, after marriage, be taxed under the higher rate applicable at $100,000. Secondly, you can divide the family income by the number of people in the household and apply the tax rate for this average income. In this case, the tax liability will depend on the distribution of incomes between the family members. If their incomes are starkly different, they will benefit from applying a progressive rate to their average income. This conundrum cannot be resolved: it is the equivalent to Arrow (1950)’s Impossibility Theorem in taxation. Of three desirable things, you can only have two: a progressive schedule, neutrality towards marriage and tax liability independent of the distribution of income in a household (Moffitt (2003, 124), Dalsgaard (2005, 29)) . I have stated here that progression is desirable, and that people should not and cannot be punished for mechanical solidarity in family and marriage. It follows that, while undesirable, the tax liability will be dependent on the distribution of income in a household. The contradictions arising in the taxation of family and marriage point to more fundamental problems of drawing boundaries (page ) based on administrative and analytical abstractions (here: , page ) that are not unambiguously applicable in real life. As this particular problem affects all progressive taxes, it will not be further addressed here. The aggregate costs of taxation depend on the responsiveness, or elasticities of what is being taxed. The individual costs — who really pays — also depend on this responsiveness. 4.1.4 Timing 4.1.4.1 Haig-Simons Identity of Income 4.1.4.2 Two Savings Norms Much of the debate on taxation in recent years has concentrated on the relative taxation of capital and labor. This dichotomy is no longer empirically meaningful, and normatively contradictory. 4.1.4.3 Empirically: Marxism is Dead — Capital is no Longer a Class in Itself. \\[sec:CapitalNoClass\\] For personal taxation as per desideratum \\[des:personal-taxation\\], capital and labor incomes are an irrelevant, false dichotomy. Today, capital is no longer a class in itself: even middle class earners have substantial capital incomes, for example from capital-based pensions or life insurances (Grabka et al. 2007 XV). In a (happily) past world of bifurcation between workers and capitalists, balancing the taxation of labor and capital incomes made redistributive sense. When almost everyone is, to some extent, a capitalist, taxation of either of the two incomes does not neatly map on (page ). Any tax mix of labor and capital incomes will have indeterminate redistributive effects. 4.1.4.4 Normatively: Two Savings Norms Conflict. \\[sec:savings-norms\\] Capital, in the last instance, is saved labor income. There are two conflicting, equally plausible norms on how to treat saving. 4.1.4.4.1 Ordinary Savings Norm. \\[sec:OSN\\] According to the ordinary savings norm, people save to shift labor incomes within their lifetime, or between generations (McCaffery 2005, 819). They receive a return to capital to make up for the the risk borne and the pure discount of the future. Because the saver and the non-saver are same ex ante, they should not be taxed differently ex post. In short, people should not be punished to smooth out their consumption over time. 4.1.4.4.2 Yield-to-Capital Norm. \\[sec:Y2C\\] According to the yield-to-capital norm, the return to capital that people receive is additionally accumulated welfare that should be taxed. Under income taxation, these two norms are in fatal tension. Any mix of capital and labor income taxation will violate either of the two norms. There is no meaningful way to tell the one kind of (ordinary) saving from the other (yield) kind: they are the two sides of the same coin. 4.1.4.5 Structural Agnosticism. A tax regime can avoid this empirical futility and normative contradictions when it is agnostic in its treatment of labor and capital incomes: \\[des:structural-agnosticism\\] A doable tax will be agnostic towards labor and capital incomes. Second, Rawls suggests two distributive norms, the difference principle and the fair equality of opportunity.73 Under the difference principle, differences in endowments as well as subsequent social and economic inequalities are acceptable only if their granting also benefits the least fortunate.74 If inequities cannot be thereby justified, such undeserved differences (by birth or other random allocations) in endowment should be corrected by intervention for fair equality of opportunity. This latter norm of fair equality of opportunity is a pure equity norm on the equality of inputs. The difference principle is more complicated. It combines equity and efficiency norms. It is that weak pareto optimum (WPO),75 which maximizes the minimum payoff (maximin) in outcomes. illustrates the two Rawlsian norms of distributive justice in context. Selected Distributive Norms and Rawlsian Distributive Justice (in green) 4.1.5 Fairness 4.1.5.1 Why Rawls? Any comprehensive discussion of distributional justice is beyond the scope of this thesis. Rawls (1971) Theory of Justice is presented here for four reasons. First, justice as fairness serves as a convenient cut-off point on a (crudely) imagined continuum of theories of distributive justice from left to right, from egalitarianism to utilitarianism and liberitarianism.76 It is assumed that all allocative norms “left of” Rawls (such as Cohen 2000), stressing equity over efficiency will strictly prefer the PCT over the status quo. Conversely, theorists on the right end of the imagined continuum (for instance, Nozick (1974)) may not share all of the normative foundations of the PCT, or even prefer it over the status quo. Second, the difference principle is (page ) when I argue the normative superiority of the progressive taxation of consumption. Thirdly, in arguing that the PCT is the perfect tax, I attempt to establish it as an reflective equilibrium (Rawls 1971, 49) emerging from a pragmatic-spirited back-and-forth between what is , and what is McCaffery (2005, 856). Fourth, Rawls (1971)’ Theory of Justice ((1971)) is an end-state theory of distributive justice (Fried 1999, 1007), not a procedural prescription (for example Dahl (1989)). A political process, aside from (???) liberty, is not prescribed in its own right. It merely serves as an independent variable in ,explaining and criticizing why it may have failed to deliver the superior allocative outcome. 4.1.5.2 Implications for Tax Design. Two desiderata for tax design emerge from Rawls (1971)’s Theory of Justice. First, the principle of fair equality of opportunity introduces equity as a goal for taxation. It is a general and unspecific norm. Translating it into a desideratum for tax design hinges on an empirical assessment of opportunity in the real world. A specification of this norm must therefore await an inspection of the (page ) in modern society. \\[sec:inequality-dynamics\\] Second, the difference principle provides a more straightforward guideline for redistributive taxation. It implies that taxation should exempt from redistribution economic inequality if, and to the extent that, it makes the least fortunate better off. This may occur when able people require economic to exert maximum efforts (desideratum \\[des:Incentives\\]). Because axiomatically assumed perfectly competitive markets can be shown to be pareto optimal, it follows that: \\[des:difference-principle\\] A desirable tax exempts from redistribution economic inequality if, and to the extent that, it makes the least fortunate better off. Generally, for axiomatically assumed homo economicus to , she needs incentives. Desideratum \\[des:Incentives\\] is closely related to desideratum \\[des:minimal-DWL\\] (). 4.2 Base, Schedule and Timing Taxation offers highly structured choices along the three dimensions of base, schedule and timing, the first two of which are cross-tabulated in table. There can be no meaningful democratic rule on taxation that does not ultimately opt for a definitive mix of distinct combinations along those dimensions. The choice among these (already high-level) alternatives cannot be abstracted away, or relegated to experts: it hinges on irreducible moral and causal judgments (for example, McCaffery 2005). 4.3 Taxes on Income 4.3.1 Taxes on Capital Income 4.3.1.1 Corporate Income Tax (CIT) \\[sec:CIT\\] The cit taxes the earnings77 of an incorporated firm. It is usually proportional. Unincorporated firms do not pay cit, instead their owners owe pit on their business earnings.78 cits are proportional, with rates around 20% in the EU. Countries have recently created issued sectoral (trade, finance) or regional (Hong Kong, London) exemptions from the cit to attract investment (Genschel, Kemmerling, and Seils 2009; Ganghof and Genschel 2007; Genschel 2005). 4.3.1.1.1 Local Business Tax (LBT) \\[sec:LBT\\] Taxes on local businesses exist only in some OECD countries, including Germany. In Germany, local authorities can tax local business a progressive tax on their earnings. Local authorities set the rate of the tax (within federally mandated margins). Firms are taxable based on the location of their establishments. Earnings of firms with multiple establishments are weighed by the sum of wages and salaries paid. 4.3.2 Taxes on Capital and Labor Income 4.3.2.1 Personal Income Tax (PIT) \\[sec:PIT\\] Under the pit, all sources of personal income (labor, capital) are added and taxed under a single schedule. pit schedules are typically progressive. Marginal (not average!) tax rates in Germany range from 14% to 42% (Bundesministerium für Finanzen 2009). 4.3.2.1.1 Dual Income Tax (Dual PIT). \\[sec:Dual-PIT\\] The 2-pit is a variation on the pit, where schedules differ between capital and labor sources of income. 2-pits burden capital relatively less than labor incomes. The 2-pits was introduced to avoid the negative growth effects of high capital taxation (particularly through its , p. ) and to counteract capital flight. In Sweden, a pioneer of the 2-pit, the top marginal (not average!) rate for labor incomes is 55%. Capital incomes are taxed proportionally at 30% (marginal rate equals average rate). 4.3.3 Taxes on Labor Income 4.3.3.1 Payroll Taxes (Payroll) \\[sec:Payroll\\] In Germany, and many other countries, social contributions for old age, health, unemployment and long term care insurance are implemented as payroll taxes. Payroll taxes are paid on labor income. Whether they are implemented as withholding taxes (the employer withholds and transfers the tax) or paid out of the employees funds is of no significance here: it does not matter for (page ). 4.3.3.1.1 Social Insurance Contributions \\[sec:SIC\\] are capped payroll, actually. Despite this terminological confusion and their administration in semi-independent public agencies, it is important to note that social contributions are government revenue, irrespective of their quasi-fiscal status. Social contributions are proper pages To the extent that government adopts universal provision of health care, unemployment insurance and long term care as allocative goals, compulsory contributions to these services are . To the extent that government finances old-age pensions out of current incomes, such PAYGO contributions are intertemporally and intergenerationally redistributive taxes.79 To the extent that private markets for health care, unemployment and long term care insurance are plagued by lemons markets (Akerlof 1970), respective contributions serve to (page ) and are proper revenue-generating taxes. Additionally, universal health care provides provision against epidemics (think vaccinations) and unemployment insurance serves as an automatic stabilizer to smooth out economic growth. Social insurance contributions are presented here for the sake of completion and clarification. They are, when typically implemented as payroll taxes, a variation of a 2-pit. Rates for social insurance contributions are usually proportional. In the EU, they vary from 20% in Denmark to 32% in Germany and 50% in France (Statistisches Bundesamt Deutschland 2007). In Germany, contributions are capped above a certain income threshold, above which additional income is not taxed. 4.4 Taxes on Consumption 4.4.1 Prepaid Consumption Taxes 4.4.1.1 Value Added Tax (VAT) \\[sec:VAT\\] The vat is best understood as an elegant advancement of a retail sales tax. A retail sales tax charges sellers a fixed percentage of the sales price on goods for which the (corporate or private) buyer is the end user. Because under a sales tax, the tax on any single taxable transaction is very high, incentives to evade the tax and to smuggle goods are great. What is more, the state looses all tax revenue on a given good when evasion occurs. The VAT was developed to address this problem. Instead of charging sellers only at the end of a production chain (before the good is consumed), sellers are charged a percentage of their sales prices on all goods, including investment and intermediary goods. Sellers invoice the tax to buyers. Buyers are then eligible for refunds on their VAT receipts: they can deduct all of their sales. Instead of kicking in only at the last transaction before consumption, the VAT charges every transaction according to the value added at the respective stage of production. It is handed down the economic cycle to whoever ultimately consumes a good. The tax burden on any single transaction is much smaller: value added instead of entire sales price are taxed. Incentives to evade the tax on any single transaction are thereby greatly reduced.80 VATs are very common in the EU, standard rates differ from 15% (Luxembourg, Cyprus) to 25% (Denmark), with the majority of countries above 20%. Reduced rates are frequently implemented for foodstuffs and other basic needs. 4.4.1.1.1 Graduated VAT 4.4.2 Postpaid Consumption Taxes 4.4.2.1 Progressive Consumption Tax (PCT) \\[sec:PCT\\] The pct is applied to the total consumption of natural persons. Crucially, it is postpaid and cash-flow based. Postpaid. It is crucial to understand that the PCT is not a VAT or sales tax of any kind with progressive rates (say, a higher rate for luxury goods). The PCT is not prepaid on the price of individual consumer goods, but postpaid on the sum of individual consumer accounts. It matters not what you consume, but how much and of what value. The PCT suggested here is called postpaid because the progressive rate is known and deducted only once all consumption in a given period is completed. Cash-Flow-Based. It would be impractical to collect and add up all consumption receipts. People usually consume very many, different things of relatively small value. Instead, the PCT relies on the Haig (1921)(1938) definition of income in ((1921), (1938) respectively). \\[\\label{eq:HaigSimons} \\text{Income}=\\text{Consumption}+\\Delta\\text{Wealth}\\] A trivial transformation yields a practical definition of consumption: \\[\\label{eq:HaigSimonsPCT} \\text{Consumption}=\\text{Income}-(\\text{Savings}-\\text{Dissavings})\\] Consumption is simply defined as that part of income which is not saved. You can, after all, only either save or spend. People create much fewer savings receipts than consumption receipts: while you may have hundreds of grocery receipts, you will have only one savings account and maybe a few other financial instruments. In addition, transactions to save money are more thoroughly formalized: the bank, by necessity, keeps good track of your accounts. Consumption out of wealth is included as a dissaving. Consumption out of loans is also treated as a dissaving. Payments on interest and principal are treated as accretions to wealth (saving). They are fully deductible. To measure the wealth of individuals, the taxman would have to list the assets and liabilities of each person. Such balance sheet accounting would be very costly administratively and require undesirable evaluation of (page ). The PCT achieves its objective through a simple cash flow accounting of individuals. Instead of evaluating all assets and liabilities, the taxman merely records all cash transactions. A dissaving is recorded when, for instance, a mutual fund is sold and its cash value wired to the checking account. Savings are recorded when payments are made from that checking account to a qualified investment (say, a life insurance). Incomes from labor or capital are recorded as the salary or interest are deposited on the checking account. Consumption is recorded as cash is withdrawn or wired to a seller. The cash flow variant of the PCT rests on the assumption that all assets are transformed into cash before they are consumed.81 Cash flow accounting is much simpler than balance sheet accounting. In a modern economy, most cash flows are already recorded by individuals and their financial service institutions. The taxman just needs to look into these records. Moreover, cash flows are by definition liquid: they constitute the equilibrium prices. The taxman does not need to evaluate anything. 4.4.2.1.1 Illustration I enlist Aesop (564AD)’s fabled Ant and Grasshopper (c.a. 564 b.c.) to explain an example.82 summarizes incomes, savings and consumption for three archetypical taxpayers. 4.4.2.1.2 Grasshopper earns 30,000 each year, of which he spends everything. He had no savings at the beginning of the year, and none at the end, when winter, or rather, the taxman comes. With no savings, all of his income is taxable at a progressive rate, of say, 10%. Grasshopper owes 3,000 to the state. 4.4.2.1.3 Thrifty Ant, who also earns 30,000 per year, saves 10,000. Her taxable consumption is 20,000 at, for example, 5%, owing 1,000 to the taxman. Ant will at some point in the future consume what she has saved, but possibly spread more evenly (thereby avoiding sharply progressive rates) and while earning an interest on her savings. This interest is not income taxed, as mandated by the (page ). 4.4.2.1.4 Big Spender Lion. Let’s introduce a third beast, the Lion, who powerful as he is, earns a great income of 600,000. He also consumes a lot (he is a predator, after all), say 300,000. Come winter the (brave) taxman will collect a 33% due on consumption, that is 100,000. Note, that although Lion consumes relatively less (50%) than Grashopper (100%), he has to pay relatively more taxes (33% as compared to 10%), because, in absolute terms, he is encouraged to spend less and save more, given his superior income-generating abilities. Also note that the tax liability does not depend on the source of income. It does not matter whether the Lion earns much because he is such a diligent hunter (labor income), or (as it happens to be), whether it is because he sits comfortably on the top of the food chain and actually sleeps most of the day (capital income). Progressive Consumption Taxation of Thrifty Ant, Big Spender Grasshopper and Big Earner/Spender Lion. Ant Grasshopper Lion Income 30.000 30.000 600.000 Saving 10.000 0 300.000 Consumption 20.000 30.000 300.000 Tax Rate 5% 10% 33% Tax 1.000 3.000 100.000 4.5 Taxes on Wealth 4.5.1 Taxes on Unimproved Nature 4.5.1.1 Land Value Tax (LVT) 4.5.1.1.1 Pigouvian Element. 4.5.1.1.2 Other Natural Resources 4.5.2 Taxes on Property 4.5.2.1 Property Tax \\[sec:PT\\] 4.5.2.2 Stamp Duty \\[sec:SD\\] 4.5.3 Taxes on Net Worth 4.5.3.1 Wealth Tax (WT) \\[sec:WT\\] The Wealth Tax is a tax on the net wealth (\\(assets\\) \\(-\\) \\(liabilities\\)) of natural persons. It requires a balance sheet account for all tax liable individuals, including some method of valuation for . Wealth Taxes are typically progressive in schedule. They are levied only in a few OECD countries, including France (up to 1.8% p.a.) and Switzerland (up to 1.5% p.a.). 4.6 Non-Taxes 4.6.0.1 Financial Transaction Tax (FTT) \\[sec:FTT\\] 4.6.0.2 ecotax \\[sec:Ecotax\\] 4.6.0.3 Negative Income Tax (NIT) \\[sec:NIT\\] The Negative Income Tax (NIT) is a subsidy for low wage earners, not a tax. It does not raise, but spends revenue. It is included here because of its broad redistributive impact and its technical implementation as a “tax”. The NIT was suggested by Milton (Friedman 1962) as an elegant replacement for minimum wage legislation and (page ) social security transfers. Under the NIT, workers would receive progressive transfer payments for hourly market wages below the socially acceptable minimum. However, in contrast to current income supplements, real market wage increases are not entirely eaten up by transfer cuts: at any given level of income, earning more on the market will leave you with more net in the bank. For example, under a minimum acceptable hourly income of €7.50 (current minimum wage proposal in Germany), moving from a €2.00 market price to a €4.00 market price job could increase your post-tax income from €7.50 (transfer €5.50) to €8.50 (transfer €4.50), always maintaining your incentives to earn more on the market. This may also help to counteract rent-seeking exploitation by low-paying firms. 4.7 The Scores scores different taxes on the desiderata suggested herein. Some desiderata are summarized in one entry, when they are very similar or mere specifications. 4.7.0.1 A Methodological Disclaimer. This table is not a methodologically sound index of desirability,83 let alone a comprehensive cost-benefit analysis (CBA) of tax choice.84 p4cmccccccccc &amp;vat &amp;sic &amp;pit &amp;Dual PIT &amp;cit &amp;Business Tax &amp;pct &amp;Wealth Tax &amp;nit &amp;vat &amp;sic &amp;pit &amp;Dual PIT &amp;cit &amp;Business Tax &amp;pct &amp;Wealth Tax &amp;nit p4cmccccccccc &amp;vat &amp;sic &amp;pit &amp;Dual PIT &amp;cit &amp;Business Tax &amp;pct &amp;Wealth Tax &amp;nit &amp;vat &amp;sic &amp;pit &amp;Dual PIT &amp;cit &amp;Business Tax &amp;pct &amp;Wealth Tax &amp;nit The Forerunner: Indirect, Proportional Taxes: VAT &amp; Payroll {#sec:ScoreVAT} Indirect, proportional taxes are the forerunners in tax choice today. For the last two to three decades, they have assumed an ever greater share of GDP, compared to other taxes (for Germany and the UK, see (Kemmerling 2009, 11). VAT and Payroll, for all their differences, share key characteristics: they are indirect, they are proportional and they fall on relatively immobile bases. 4.7.0.2 Neutrality. The key merit of indirect, proportional taxation is its neutrality to relative prices in the economy: deadweight losses are indeed , as per desideratum \\[des:minimal-DWL\\]. 4.7.0.2.1 Everyone Has to Consume. For the VAT, the neutrality case is clear-cut. Ultimately, people can only spend the savings and (capital, labor) income they have on goods on the market. If buying of all goods is proportionally taxed, people have no way to avoid the tax. They cannot react to a price signal that is universally increased: their (page ) is minimal. The same holds for sellers: they have to sell to someone. When every potential buyer has to bear the same tax, sellers have no way to react. Their (p. ) is minimal, too. The VAT falls on a as per desideratum \\[des:tax-inelastic\\]. 4.7.0.2.2 Everyone Has to Work … A proportional payroll tax is neutral when one accepts an exclusive, (page ). When capital rents are only compensation for risky, deferred consumption all savings and capital rents are ultimately labor incomes. Under the , someone living off her savings is someone living off her past work. In short, to consume and to live at all, everybody has to work, for someone, sometime. If you have to work for someone, and every potential employment is subject to the same, proportional tax, you cannot react to the tax. Again, the of labor supply (page ) is minimal. The same holds for employers: they have to employ someone. If every potential employer is subjected to the same, proportional tax, their of labor demand (page ) is minimal, too.85 4.7.0.2.3 …Except the Rich. Under a (page ), a proportional tax is not neutral. When capital rents are a genuine increment to wealth, there is an alternative to work: you can either work, or live off your interest. In formal terms, rich people can substitute their labor income with capital income. As the availability of substitutes increases price elasticity, the labor supply by rich people is no longer price inelastic. Absent the payroll tax, they might have elected to work more than they will if their labor income is taxed. 4.7.0.2.4 Ideally, A Ordoliberally Hygienic Tax. A neutral tax, by definition, has no fee or pigouvian component. A VAT or payroll tax with a proportional, uniform rate is as per desideratum \\[des:ordoliberal-hygiene\\]. 4.7.0.3 Redistribution \\(=\\) General Revenue? To the extent that proportional taxation is accepted as an equity norm (!), a VAT or payroll that goes into general government revenue is a proper instrument both to , as a desirable tax should as per desideratum \\[des:redistribution-and-revenue-are-one\\]. In Germany and many other countries, capped payroll taxes are used to raise social insurance contributions. Here, redistribution and revenue generation undesirably interact. First, social insurance contributions are proper taxes: they serve to pool risks and they redistribute between old and young, healthy and sick. Second, these services are actually provided universally and unconditionally: as a last resort, the (German) welfare state pays even for those who have not contributed. The welfare state polity, has in other words, already adopted an allocative norm. Raising revenue for these very services on a regressive schedule (capped) on only some incomes (employed labor) embodies another, very specific allocation, for which no normative justification is readily available. In Germany, only employed people pay for the health insurance of all other employed people. Exactly why, at minimum, self-employed people do not also contribute to this risk pool is unexplained, but allocatively consequential. 4.7.0.4 Fairness. At best, a VAT or payroll tax can be regarded as a proportional taxation on as per desideratum \\[des:personal-taxation\\].86 It cannot accommodate progression, let alone an arbitrarily steep schedule. 4.7.0.4.1 Payroll: Regressive. An (ideal) payroll tax is proportional only under an (page ). When capital rents are regarded as proper income (, page ), a payroll tax falls only on one of two types of incomes. A payroll tax is clearly not , as mandated by desideratum \\[des:structural-agnosticism\\]. 4.7.0.4.2 VAT: Proportional. No matter the savings norm, a VAT is straightforwardly proportional, because all income is ultimately consumed and tax liable to generate utility. 4.7.0.4.3 Different Rates Are Not An Option. One common alteration to make the VAT fairer is to adopt different rates for different types of products. Goods that cover basic needs (food) often enjoy a lower VAT rate. In a complex economy and modern society, it is impossible to easily determine which goods satisfy basic needs and which do not. Is a meat a basic need? Is a filet mignon? Is a filet mignon every day? When the state does establish different VAT rates, it risks becoming a micro-manager of fairness and will create arbitrary incentives for cross-substitution. 4.7.0.5 The Problems of Proportional Taxation. Aside from not being progressive, a proportional tax raises a number of problems (page ). 4.7.0.5.1 Diminishing Utility and Positional Consumption. As proportional taxes, VAT and payroll cannot consider of wealth, income or consumption. Whether you are struggling to pay for the first class trip of your child or whether you are thinking about signing up your child for the third summer school, the taxman will treat these two transactions equally, irrespective of the clearly differential impact on the subjective well-being — not to mention opportunity — of parents and child in these two families. Similarly, a proportional VAT on consumption can also not curb (). Whether you are buying a high-priced Mercedes sedan because you enjoy a luxury car during your frequent trips on the German autobahn, or whether you are buying it merely because you do not want the smaller car than your neighbor, the taxman will treat these two transactions equally, with no regard to a potentially wasteful, positional component of consumption. 4.7.0.5.2 Proportional Taxation Raises the Effective Price Floor of Labor. \\[sec:PropTaxDWL\\] Most problematically, any proportional taxation will make it relatively more expensive for low-income earners to make ends meet. In welfare states with legislated or effective minimal standards of living, such an increase in the real costs of living will push more people into structural unemployment. When every trip to the grocery store gets more expensive, more low-skilled people will be unable to afford a socially acceptable minimal standard of living from their equilibrium incomes. Conversely, the (very price-elastic) demand for such low-qualification service jobs drops dramatically, as struggling workers try to extract higher wages (Scharpf in foreword to Ganghof (2004, 12ff)). An equivalent dynamic holds for proportional payroll taxes. Proportional taxation raises the in violation of desideratum \\[des:low-price-floor\\]. Present some income-replacing transfer scheme, as is nearly universally the case in developed welfare states, supply for low-productivity work becomes very elastic: you can either work for less net purchasing power, or receive same or a little more from the state for not working at all. Proportional VAT or payroll taxation then creates a substantial of structural unemployment, disallowed in desideratum \\[des:minimal-DWL\\].87 4.7.0.5.3 Limited Stabilization. Proportional taxation, by definition cannot serve as an automatic stabilizer. When the schedule is linear, it does not matter how lumpy your consumption is. A VAT or ideal payroll will do nothing to prevent you from buying nothing during a bust, and too much during a boom. A proportional VAT could, theoretically serve as an discretionary stabilizer if government were to lower the VAT rate during the downturn. People would have an incentive to buy at lower taxes during the downturn, rather than later at higher rates. Unemployment insurance works as an automatic stabilizer because of how the revenue is used, and not how it is raised. Consumption is smoothed out as the incomes of laid-off workers are insured in the medium term. Two problems remain: First, laid-off workers can still consume less than they optimally should, maybe because they are anticipating a prolonged, uninsured downturn. Second, unemployment insurance would be a much better automatic stabilizer if it were raised in a manner that also smoothes consumption. 4.7.0.6 A Mixed Bag For Capitalism. On the pro side, a VAT and proportional payroll tax let entrepreneurs make their own production decisions, and on their own dime and incentive. A VAT and proportional payroll tax also do not require , as disallowed by desideratum \\[des:liquid-assets\\]. On the other hand, a VAT and proportional payroll tax does little to avoid (page ff.). If capital is concentrated in the hands of only a few people, the rest will not be able to close intelligently incentivized deals on their unobservable effort and uncertain innovations. 4.7.0.7 A Tiny Little Bit Difference Principle. For all its equity and efficiency shortcomings, a proportional VAT shares the fundamental merit of all consumption taxes: it taxes a normatively justifiable basis. Taxing consumption, not incomes, may be desirable under the the , as per desideratum \\[des:difference-principle\\]. This is presented in greater detail and strength in \\[sec:foundational-beauty\\] on the Progressive Consumption Tax. Absent progression, the difference principle merits of the VAT are negligible. The merit of consumption taxation under VAT also manifests itself in the (somewhat88 positive savings rate it allows for.. If government would raise its revenue exclusively through a VAT, people’s incentive to save would be unaltered: as argued in the above, a VAT is agnostic towards capital or labor incomes. 4.8 The Old Hand — Direct, Progressive Taxes: PIT, CIT &amp; Dual PIT The pit is the archetypical modern tax designed to redistribute. It was conceived in the early to mid 19th century to address the inequities of early capitalism. 4.8.0.1 Income Taxation Punishes Saving. A pit can cause two different kinds of in desideratum \\[des:minimal-DWL\\]. First, if insufficiently progressive (= near proportional), it causes the discussed for VAT and payroll taxes (page ). Second, if it is sufficiently progressive on both labor and capital incomes, it taxes savings twice: once when they are earned, and a second time when they yield an interest (and yet again when they yield a compound interest). This has a number of negative implications. When people can avoid double taxation by consuming income right away, they may very well do so. Capital incomes are taxed, even though saving is elastic. This conflicts with desideratum \\[des:tax-inelastic\\] (). Things become even more awry when we consider investments (savings) of different risk. When the (uncertain, but higher) rewards of risky investment are taxed progressively, people will have a smaller incentive to run the risk in the first place. The PIT still allows for local, decentralized production decisions but incentives are substantially dampened. A PIT discourages saving, and it falls twice on capital income, and only once on labor income. 4.8.0.2 The Merits of Progression. It also serves to of labor as per desideratum \\[des:low-price-floor\\]. The problematic double taxation of capital incomes also serves to substantially dampen and ensures some degree of broad ownership. 4.8.0.3 The Limits of Progression Under the PIT. A tax on income, by definition, does little to curb . More fundamentally, a PIT does not allow for truly arbitrarily steep schedules. For a high average tax rate on incomes say, 50%, marginal tax rates would be prohibitively high. Such high marginal tax rates would all but destroy at that margin at all. It seems likely that a PIT will be no match to the (page ), given its limited steepness in progression. 4.8.0.4 No Stabilization. As any tax on income, the PIT cannot serve as an effective automatic or discretionary stabilizer. 4.8.0.5 The Systemic Flaws of the PIT. Aside from its limited and problematic merits, the PIT carries with it some unavoidable, inherent flaws. 4.8.0.5.1 The PIT’s Ugly or Unfair trade-off: Accrual vs. Realization. Because the PIT is based on flows of economic welfare, it requires a method to measure these flows. Whenever assets are illiquid the taxman can either wait indefinitely for realization, or tax on some assumed accrual. Either way, substantial welfare losses ensue and desideratum \\[des:liquid-assets\\] on the is violated. Income taxation on accrual or realization also has substantial equity implications. In contrast to the VAT, it is not merely a matter of timing when we pay the income tax. “Such a statement reveals a basic failure to understand — or a desire that others not understand — the nature of income taxation and tax avoidance: \\[...\\] if income tax can be postponed \\[...\\], interest (or other capital income) can be earned on a tax deferred basis in the meantime.” — McLure (1988 Seidman (1997, 125)) In short: taxation on realization ignores compound interest. 4.8.0.5.2 Tax Evasion 101: Buy, Borrow, Die. \\[sec:TaxEvasion101\\] A sinister exploitation of the evaluation problem of the PIT goes by the name of buy, borrow, die.89 It emerges from the following three doctrines, all of which make sense in isolation. First, a PIT never taxes debt as income. It makes little sense to tax debt: according to Haig (1921)(1938) (equation eq:HaigSimons) and common intuition, the income-generating ability of a person does not change just because she takes out a loan. Consequently, people pay their taxes on the income they use to repay principal and interest. Second, for the practical reason of avoiding evaluations by a central planner, the PIT is levied on realization, not accrual. Third, for Haig (1921)(1938) and common intuition, heirs are (estate) taxed only net of assets and liabilities handed down to them. So far so good? These three trivially intuitive doctrines, in combination constitute the fatal achilles heel of effective capital income taxation. Tax evaders can follow a simple, and entirely legal strategy. First, they buy appreciating, but illiquid assets. In a second step, they borrow money on the appreciating asset as collateral. Recall that this cash inflow is not taxed. Also recall that lenders may very well consider the yet-illiquid appreciation of the asset in extending their loan terms. Tax evaders can now spend out of the appreciation of the asset without paying any capital income tax on this de-facto accrual of welfare. Third, tax evaders die and bequest their assets to someone else. That someone else is liable only for estate tax on the net value of the assets and liabilities. The consumption of the deceased tax evader is deducted from the net value. Again, ultimately, and irrevocably, consumed capital incomes escape taxation. This loophole, like the underlying problem of , is not a practical aberration that could conceivably be closed. Any income tax on realization will offer this evasive strategy. This is not a hypothetical or esoteric problem. The pre-2007 housing bubble in the U.S., Spain and elsewhere was — amongst other factors — also fueled by this strategy. Houses were good illiquid assets (buy). Banks willingly offered mortgages on the houses (borrow). They also happily refinanced the loan, if the suggested (but unrealized!) fair market value of the house had increased. Consumers took out a bigger mortgage on the (supposedly) appreciated house and spent the difference on a new SUV and a flat screen TV. All tax free. Appreciation in illiquid assets is no small component of our economic growth. A colossal share of the real welfare that accrues to capital income earners escapes income taxation without further notice, and any readily available fix. 4.8.0.5.3 The PIT’s Ugly Backstops: The Estate and Gift Tax. It follows from the strategy of tax evasion 101 that an estate and gift tax are necessary backstops to the PIT. Without a tax on the substance bequested, unrealized appreciations on illiquid assets would go entirely untaxed. With an estate and gift tax, “only” the consumed part of unrealized appreciations escapes taxation. The estate and gift taxes are, in themselves ugly beasts. Just like the PIT, they require an evaluation of a flow of economic welfare in the potential absence of a market equilibrium price of that very flow. If an entrepreneur bequests her family-owned firm to her daughter, no one can know how much it is worth. Again, problems in the taxation of illiquid assets ensue. 4.8.0.5.4 The PIT’s Ugly Backstops: The CIT. \\[sec:ScoreCIT\\] A PIT has another, even uglier backstop: the Corporate Income Tax. Recall that a PIT is meant to tax all incomes, including interest and compound interest. With only a PIT, tax evaders would have at their disposal a strategy even simpler than tax evasion 101: they could place (or retain) their savings in an incorporated firm. First, tax evaders could spread out their (spiky) income over a longer period of time to play the progressive schedule. A small, but steady trickle of income is better than a large, lumpy flow under a progressive tax. Second, and more fundamentally problematic, tax evaders can also retain their initial capital income in the corporation and have it earn a compound interest. Note that the compound interest is earned on a capital income which, ideally, should have been taxed already. To avoid this loophole, a PIT needs a CIT to get at the corporate shelter from the income tax (Genschel 2005, 7). Also empirically, PIT and CIT rates go hand in hand, between countries and over time, further supporting the intricate, if dysfunctional link between the two (Piatkowski and Jarmuzek 2008, 14). The CIT is as bad as the PIT, minus the merits of progressive, personal taxation of the PIT. First, the CIT has no as demanded in desideratum \\[des:determined-incidence\\] (McCaffery 2005, 918). Corporations are fictions. If you tax them anyway, you violate desideratum \\[des:personal-taxation\\] (). Second and related, the CIT knows no progression. To the extent the the CIT ever reliably fell on capital, it made some sense historically when corporate ownership was a reasonable proxy for the propertied classes. This is no longer the case. Many people receive capital incomes (Grabka et al. 2007 XV). The CIT has no way of distinguishing between different owners (McCaffery 2005, 918). Whether a particular Volkswagen share belongs to the life insurance of a single working mother or a stock market tycoon, both are taxed at some uniform corporate income rate . Third, much like the PIT, the CIT itself is plagued by unavoidable loopholes. The CIT is very sensitive to international tax rate competition: if CIT rates differ from country to country, multinational companies can easily shift profits to the country with the lowest rate. A number of strategies are available to do this, the simplest of which is to charge inflated (deflated) prices in intra-company trade (Ganghof 2004, 43ff). This loophole cannot be closed as inflation (deflation) of intra-company trade prices cannot be reliably detected. More fundamentally, “any national claim to a particular share of \\[a multinational&#39;s\\] profits is hard to justify on the basis of principle” (Genschel 2005, 61). 4.8.0.6 The Downgraded Upgrade: The Dual PIT. \\[sec:ScoreDualPIT\\] To the extent that a Dual PIT has substantially lower, and proportional rates on capital incomes, it alleviates the PIT’s disincentives to save and invest. The Dual PIT is simply another non- relative burdening of capital and labor incomes in violation of desideratum \\[des:structural-agnosticism\\]. 4.8.0.6.1 A Hollowed-Out PIT. The relatively smaller burden on capital is hard to justify on principle. The largest incomes are, by definition, capital incomes.90 Charging capital incomes a lower, proportional rate will greatly depress the overall progressivity of the PIT Progression is limited under the PIT already, but will be even more compressed under a dual rate structure. To the extent that more of the tax burden falls on labor, a Dual PIT will also raise the , as disallowed in desideratum \\[des:low-price-floor\\]. The Dual PIT is a PIT only in name. It hollows out the ideal of universal progression. The lower the rate on capital gets, the more a Dual PIT approximates a (progressive) payroll tax. 4.9 The Streaker — Local Business Tax (LBT) A tax on the earnings of local businesses shares all of the disadvantages of the CIT. More fundamentally, the LBT is not a proper tax. It should not need to be discussed here at all. Local authorities provide different kinds of (see ). The , and it provides should be financed out of general revenue. A general revenue tax should fall only on as per desideratum \\[des:personal-taxation\\]. Local authorities also provide such as parks and such as sewers.91 Common good and natural monopolies should be financed out of Pigouvian levies (at marginal cost) and fees (at average cost), respectively. The LBT sits in the nomansland somewhere between general revenue taxation and Pigouvian levy or fee. It violates the norm of as per desideratum \\[des:ordoliberal-hygiene\\]. To the extent that the LBT is used to finances risk pools, public goods and redistributes, it should not be allowed to differ between municipalities. If universal education is an allocative goal of the broader, national polity, everyone should pay for it. If fire protection is a public good, everyone should pay for it. LBT revenue used to finance these things should be replaced by transfers from higher levels of government. To the extent that the LBT is used to finance common goods or natural monopolies, it must specifically fall (at marginal or average cost, respectively) on the very consumption of these goods, and not on the earnings of a firm in general. If industry pollutes a local river, every unit of discharged pollution should be (Pigouvian) taxed at marginal cost, not every firm in the city based on their earnings. If some firms or citizens in a rural municipality require high-speed internet access (a natural monopoly), each user of the network should pay at average costs, not every firm in the vicinity. Also note that LVT can replace this purpose to some extent. In short, the LBT is an anti-ordoliberal streaker that should never have entered this game of perfect taxation. It should be marshaled off the playing field as soon as possible. 4.10 Impotent Redistribution “Advocates of redistributive taxes must wake up and realize that their end is in jeopardy on account of their poor choice of means: they are fighting, and losing, the wrong war.” — Edward J. McCaffery (2005, 848) 4.10.1 Dangerous Backstops 4.10.2 One Tax, and You’Re Done: As Close to a Beer Coaster Tax as it Gets The PCT emerges as the by far superior tax of all of the redistributive taxes scored in the above, both real and hypothetical. It can replace all other redistributive taxes, including the (ugly) backstops to income taxation, the corporate, estate and gift taxes. Administering the PCT together with any of the existing taxes in the above will at best create unnecessary complications, and at worst, unintended double taxation. Organizing redistribution through only one tax has tremendous advantages. It will reduce costs in the administration and cut red tape for economic activity. It will also greatly enhance the transparency of taxation. Once the PCT is thoroughly explained, well-designed and implemented, it can enlighten the politics of tax. Only if we all understand and know how we are taxed and on what, can we begin to have a deliberative debate on how much we ought to redistribute. 4.10.2.1 One Tax Only, Plus Two. The PCT may need a supplemental wealth tax and NIT. Both of these taxes are very imperfect taxes. Metaphorically, they are like powerful antibiotics: very effective as a last resort, but at the cost of severe side effects. They should be prescribed with care, and only if absolutely needed. 4.10.3 PCT 4.10.3.1 It’s Win-Win-Win-Win: No DWL Or No DWL &amp; Less Waste Or Diminishing Marginal Utility. \\[sec:WinWin\\] The DWL case for the PCT is not clear, but either way, it’s good. Just in which way the PCT is good depends on the in excessive and positional consumption. Without further empirical research, they cannot be known. The PCT will have very high marginal rates, possibly well above 100 %. Because universal, stark price hikes in the markets for excessive consumption have never occurred, we have no systematic observations regarding the responsiveness of excessive buyers to such price changes. So what are the alternative outcomes? 4.10.3.1.1 What If: People Still Consume Excessively. If people do not respond to the post-tax price hike in excessive consumption, the PCT will tax an . The DWL will be , as per desiderata \\[des:tax-inelastic\\] and \\[des:minimal-DWL\\]. Most of what consumers no longer enjoy in excessive consumption will be recouped as government revenue, as illustrated in . Producers will be perfectly elastic in their supply. At least in the long run,92 all producers can shift their production to less expensive consumer goods, or, as described later, to investment goods. The burden of the PCT will fall exclusively the who ultimately consume goods, as mandated in desideratum \\[des:personal-taxation\\]. 4.10.3.1.2 What If: People Consume Less (Expensive) Things. If people do respond to the post-tax price hike in excessive consumption, the PCT gains a Pigouvian component, but one that works. When people consume excessively, they pay for the negative externality of defection in the (page ). Recall that when the Joneses buy a BMW (defect) instead of a VW (cooperation), the utility enjoyed by the VW-owning Does decreases: they cannot stand having the cheaper car. Under the PCT, the Joneses pay (dearly) for showing off their BMW. If their demand for BMWs is price elastic, they may buy the same VW as the Does. Also recall that the level of cooperation in a multiplayer Prisoner’s Dilemma is a (page ). Contributing to cooperative behavior creates a benefit to other players from which they cannot be excluded. Establishing cooperation is, however, rival: if no one, or too few people cooperate, the benefits of cooperation will not materialize. Buy buying a VW, the Does contribute to a common-good-like lower level of automobile consumption, from which everyone else can invariably (relatively) benefit. The Joneses can exploit the thriftiness of the Does by buying a BMW (greater relative utility), or they can partake in the Does thriftiness by buying a cheaper VW (same relative utility, but lower absolute utility), without being looked-down upon by a BMW-owning neighbor. Either way, the Does cannot prevent the Joneses from benefiting (non-exclusion). The Joneses buying of a VW is, however rival. If they end up being the only family on the street that buys the cheaper VW, their triftinness will be inconsequential for everyone else. Common Goods are best financed out of Pigouvian taxes. The PCT is that Pigouvian tax of excessive, positional consumption. It falls on exactly that behavior (excessive consumption) at exactly that price (marginal rate!) which it is meant to discourage. If rates are sufficiently high and progressive, the PCT can effectively curb . At minimum, it can lower the level of material waste at which positional consumption occurs. 4.10.3.1.3 What If: People Don’t Mind Buying Less (Expensive) Things. A Pigouvian tax, by definition, has a DWL. The DWL of a Pigouvian tax is the very consumer and producer surplus that should never have been realized, because of the social costs hidden to them. The definition of a DWL assumes that the utility of consumers is accurately measured in their willingness to pay. Utility and nominal demand curve are the same. (page ) suggest that people derive utility not from absolute, but relative consumption or status. People want to have more, or at least not less things and status than other people. If excessive consumption occurs for positional gain, there can be no DWL when that consumption is curtailed for everyone. No one will be relatively worse off, only less relatively different. If anything, a more compressed stratification in the material signs of affluence and status will make most of us happier. 4.10.3.1.4 What If: People Really Care About Luxury. People enjoy having many, and many expensive things not only because they want more than their neighbor. Driving that BMW on the autobahn may be aesthetically pleasing and functionally faster in absolute terms.93 Let’s assume for a minute that there really is an absolute, sheer driving pleasure94 in driving an expensive car. Even if there is absolute utility, it is likely to be of marginally decreasing return. Upgrading from a VW to a BMW will probably bestow more utility on someone than upgrading from a BMW to a Bentley. By taxing big spenders more, the PCT accounts for in excessive consumption. Even in a world of sheer driving pleasure, when more people get to upgrade from VW to BMW and fewer from BMW to Bentley, aggregate autophile utility will be maximized. 4.10.3.2 Sky-High Progression. The PCT can be arbitrarily progressive. For very excessive consumers, rates can be as high as several hundred percent: literally, the sky is the limit. Because the PCT falls on consumption, and not on incomes, incentives or entrepreneurship will not be dramatically curtailed. The ultimate reason for homo economicus to work hard and invest wisely is superior consumption. At first sight, depressed consumption (at higher prices) may appear to reduce incentives. Consider the trade-offs for homo economicus under the PCT: She can either work an extra hour, or not. Because she is already very diligent, and likes to work hard / party (consume) hard, any additionally enabled unit of consumption will be taxed at a fairly high rate. At any rate of consumption taxation, she will still be better off if she works the extra hour. At the other end of the progressive schedule, rates can be very low or at zero for people who consume very little. Aside from equity, this also serves to labor as per desideratum \\[des:low-price-floor\\]. If the working poor or structurally unemployed pay a zero tax rate on their necessarily minimal consumption, their living standards increase and their reservation wage vis-a-vis the socially acceptable minimum living standard drops. 4.10.3.3 The PCT Smoothes Consumption. Under progressive tax rates on consumption, people are punished for spikes in their consumption. To have the lowest possible PCT rate for a given level of lifetime consumption, you have to spread out the shopping sprees as evenly as possible. In a bust or boom, the PCT serves as an automatic stabilizer. Without legislative intervention, people are always incentivized to smooth out their consumption. The PCT can also serve as a powerful discretionary stabilizer. In a bust, legislatures can opt to lower the entire PCT schedule95 . For the duration of the bust, people have a greater incentive to consume, knowing that higher rates will return once the economy reverts to its long-term growth path. The PCT as discretionary stabilizer will be general in impact. Whatever you choose to buy will be cheaper to buy. Government can inject discretionary stimulus without opening up to clientelist pressures. The PCT also minimizes windfall effects. Because you have to consume first to take advantage of the lower rate, you cannot channel the government stimulus into further savings. In fact, you have to divert from your average consumption path, and consume more during the bust. Just as a cash-for-clunkers scheme, the PCT incentivizes people to dissave, except for all kinds of consumption and without the waste of clunkers. 4.10.3.4 Deciding How Much to Consume, and Who Consumes. The PCT allows government to set an arbitrary savings rate. Because it is progressive, the PCT can incentivize saving without (see page ). The PCT allows for greater saving than the VAT because it (greatly) incentivizes exactly those people to save more, who can: the excessive consumers. 4.10.3.4.1 How the PCT Makes You Save More. The PCT offers two mechanisms to increase the savings rate. First, the PCT is simply an elegant revenue stream for government from which it can finance public good projects (public works) or buy investments (sovereign wealth fund). Both a new (non-congested) motorway and public ownership of some corporation constitute saving. Second, the PCT is an instrument to alter the save-vs.-consume trade-off for natural persons. It encourages saving in three distinct ways (the following is from (Seidman 1997, 28ff). The incentive effect of higher post-tax interest rates is more complicated than might appear at first sight. Intuitively, tax exempted saving and capital incomes will increase the post-tax interest rates and cause people to save more. A substitution effect may ensue: for every unit of resource saved now, you will receive a greater post-tax payoff in the future. You save more. On the other hand, a higher post-tax interest rate is an accrual to wealth: you just got richer. According to the income effect of higher interest rates, you may save less. For fewer units of resources saved now, you will receive a same payoff in the future. The PCT is less ambiguous in its effects than a change in the market interest rate. It has only a substitution effect. Because people are (income) taxed before the PCT, too, introducing the PCT does not make them any richer. Taxation does not disappear, it merely shifts the base. Think this through again The horizontal redistribution (heterogeneity) effect occurs between equally affluent persons who differ in how much they consume. Consider two millionaires, one of whom spends all his income (affluent consumer), one of whom saves all her income (affluent saver). Under income taxation, both pay a (same!) tax on their millionaire income. Under the PCT, the affluent saver pays no tax at all, and the (price inelastic) affluent consumer pays twice the income tax for revenues to stay constant. The affluent saver gets to save all of her income. Under income taxation, she can save only the post-tax income. In sum, at same public revenues, more money is saved under the PCT than under income taxation. Seidman (1997) estimates US savings rates to increase by roughly 11% due to the heterogeneity effect of a PCT transition. The postponement effect occurs as a one-off effect as the tax burden shifts within cohorts from saving at working ages to dissaving during retirement. An income tax burdens savings immediately, and early in life once they are earned. A PCT burdens savings late in life, when they are consumed. At same levels of retirement consumption, people will be able to accumulate more capital earlier. To the extent that the postponement effect occurs within cohorts, it is a zero-sum intertemporal redistribution between private saving and public revenue. It does not, as Seidman (1997) seems to imply, generate genuine new saving in the long term. It just postpones government revenue. To the extent that the postponement effect occurs between cohorts, it can genuinely improve the savings rate and public revenue by taxing old savings twice (or even three times). The intergenerational equity considerations of this effect are later discussed in . 4.10.3.5 It’s not All Pleasure-Delaying. So far, the PCT appears to encourage thriftiness over all else. As Ant — and even the most die-hard supply-side economist — will attest, that alone would make for a fairly sad state of affairs, with all saving, and ever-delayed consumption. As Ant learns, the heart wants some music, too. 4.10.3.5.1 The Party’s Over. As argued in , there are good reasons to believe that we are growing much slower than we could at greater savings, and that, already, we have taken out loans on the future. No matter what the heart wants, we may have to turn down the music quite a bit. 4.10.3.5.2 Deciding How Much To Consume. Correctly understood, the PCT is actually agnostic towards how much we, as a society, consume or save. What is missing in the description so far — as it is in much of the politics of tax — is what happens with the revenue. Taxation does not reduce resources, but (with a small DWL), merely redirects them. It is then up to the democratic state, or the individuals to which it redistributes, to decide how much should be saved or invested and how much should be devoted to consumption. 4.10.3.5.3 Deciding Who Consumes. Let us (optimistically) assume that the current savings rate is optimal, that aggregate consumption must remain on current levels, and that the public expenditure quota is optimal. Even in this (unlikely) scenario, the government can use the PCT as a pure redistributive tax to redirect resources from big spenders to small spenders, with no effect on the savings rate or public budgets. All government would have to do is to either zero-rate the small spenders (a de-facto redistribution to them), or hand out additional transfers to them. To go back to the car example, the PCT would redirect resources from buyers of BMWs to buyers of budget Dacia Logans. Alternatively, government itself can also spend the revenue on public goods or risk pools. For instance, government could organize a big parade or a fireworks display on the national holiday. The savings rate would be unaffected. The same amount of resources would be spent, just by a different body and for a different purpose. 4.10.3.5.4 S-Classes to Subways. Let us now consider what happens when the PCT is used to increase the savings rate, as described in the above. When implemented with an appropriately , the PCT does not lead to a drop in aggregate demand (page ). It merely redirects economic resources from the production of consumer goods to the production of capital goods. Under axiomatically assumed , factor markets always clear. That means that surplus incomes (savings) will always find profitable investment. This capital deepening of the economy can take many forms. In the public budget, it can mean building a new school, upgrading our infrastructure, funding basic research or abating and adapting to climate change. On the marketplace, it can mean building a new factory for solar panels, developing a new biotech patent or building more equity to guard against the next financial crisis. Whatever the use, in a perfect market, capital deepening is an unambiguously good thing. It means to enrich our world, with powerful factories, liberating technologies and empowering education. When people and institutions are given enough time to adapt to the new incentives, no welfare will be lost and no factors will sit idle. Mercedes Benz just needs some time to convert its production line from S-Class sedans to economic minivans, or even public transportation. 4.10.3.6 The Foundational Beauty of the PCT. \\[sec:foundational-beauty\\] Gleaming behind its ability to make us all enrich our world, there is a greater normative appeal to the PCT: its beautiful alignment with a foundational theory of private property. By taxing consumption, the PCT leaves incentives to work and save unaffected. The PCT is as per desideratum \\[des:structural-agnosticism\\]. Like the VAT, it taxes neither of them, but only the component of all income that is consumed. In contrast to the VAT, it does so without sacrificing progression. “Under the progressive consumption tax”, as industrialist C. William Hazelett put it to the US Senate in 1939, “we define income at what it really is. There is no economic income but the living standard of the taxpayer” (Hazelett as cited in (Bank 2004, 15). Consumption is the very transaction at which we take a part of the riches of our economy, exclude other people from its partaking and use it up. The PCT taxes you for what you take out of the cake, not for what you contributed to the cake (Seidman 1997, 58). This foundational ideal of taxation goes back a long time. Thomas Hobbes (1651), in his “Leviathan”, wrote: “\\[T\\]he equality of imposition, consisteth rather in the equality of that which is consumed than of the riches of the persons that consume the same. For what reason is there that he which laboureth much, and sparing the fruits of his labour, consumeth little, should be more charged than he that living idlely, getteth little, and spendeth all he gets, seeing the one hath no more protection from the commonwealth than the other? But when the impositions are laid upon those things which men consume, every man payeth equally for what he useth, nor is the commonwealth defrauded by the luxurious waste of private men.” — Thomas Hobbes (1651, 386) Adam Smith (1776), in his “Wealth of a Nation”, wrote: “Every tax ought to be levied at the time, or in the manner, in which it is most likely to be convenient for the contributor to pay it. \\[...\\] Taxes upon such consumable goods as are articles of luxury, are all finally paid by the consumer, and generally in a manner that is very convenient for him. He pays them little by little, as he has occasion to buy the goods. As he is at liberty too, either to buy, or not to buy, as he pleases, it must be his own fault if he ever suffers any considerable inconveniency from such taxes.” — Adam Smith (1776, 778) Both Thomas Hobbes (1651), the founding father of the modern state, and Adam Smith (1776), the founding father of the modern economy, hinted at, if not endorsed a (progressive) taxation of consumption.96 4.10.3.6.1 The Difference Principle of Consumption Taxation. They probably did not know it then, but what already Hobbes (1651) and Smith (1776) saw in the progressive taxation of consumption was the ingenious conciliation of equity and efficiency in the modern world which we now call the (???) and which here is listed in desideratum \\[des:difference-principle\\]. The PCT leaves untouched that economic inequality in incomes, in incentives, and in entrepreneurship which, in a perfect market, pareto-grows the cake for everyone, including the least well-off, as the difference principle mandates. The PCT taxes, discourages, disallows that economic inequality, in excessive consumption and positional gain, which slices up the cake in exclusive, inequitable pieces, devours it, leaving only the shameful crumbs for the rest. 4.10.3.6.2 Private Property Isn’t Theft. But Neither Is It God-Given. \\[sec:AntiEntitlement\\] A foundational understanding of private property and taxation is not without its discontents. If you believe, as Robert Nozick (1974) does, “that a person is entitled to those goods acquired in uncoerced exchanges with others” (1974, 149), there is no good reason (if at all) to tax this entitled private property again when it is consumed. A comprehensive (de)appreciation of such hyperliberal entitlement theory is beyond the scope of this thesis, but a preliminary critique I must offer. Naturalizing whatever the allocative results the free market produces as inalienable private property is as ahistorical as it is uncritical. Just a cursory look in the history reveals that private property has not been around for a very long time. For a constructed entitlement, written and signed on a piece of paper to be powerful enough to guarantee inalienable access to whichever economic resource it concerned, one thing was needed above all: security. If you have to use a stick everytime you want to exercise your property rights, they are not much worth to begin with. In an insecure world, sticks and physical power mean everything, pieces of paper mean nothing. This (???) nightmare of violent anarchy, incidentally, was resolved by the state. How did the state and its prototypical racketeers do it? They extracted protection money from the people they protected from (or, equivalently, threatened with) violence and theft (Tilly 1985). As these racketeers enjoyed technological innovation (gunfire) and economies of scale in their protection of violence, they became larger until they morphed into the modern state. From that day on, their protection money was called a tax. That hyperliberalism insists on essentially entitled, private property, but now begrudges the polity and the modern state its revenue is, to say the least, ungrateful. Private property is not the antithesis to a potent polity. They are part and parcel. You cannot have the growth that empowers a polity without private property (for now). You cannot have private property without a polity either violent enough, or equitable enough to maintain these very entitlements. Taxation should treat private property accordingly. Not as theft, but as taxable. 4.10.3.7 The Perfect Tax. “A progressive postpaid consumption tax emerges as the fairest and least arbitrary of all comprehensive tax systems, precisely because it chooses to make its decisions about the appropriate level of progressivity at the right time. In doing so, it burdens some but not all uses of capital and its yield, and for normatively attractive reasons.” Edward J. McCaffery (2005, 812) Let me introduce you: the perfect tax. The duty that is both elegant, and fair. The regime that lets our economies grow, and have everyone partake in its fruits. The excise that reconciles efficiency with equity. The only tax that does not either discourage savings, or is regressive (Seidman 1997, 2). The set of rules that raises the revenue for a potent polity, and, at the same time, curbs wasteful decadence. 4.10.4 The Nuclear Option — Wealth Tax A wealth tax, or more accurately, an expropriation tax shares many of the advantages of the PCT. A wealth tax is superior to a PCT in its ability to disperse the ownership of means of production. When inequality is (page ), taxing consumption at whichever progressive rate may not suffice to dampen capital accumulation in the hands of very few. Three other, more hypothetical considerations speak for a wealth tax. First, if there were a macroeconomic crisis of Marxian underconsumption, a wealth tax would provide a powerful lever to redistribute resources (to other people or the state) and consume them. This is an unlikely scenario: Marx’s gloomy predictions on the crisis of capitalism have not become reality, and, if anything, OECD economies save too little. This is also about a liquidity crunch, and in that respect, it might help, if through a highly discretionary lever, which is not so great. In fact, there is both a Marxist (capital accumulation) AND a Keynesian (liquidity crunch) argument for it. Second, if there were an inefficient misallocation of resources in an overproduction of private goods (cars) and an underproduction of public goods (levees against rising sea levels), concerted expropriation under a wealth tax could provide for public works revenue much faster than a PCT can. Third, if there were a sustained excess indebtedness in public budgets, well-organized and progressive expropriation may be the least inefficient and arbitrary of all evils. Sovereign defaults are prone to cause panics and ripple effects in the world economy, as recently evidenced by the Greek debt crisis (not default) in 2009. Inflating your way out of sovereign debt also has severe adverse effects. Rational actors will freeze up their capital in non-monetary assets (such as real estate, or even gold) and be excessively conservative about the future. Most importantly, both sovereign defaults and inflation burden natural persons arbitrarily: like a corporate tax, defaults and inflation have no well-determined, let alone progressive incidence. A wealth tax, for all its shortcomings, has both. 4.10.4.1 Better Than Income Taxation. In contrast to sharply progressive taxation of capital incomes, a wealth tax still maintains (if not strengthens) as per desideratum \\[des:Incentives\\]. Even if the taxman will expropriate a certain percentage of your net worth at the end of the year, you will still try to make as much profit with what you have. A wealth tax is neutral with regards to the risk aversion of investors. 4.10.4.2 Property is Power. There is an equity argument for a wealth tax, too. It requires that we relax condition itm:MaximizeProfits and allow homo economicus to make irrational investment decisions. If owners of capital exercise discretion in their decisions, rather than fully rational (if sophisticated and privately informed) cost-benefit analyses, great wealth bestows great power. Such investors, in the classic definition of power, can make other people do something they would not otherwise (pareto-optimally) have done (Geoff, Segal-Horn, and Lovitt 2002, 8ff). A wealth tax gets at that accumulation of power. 4.10.4.3 Redistributive Instrument of Last Resort. A wealth tax has immense normative and practical drawbacks. It is antithetical to capitalist entrepreneurship. It disallows market production decisions on those means of productions with are confiscated. Most fundamentally, a wealth tax is plagued by the dilemma of having to evaluate in violation of desideratum \\[des:liquid-assets\\]. As a tax on capital, it has to either evaluate illiquid assets as a central planner, or let owners escape taxation and freeze up liquidity. Since expropriation is already a state intervention in the economy, it probably makes a certain amount of sense to rely on accrual-based taxation through a central planner. 4.10.4.4 A Nuclear Option. The flaws of a wealth tax are by no means trivial. Taxation on realization will allow evasion and wreak havoc on liquidity. Taxation on accrual, as expropriation in general, will put the political process under inordinate pressure by private interest. The (partial) central planner may be contaminated by widespread clientelism, or incompetent to begin with. 4.10.5 The Backup — NIT A negative income tax (NIT) can serve to for low-productivity labor by subsidizing the purchasing-power parity minimum living standard of these small earners, as per desideratum \\[des:low-price-floor\\]. It does so more elegantly and with a smaller (desideratum \\[des:minimal-DWL\\]) than minimum wages or combined wages. At any level of productivity or gross income, workers will still have (desideratum \\[des:Incentives\\]) to earn more. Workers experience less moral hazard not to work, or to work less than they otherwise would. Free-riding firms will have a harder time cannibalizing non-subsidized employment. 4.10.5.1 A Methadone Program for Structural Unemployment. A NIT may be a very effective treatment of structural unemployment in the short term. In the long term, only greater education and productivity alleviate the root cause of structural unemployment. Transferring resources, however elegantly and at whichever level only treats the symptoms, not the disease. As a discretionary stabilizer, a NIT can be a structural painkiller, disguising necessary pressures to adapt and reform. 4.10.5.1.1 Bringing in the Nuclear Option: Tax Wealth Only If Absolutely Needed. The PCT alone may be insufficient to broaden the distribution of capital, given the self-reinforcing and excessive dynamics of inequality today. Taxing only the consumed component may be inadequate to get at some riches (Shaviro 2004, 106). For some people, no matter how progressively their newest sports car is taxed, they will spend only out of their interest, if not out of petty cash. If, and only to the extent that the overall distribution of private property is deemed unacceptable only for the reasons mentioned in the above, a wealth tax may be advisable. It should target exclusively that inequality of the very, very rich which cannot be adequately taxed on their consumption only. 4.10.5.1.2 Bringing in the Backup: NIT Only If Absolutely Needed. The PCT goes some way to lower the price floor of low-productivity labor. Very small spenders would be zero-rated in their consumption. In effect, these people would receive for free all public goods and risk pools they ever benefit from. The productivity of some people may be so low, that even no taxation at all does not make ends meet. If, and only to the extent that people cannot live off their equilibrium labor incomes with zero-rated consumption, government should subsidize such labor through a NIT. 4.10.5.2 Implementing The Perfect Tax A comprehensive suggestion for the implementation of the PCT is beyond the scope of this thesis and my competence. Much legal, administrative and design specification will be required to make the PCT real. I provide here some preliminary thoughts on implementing the PCT and its problems. 4.11 Durables 4.11.0.1 Depreciation, not Expensing. A simple addition to a PCT is that durable consumer goods should not be expensed, but depreciated over time. A desirable tax as per desideratum \\[des:minimal-DWL\\] will leave , including those between more durable (and expensive) and less durable (less expensive) goods. A simple cash flow treatment (expensing) of durable goods would punish people for buying high-quality, presumably expensive durables. If you buy a set of new, high quality household appliances in one year, your consumption spikes, irrespective of how long you will use the devices. Under a progressive schedule, you would be better of buying cheaper appliances more often (smoother consumption), rather than more expensive appliances less often. To avoid such distortion, a PCT should depreciate durables over time. The “consumption” of a durable good would incur to your as they are depreciated every year. Depreciation schedules for durable goods are tested and readily available in most tax legislations. 4.11.0.2 Really Durable Goods: Real Estate. Depreciation of durable goods becomes more thorny for goods that never substantially depreciate, or even appreciate in value, as for example owner-occupied real estate. There are three possibilities to tax owner-occupied housing, all of which have severe drawbacks. First, house owners could be taxed on the imputed rents of the real estate. House owners would be treated to tenants, as mandated by desideratum \\[des:minimal-DWL\\]. Any appreciation or depreciation of the house would go untaxed just like any other financial investment. This would require that the state, in the absence of an equilibrium rent, approximates market rental rates. Second, house owners could be taxed on a depreciation schedule set by the government (taxation on accrual). This would substantially alter incentives to invest in owner-occupied housing. In a house that appreciates in market value, occupying owners would be at a relative disadvantage to non-occupying owners and their tenants. Much like taxation on accrual, depreciation requires the government to act as a central planner, albeit in a much smaller sector of the economy. Under a PCT, only owner-occupied housing, or owner-used durables may require government valuation, not all illiquid assets as under income or wealth taxation. Third, house owners could be taxed on the resale price of the house (taxation on realization). When a house looses market value, occupying-owners will be taxed on the difference in price. Owner-occupied housing that gains in market value would escape taxation. Much like taxation on realization, taxation on resale price will substantially alter the incentives in the economy, in violation of \\[des:minimal-DWL\\]. Markets of depreciating owner-occupied housing may freeze up entirely, in fear of the impending tax burden. Markets of appreciating owner-occupied housing may overheat, with demand inflated by the tax-free consumption of such housing. Occupying owners would be treated starkly differently than non-occupying owners and their tenants. Occupying owners would be subjected to a bifurcated risk distribution. If their house gains in value, they go free entirely: they gain an income and receive tax-free consumption. If their house looses value, they get punished twice: they loose wealth and get taxed on the loss. On cursory inspection, the least arbitrary and disruptive method to tax owner-occupied housing or owner-used durables in general appear to be imputed rents. 4.12 Introducing the PCT The transition to an exclusive, progressive taxation of consumption will require careful planning and specification. 4.12.0.1 Go Slow. \\[sec:GoSlow\\] Most importantly, a transition to the PCT must be credible, but very slow. Moving to the PCT is probably more a process of (one to three) decades, than of years. The principal reason to transition very slowly is that the economy needs time to exit from the production of (positional) consumer goods to the production of investment goods and/or cheaper consumer goods. If incentives change faster than firms can cheaply exit and enter, welfare is lost (Seidman 1997, 20). When Mercedes is forced to abandon the S-Class production line before it is depreciated or before it can be cheaply converted, we are just burning capital. It will probably be wise to phase in the PCT over several years. The PCT should be introduced on a binding timeline grandfathered-in at year one of the transition, to reliably incentivize firms to change their production. Additional problems not discussed here may stem from short- and medium term monetary dynamics. A careful investigation of monetary correlates of taxation and the introduction of a PCT in particular must (page ). 4.12.0.2 Intergenerational Incidence. \\[sec:IntergenerationalIncidence\\] A tax reform as paradigmatic as from income to consumption will affect people differently depending on when in their lifetime income and consumption streams the reform sets in (Graetz 1979, 1649). People with savings, absent other specification, will be taxed three times on these savings: once when they were initially earned, a second time when they earned an interest (), and a third time when they spend it under the newly introduced PCT. People with no savings, absent other specification, will be taxed only once. Savings and income follow a probabilistic lifecycle: people save out of their income in their middle years to spend when they are old and have no income.97 When the PCT is introduced, people who are old at the time of introduction will contribute more to the polity over their lifetime than people who are young at the time of introduction. To a certain extent, intergenerational redistribution of this scope may well be the unavoidable side product of a changing society. Equity considerations may come to an end just here: you are not entitled to recompensation for when in the history of a society you are born. Still, the magnitude of triple taxation of old people may be so immense that a PCT should address the intergenerational incidence caused at the time of introduction. 4.13 Investment vs. Consumption 4.13.0.1 Deductions. The PCT requires a white list for cash uses that qualify as saving. For many financial products, the distinction will be straightforward: whenever means of production are acquired (equity) or financed (debt), whether as mutual funds, bonds, stocks or life insurance, the respective payments will be deducted from income and dissavings (, page ). 4.13.0.1.1 Education. For some cash uses, the distinction may be more complicated. Education is a good example. On the one hand, some component of some education constitutes saving into one’s own human capital, one’s own means of production. On the other hand, education can (and should!) also serve other purposes of self-realization, fulfillment and leisure. A summer school in the fine arts is something that we may want many people to enjoy, but it is not in any strict sense an investment. Ideally, it should be taxed as consumption. These and other lines will be hard to draw in practice. Given the broadly beneficial and various spillover effects of education (maybe the fine arts do inspire you, after all), it will probably be wiser to err on the side of too much deductable education, rather than too little (Seidman 1997, 8). 4.13.0.2 Self-Employment. The line between consumption and investment gets even more blurry, when people own their own means of production other than their human capital. First, the distinction between self-employed people and employees should be resurrected. When people do not bear the full financial risk of their economic activity, their expenses other than education should not be deductible. Both the commute to work (Pendlerpauschale, commuter tax allowance) and the costs of securing a job (Werbungskosten, income-related expenses), in a perfect market, should be reimbursed in equilibrium incomes, not through tax breaks. Second, when people are their own risk-takers, all their expenses should be deductible. Unincorporated, self-employed people must have a tax burden fully equivalent to that of incorporated firms. Under a PCT regime, both should not pay any redistributive or general revenue taxes. The private consumption unrelated to the business activity of self-employed people should be tax liable. In real life, this line will be very blurry: which percentage of a large sedan or business attire is required for the job, and which percentage for private use? Specifying these rules and regulations for the tax liability of self-employed people (and unincorporated firms) will be very tedious. Still, a necessarily unelegant, but rough approximation of what is private and what is business expense should be possible. 4.13.0.3 Fringe Benefits. An equivalent problem occurs when incorporated firms hand out non-cash benefits to employees (or owners). These non-cash benefits, while never formally detected as a cash flow under , may constitute additional non-cash income. Without further specification of the PCT, such fringe benefits would escape taxation entirely: private consumption of the (luxurious, gas-guzzling) company car would go untaxed. To avoid untaxed fringe benefits, the taxman has to audit the books of corporations to detect and quantify non-cash Benefits98 . Employees who receive non-cash benefits would be tax liable on the cash equivalent of these benefits as incomes. All other things equal, if they do not offset their company car use by additional saving, they will be taxed on its consumption. 4.14 Gifts and Bequests Gifting and bequesting will not be taxed. It is an accrual to (or loss of) wealth (income), but does not constitute consumption. Instead, gifted resources will be taxed when recipients consume them out of their respective (page ). When non-cash resources are gifted, donor and donee have to agree on an evaluation of the gift. Donors and donees will have opposing incentives to evaluate the gift. Donors will have an incentive to overstate the value of their gift: the more inflated the value, the smaller their income, the smaller their taxable consumption. If real income drops less than the gift-inflated reported income, people can dissave and spend the difference tax free. Donees will have an incentive to understate the value of their gift: the more deflated the value, the smaller their income, the smaller their taxable consumption. If real income increases more than the gift-inflated reported income, people can dissave and spend the difference tax free. Ideally, donor and donee will agree on and report an evaluation close to the equilibrium price of the gift as a result of their opposing incentives.99 4.15 Imputed Income People can consume their own income without either the income or the consumption ever showing up in a cash flow. This occurs when people iron their own clothes, or when they sit idle and enjoy their free time. Both self-supply and leisure would go untaxed under a pure PCT. People would be incentivized to iron their own clothes, rather than have someone else do it for a cash flow detected price. Likewise, people would be incentivized to enjoy their leisure, rather than enjoy some additional good or service they could have earned in the same time. This is undesirable for two reasons. First, it pre-tax market prices in an arbitrary way, in violation of desideratum \\[des:minimal-DWL\\]. Ironing your clothes yourself or giving them to a dry cleaning should not be taxed differently. Second, it favors autarky over welfare-improving trade. When cash-based division of labor is disincentivized, many absolute and comparative advantages may be foregone. The dry cleaner may be much more efficient, or at least relatively more efficient in ironing your clothes. This problem can be solved by including an imputed income in the cash flow for everyone. If you earn no income at all and spend nothing at all, the assumption would be that you consume your own work entirely. If you are living the autarky lifestyle, you cannot offset your imputed income with any savings. All of your imputed income is, by definition, consumed, and taxable. If you are living the separation-of-labor lifestyle, you receive an actual income on top of your imputed income. You can spend your actual income and save out of your imputed income. At (presumably) equal living standards, only one of the two incomes is consumed, and taxed. The PCT with an imputed income is neutral towards the relative configurations of leisure, self-supplying consumption and cash-compensated separation of labor. An imputed income also serves as an elegant backstop to barter transactions undertaken to evade the PCT. 4.15.0.1 Strengthening Comparative Advantage. The theory of comparative advantage dictates that one should specialize in whichever production one is relatively more efficient (Ricardo 1817). Even if a doctor can iron her shirts faster than the worker at the dry cleaning, she should outsource ironing because she is relatively even more productive at providing medical care, compared to the worker with no medical training. Efficient incentives should reflect the comparative advantage of the doctor in providing medical care. Under the PCT, even with a flat imputed income, this is not the case. All other things equal, a highly productive doctor has an equal tax incentive to send her laundry to the dry cleaning as, say, a social science PhD student with questionable productivity. When they do their own ironing, both are taxed on the same, flat imputed income that they are consuming in supplying themselves. In reality, the two have very different opportunity costs in ironing: doing another brain surgery, or spending another afternoon on the internet. Ideally, the doctor should be incentivized to outsource much more at same levels of consumption than the PhD student. A variable, imputed income based on past earnings accomplishes just that. If you have shown to be very productive, you will be incentivized to devote your time to that highly productive and well-paid activity. If the doctor irons her own shirt, she consumes out of a much higher imputed income on which she will be taxed. If she instead gives the shirts to the dry cleaning and does another brain surgery in the meantime, given same levels of consumption, she will be taxed only on the presumably lower price paid in cash at the dry cleaning. It is important to note that including even a variable, imputed income based on past earnings does not force more productive people to work more. At same levels of consumption, it merely incentivizes them to work whichever time they choose to work at whatever they are best at. An imputed income based on past earnings also does not force highly productive people to adopt any specific combination of work and leisure. It merely incentivizes them to adopt the same combination as other people. Absent a dynamic, imputed income, very productive people could maintain the same standards of living (consumption) with much less work than less productive people. Still, to incentivize productive work, an imputed income based on past earnings must not entirely reflect higher productivities. If higher productivity does not allow you to either work less or consume more, you may never strive too hard in the first place. 4.16 Cross-Border Transactions As with any tax, additional complications arise when cross-border transactions are concerned. The problems arising from different tax rates or tax regimes between countries are not addressed here. The real existing international political economy of taxation is what is malleable, or what demands explanation given the doable and desirable hypothetical established here. 4.16.0.1 The End Game. First assume that all developed countries have transitioned to the PCT. All income, value-added, corporate and withholding taxes are abolished. People would be taxed based on residence (Graetz 1979, 1645).100 Consumption abroad would be liable to taxation at home. Investment abroad, when properly documented, would qualify for tax deductions. If in doubt, the taxman can simply assume that cash flows to another country constitute consumption until otherwise proven. Conversely, aliens consuming in another country would be taxed in their home country of residence. 4.16.0.2 The Real World. In the real world, international transition to and harmonization of the PCT is likely to occur slowly, if ever. In the meantime, introducing the PCT will create some problems and complexities. Citizens investing or consuming abroad can be taxed on their global consumption at home. Receipts for VATs, income or withholding taxes paid abroad can be deducted from their PCT liability. Taxing consumption abroad may violate WTO regulations on the free movement of goods, services and capital. People can evade the PCT if they cash in foreign capital and labor incomes abroad and consume it there. Considerable international tax cooperation will be necessary to avoid such evasion. Non-PCT countries may have few incentives to cooperate. Foreign residents earning labor or capital incomes at home, absent any PCT account for them, would still have to be income taxed. Similarly, to maintain stable revenues, government would have to retain withholding taxes on foreign-owned corporations.101 This will create considerable administrative duplications and contradictions. 4.17 Family Notwithstanding in accordance with desideratum \\[des:personal-taxation\\], people who share living quarters and pool resources should be taxed as one household, as argued in . Under a progressive schedule and with no, or only a small penalty to mechanical solidarity in marriage or other forms of intimate cohabitation, the tax liability of such units will depend on the distribution of consumption (not income) in a household. A big spender marrying a small spender will enjoy a much greater benefit than two small spenders. Aside from spouses, children are often dependents relying on mechanical solidarity. Under the pure PCT, they are treated equivalently. Their income and saving is added up to a household-wide , consumption is then divided by the number of persons in that household and taxed at the respective rate of average consumption in the family. While this will already give parents a tax break, it is not clear that the relief would be sufficient or appropriately distributed. 4.17.0.1 Why Averaging May Not Suffice: Children Are Investments. Averaging taxable consumption alone will treat a child exactly like a spouse. Considering the low fertility rates in many developed countries and the severe dissaving of an aging (not shrinking!) population, this equal treatment may be insufficient. Rearing a child is fulfilling, but it is also a costly investment into the future of your society. Averaging taxable consumption alone will give limited financial incentive to procreate. I cannot address here whether such pro-natalist policy would be normatively desirable, or even effective. Either way, the polity should be able to incentivize child rearing, if it so sees fit. 4.17.0.2 Why Averaging Only May be Undesirable: Tax Breaks Favor the Big Spenders. Again, as in marriage, the value of the tax break depends on the distribution of consumption within the family. The starker the differences, the greater the benefit. Assume that children require relatively less consumption than adults, and are less prone to excessive consumption on their behalf: compared to the very big spenders on cars, yachts and houses, even a designer-clad toddler is relatively cheap. It follows that those very big spenders, taxed at higher rates will benefit the most from averaging their consumption with that of their (truly) smaller spender children. It is hard to defend normatively why of all potential parents the big spender should reap the greatest fiscal benefit for rearing a child. 4.17.0.3 Add to the PCT: An Allowable Deduction for Children. Both of these shortcomings can be addressed by supplementing the PCT with an allowable deduction for children. Such a flat deduction would treat consumption on behalf of a child up to a certain amount as an investment into the future. It would be fully tax deductible. This could be accomplished by simply adding a fixed amount of annual saving for every dependent child to that household’s Haig-Simons equation. An allowable deduction for children would create an extra incentive independent of the level of spending of the parents, so as long as they consume anything102 . 4.18 The Formula for the PCT Specifying the schedule and formulae for the PCT will require extensive econometric and fiscal research, as well as frequent adjustments during its introduction. I can only list the terms of the formulae here, and suggest some tentative criteria for each of the terms. 4.18.0.1 Taxable Consumption in Real, not Nominal Terms. Taxable consumption is given by the (page ), with the addition of gifts and bequests made and received, imputed income and an allowable deduction for children. Taxable consumption is then averaged over the entire household. Taxable consumption should not be in nominal values for two reasons. First, nominal consumption would be subjected to inflation. If the schedule is constant, people pay ever more taxes by a dynamic called cold progression103 . Second, economies grow. As our economies get richer and richer, consumption should grow, too. Notwithstanding the need for a higher savings quota, there is no reason to tax consumption in absolute terms for all time. Rather, consumption should be taxed progressively in relation to overall production, or the size of the cake. For these reasons, taxable consumption should be denominated relative to a multi-year running average of GDP.104 4.18.0.2 Progressive Schedule. The progressive schedule, while open to a great deal of political discretion must satisfy the following conditions. 4.18.0.2.1 Sufficient Revenue. To be revenue neutral, or to raise more revenue for underprovided public goods and risk pools, the rates must be sufficiently high and sufficiently steep. Because discrete price elasticities of demand in positional or excessive consumption are unknown, revenues for any given schedule are hard to project without further econometric research. The schedule will likely require careful adjustment as it is introduced and actual price elasticities are observed. 4.18.0.2.2 Savings Rate. The schedule must be sufficiently steep to incentivize the democratically elected savings rate, taking into account real dissavings. Again, it is hard to project savings behavior for any given schedule without further econometric research or implementation in the real world. 4.18.0.2.3 Generational Incidence. To alleviate the triple taxation of old savers upon the introduction of the PCT, the schedule could be allowed to vary for people of different age. A term expressing the percentage of life expectancy lived under income taxation could be included in the formula to discount the tax burden for older savers. 4.18.0.2.4 Entry Bracket. The lowest bracket will be particularly hard to determine in width and rate.105 On the one hand, it must be sufficiently wide and sufficiently low rated to minimize the as per desideratum \\[des:low-price-floor\\]. On the other hand, an entry bracket too wide in scope and too low in rate may allow too many small-spending people to free ride on the public goods and risk pools financed by everyone else. 4.18.0.3 Design. For the PCT to garner popular support and to work its magic, intelligent design is key. 4.18.0.3.1 Fully Automatic. Both to enable instant marginal rates and to reduce administrative cost, the PCT liabilities should be calculated with minimal human discretion, and invoiced fully electronically. Since cash flows are already recorded at financial institutions it probably makes sense to interface them directly with tax administration and have them withdraw tax liabilities on a regular basis. Given modern information technology and enough time for careful development, an ideal PCT should work without people ever handing in a tax return or many other (paper) forms of any kind. The tax administration will establish which cash flow recipients qualify as investment. Citizens will indicate cash flows that are donations or gifts. All other cash outflows will be treated as consumption. Whenever consumers buy an officially recognized durable good, the respective price will accrue to their consumption over time according to a fixed depreciation schedule. Lists of durable goods and respective depreciation schedules already exist in developed tax administration. Whenever consumers buy owner-occupied housing (or some other extremely durable, owner-used good), they are charged with the imputed rental rate. Tax administrations would have to establish electronic lists with fair market rental rates for such goods. 4.18.0.3.2 Instant Marginal Rates. PCT liabilities must be invoiced much more often than income taxes. People cannot wait until the end of the year until they know how much consumption tax they paid on the last present they bought for Christmas. For the PCT to be transparent, and to reliably signal people against positional consumption, people need to know in real time their marginal rate of progressive consumption taxation at every possible consumption decision. Because the PCT is personally progressive the listed gross prices of goods in the store will have little resemblence to the net prices incurred to individual taxpayers. One apple does not have the same VAT rate for everyone, but it may have a tax rate of zero for one small-spending person, and a tax rate of 800% for the big spender next in line at the grocery store.106 Modern ICT (think mobile devices) should enable intelligent designs to inform people about their current marginal tax rate. 4.18.0.3.3 Running Averages. Annual invoicing of the PCT also makes little sense because that would cause lower PCT rates for everyone in January than in December of each year. At the other extreme, it also makes little sense to tax people for their consumption every day, or even every month. Not only would consumption be cheaper in the morning than in the evening, people would also be punished excessively for unavoidable, short-term spikes in their consumption. Under daily taxation, buying a suit on one day would be taxed at a higher rate, than buying the jacket and pants on different days. Under monthly taxation, a long summer vacation would be taxed at a higher rate than monthly weekend trips. This makes little sense because it violates the (page ). People must be able to shift consumption freely to some extent. In the real world, people probably also receive utility from a long summer vacation for a longer time than just the duration (let alone the infinitesimally short expensing) of the trip. Summer vacations, suits and other consumables cannot be practically depreciated. Instead, people should be taxed on running averages of their consumption over extended periods of time. At minimum, an annual, moving average of consumption will be required for meaningful and practical taxation of consumption. A lifetime average consumption, with significant discounting of long past consumption may also be worth consideration. 4.18.1 Not All Progressive Consumption Taxes are Created Equal. “The real and pressingly practical question for tax is not whether to have an income or a consumption tax, but what form of consumption tax to have. The stakes in this battle are clear and dramatic: the fate of progressivity in tax lies in the balance.” — Edward J. McCaffery (2005, 817) The PCT has been considered and suggested many times in recent history. It was considered after World War I as a graduated spendings tax (Bank 2004, 2), again in 1942 by Treasury Secretary Henry Morgenthau under the label of graduated surtax on spending and, most recently, in 1995 by Senators Sam Nunn (D-GA), Pete Domenici (R-NM) and Bob Kerrey (D-NE) as the Unlimited Savings Allowance (USA Tax).107 It is important to note that not all these, nor current proposals are created equal. They differ in important details from the PCT proposal suggested here. First, many PCT proposals, including the 1995 USA tax do not include dissavings in their definitions (Bank 2004, 18). A PCT that exempts consumption out of debt or savings, leaves a colossal share of excessive consumption untouched. Such a PCT is very limited in its progression. Second, many PCT proposals are not radical enough in their departure from an income tax regime. They keep some of the old clutter and cause debilitating interactions and complications. The USA tax, for example, did not terminate the estate and gift taxes, even though these are transfers of wealth, not new consumption (Bank 2004, 19). The resulting administrative complication (evaluation of illiquid goods) and necessary deductions to avoid double taxation of spent gifts and bequests made it an incoherent nightmare.108 4.18.1.1 The PCT’s Evil Neocon Twin. A particularly brazen disfigurement of the ideal of progressive consumption taxation comes from the American Enterprise Institute (AEI), which sponsors and markets the (???) X-tax as a progressive consumption tax (???). The Bradford (1986) X-Tax is a two-tiered tax originally suggested in (1986). It comprises of a flat, European-style VAT with deductions for all wages, and a progressive wage tax. Why anyone would use the term “progressive consumption tax” to describe the Bradford X-Tax is hard to explain, except for sinister intent to create confusion.109 4.18.2 The Least Imperfect Tax Borrowing Churchill’s classic adage, the PCT may not be the perfect tax, but for all the alternative taxes we know. It has two fundamental shortcomings. 4.19 Work/Play, A Constructed and Diffuse Border On Which Taxation Depends The fundamental problem in bringing the PCT to the real world is that it depends on a very clear distinction between investment and consumption. The problem is: this “notion of a sharp division between pleasure-seeking and profit-seeking is alien to human psychology and essentially unrealistic” (Bittker 1973, 203). Just like (page ) in (page ) the PCT depends on a distinction that does not unambiguously exist in the lives of people. %this is the big-ass homo-economicus question. %Include a somewhat extensive discussion, include extensive references, argue why tax (materialism) still matters. Where PCT administration defines and enforces this boundary anyway, severe distortions, evasion and administrative complications necessarily ensue. The problem of identifying fringe benefits already exists under the income tax (Graetz 1979, 1591), and they will become more severe as incentives for tax-free, excessive consumption increase. There is no easy or elegant fix to this. Tax administrations cannot fairly and simply establish whether an AC at the office, an extravagant business lunch or an executive MBA in an exotic location is an investment, or consumption. It is probably both, to some extent. These gazillions of loopholes for tax evasion cannot be elegantly closed, by one strike of tax administration genius. Instead, the PCT will require tax administrations to plug each and every one of them at a time, all the while not stifling economic activity and maintaining some degree of fairness. It appears that whenever government wants to tax progressively, it is, to varying degree forced into the role of central planner and micromanager of inequality. The PCT may just offer the least severe of these administrative nightmares, or at least, no worse than the nightmare of progressive income taxation (Graetz 1979, 1595). 4.20 Stratification, Elsewhere. Notwithstanding its grandiose ambition to curtail positional consumption, the PCT will not do away with all harmful stratification in the modern world. People may simply find other avenues for the very same, very dysfunctional positional games. Straightforwardly, people can show off their wealth by displaying slowly depreciating durables, or even jewelry. Even when these are taxed on imputed rents, people can still play status games in all our immaterial dimensions of difference. With no notice to the taxman or your democratic polity, you can break The Spirit Level (Wilkinson and Pickett 2009) through cultural distinction (Bourdieu 1984), romantic and sexual relationships (Mazur 1993) or anything else that makes us not only different, but also unequal. This is not so much a shortcoming of the PCT as it is the end of democratic, liberal policy. Whenever policy moves beyond material leveling, by prescribing habitus, or even whom people meet and mate, it risks poisoning the polity with totalitarian zeal. 4.21 An Angel in Tax “I will not add fuel to think tank fires. Complexity can wait. The devil may indeed dwell in the details, but we first need to find an angel or two in the abstractions that govern tax.” — Edward J. McCaffery (2002, 6f.) The limitations and problems of the PCT notwithstanding, it is the least imperfect tax. In the abstractions even more than in its implementation, these angels point to the PCT. References "],
["crises.html", "Chapter 5 A Tale of Three Crises 5.1 A Crippled Mixed economy 5.2 Possible Better Worlds 5.3 Better Taxation 5.4 Better Democracy 5.5 Bad Tax 5.6 Deliberation and Democracy 5.7 Misunderstanding Tax 5.8 A Vicious Cycle 5.9 Critique of the Literature 5.10 Why it Matters: The Welfare State as Mixed Economy 5.11 Tax 5.12 Dysfunctions 5.13 An Old Deal 5.14 Why Tax Matters to Democracy 5.15 The Social Contract", " Chapter 5 A Tale of Three Crises There are a lot of rational (tax) ideas out there, there are a lot of ways you could build something from scratch that would look roughly like the society we have now, but would work better, would be smoother and fairer and all that. And of course, these things are proposed in politics […] and then they cannot be done. Then our system prevents them from happening or garbels them in such a way that they look supremely ugly, once they are created […]. This is a machine for creating disillusionment with government, cynicism, Tea-Partyism, and over time, this is our county, our democracy eating itself alive. […] And that’s why I get a little discouraged sometimes. — Hendrik Hertzberg on the * Political Scene* podcast by the New Yorker, discussing US tax reform plans in 2011. 5.1 A Crippled Mixed economy In the preceding , I have tried to shed some light on the in much of the current oecd-world (, p. ). The puzzle is now a little clearer: In spite of unprecendented prosperity, late oecd welfare states are less efficient, equitable and sustainable than they could be, because they fall short of an (, p. ). Among the institutions of the mixed economy, especially, because it is by far the most effective and efficient tool for government to improve upon, or redistribute market outcomes (, p. ). Equipped with a deficient mixed economy and suboptimal taxation, democracies are forced to make unnecessarily unattractive trade-offs between efficiency, equity and sustainability, eroding their output legitimacy. At the same time, the legitimacy of democratic inputs may be undermined by a widening gap between the complexity of an advanced (mixed) economy and the simplifying tendencies of electoral democracy. Potentially, pluralist democracy may, in part, have fallen victim to the tightly concentrated special interest that late capitalism offers, especially in the field of taxation. Taken together, such diminished legitimacy in outputs and inputs, may help explain widely reported dissatisfaction with democratic performance (but not institutions) (for example, Dalton 2004) and contribute to the apparent mismatch between popular opinion and the governing abstractions of the mixed economy. Net of such constrained welfare and confused democracy, material and political equality would suffer. A deficient mixed economy and suboptimal taxation would corrode the social contract, by which democratic government should have some ability to redistribute the fruits of non-zero-sum games borne by the free exchange of state-protected property rights (compare Crouch 2004). Offered only unattractive choices, and thoroughly confused about the abstractions of the mixed economy, citizens would be increasingly impotent under the social contract. One suspects, especially poor and poorly educated citizens would be effectively disenfranchised from the social contract. 5.2 Possible Better Worlds 5.3 Better Taxation In the following (p. ) and (p. ), I show that under liberal-pragmatic axioms and orthodox ontology (, p. ), we could have vastly and . Among doable taxes, a potentially supplemented by a , and/or appears to offer more attractive trade-offs between equity, efficiency and sustainability than the current tax mixes dominated by , and (see , p. ). The Vector Field of Taxes with Trends 5.4 Better Democracy Among doable forms of democratic rule, deliberative democracy (for example, Cohen 1989) appears both more promising and demanding than pluralism. Deliberation promotes egalitarian, inclusive, well-reasoned and civic-minded discussions for (possibly) consensual political decision-making. Its normative theory draws heavily on Habermas (1984)‘concept of communicative action and Rawls (1971)’ “Theory of Justice” as fairness. Where pluralism encourages the representation of particular interest, deliberative democracy demands alternative conceptions of the common good (Cohen 1989, 18). Where pluralism aggregates given, pre-social preferences, deliberative democracy is all about forming preferences. Where pluralism assumes individual voter error to balance out, when errors are random (Page and Shapiro 1993; Surowiecki 2004), deliberative democracy insists on perfecting the deliberators understandings (see , p. ). Deliberative democracy offers more attractive trade-offs between participation, enlightened understanding and political equality (Fishkin 2009) (, p. ) and promises to alleviate some of the public choice contradictions of pluralism (for example, de Condorcet 1785; Arrow 1950) (, p. ). It also transcends the impoverished, and dubious ontology of pluralism (homo economicus!), and opens legitimate rule to feminist or even virtue ethics. 5.4.1 Crisis of Equality Again, not an empirical claim per se, but it seems worth nothing that as per a priori logic, there may be runaway dynamics of inequality, for which we might have no possible counteracting tax institution. pperhaps, we don’t actually want to use that institution, or only to a limited amount, because its costs are too high, or the boons of overall growth are too great, but whatever, it’d be nice to have it available to the sovereign. A complete empirical test of the governing dynamics of inequality is beyond the scope of this thesis.110 Here are some tentative statistics which support the thesis of self-reinforcing inequality. According to the US Survey of Consumer Finances (SCF) the range between the lowest and highest sextile (!) of median before-tax family income has risen from $145,000 in 1989 to $170,000 in 2004 (Bucks, Kennickell, and Moore 2006). During the same period, the mean gap between the outer sextiles has increased from roughly $245,000 to $295,000 (ibid.). The mean income gap for extreme quartiles of education of head of household has increased from $80,000 $115,000 (ibid.). While the median net worth is reported to to be roughly $90,000 in 2004, the respective mean is around $450,000. The median (mean) range between the outer quintiles (!) of net worth has increased from $1,000,000 (around $1,800,000) in 1989 to above $1,400,000 ($3,000,000) in 2004. Interestingly, even the median (mean) gap between the topmost quintiles has increased from $600,000 ($1,500,000) to $900,000 (above $2,500,00).111 In Germany, a similar trend is discernible. From 1998 to 2006, the Gini coefficients112 in real gross wages (not incomes!)113 continuously rose from 0.407 to 0.453 (Bundesregierung der Bundesrepublik Deutschland 2006, 14). Interestingly, even the range between the topmost deciles in share of all gross wages rose from 9.8 to 10.6 (Grabka et al. 2007, 64). For all gross (net) incomes the Gini coefficient rose (roughly continuously) from 0.465 (0.261) to 0.520 (0.316) (2007, 82). The Gini of wealth distribution rose continuously from 0.745 in 2002 to 0.795 in 2007 (2007, 138). In sum, inequality seems to be increasing over time, in labor and capital income, and in wealth. Distributions of income and wealth are, in fact, decidedly non-normal, but starkly positively skewed as evidenced by the ever diverging medians and means. These tentative statistics then support the central tenet of self-reinforcing governing dynamics of inequality. 5.5 Bad Tax “Things in tax are bad today.” — Edward J. McCaffery (2005, 893) 5.6 Deliberation and Democracy Why does deliberating taxation matter for democracy? A dysfunctional mixed economy without good taxes may produce illegitimate outputs, and, in so far as it disturbs the state-market balance of the mixed economy, violates output congruence of a democracy (compare Zürn 2000, 190). Yet, even if experts could agree on a tax — such as a promising, post-paid, cash-flow based pct — the tradeoffs involved in choosing or calibrating a tax regime are irreducibly political. For example, experts cannot positively inquire whether economic “growth”114 is in itself virtuous, which inter-subjective aggregation would be utility-maximizing, what deontological rights owners have or which human relationships should be governed by prices or command at all — even if they could agree on any of those alternative ethics. These metaphysical conundrums are reflected in very concrete, technocratic issues in taxation. Likewise, experts cannot justify the axioms on which their (economic) analyses rest: whether, and under which institutional circumstances we are homines oeconomici, cannot be exhaustively answered from observation, because nature provides no intelligible moral messages on what kind of man we should be (Gould 1982, 43). This ontological stance matters for tax, too: for an expert to model a general equilibrium is to invoke, as well as to make, a homo economicus. Of all conceivable institutions to resolve these disagreements, deliberative — not aggregative, not pluralist, not participatory — democracy offers the greatest normative appeal, because these disagreements raise, in Habermas (1984)’s words, competing validity claims, in need of argumentative redemption, as in: is “growth” comprehensible, who truly bears a cit, is homo economicus right, and are social insurance employer co-payments truthful? I illustrate further more, and less valid claims on taxation. Liberal democracy, born out of the hope that material inequality and political autonomy can be reconciled, especially, rests on deliberation, because if anywhere, it is in taxation that money and power may have replaced language to govern meaning and action (Habermas 1971). In fact, if you wanted to hide some process of (re)producing inequality anywhere, you could hardly find a better place than the intricacies of (income) taxation (consider, for example “tax planning 101” in McCaffery (2005, 888)). Instead of communicative action (Habermas 1984), what emerges today from citizens, their legislatures and public spheres, may be more “analytic muddle of tax” (McCaffery 2005, 862). The beliefs held and issues considered in the public (for example, Caplan 2007), appear to bear little resemblance to the abstractions suggested by economists (for example, Harberger 1974). People seem to ignore fundamental choices in tax (for example, McCaffery and Hines 2010), and may fall for inconsequential alternatives (for example, Sausgruber and Tyran 2011), possibly even err systematically against their own material interest — if they are on the lower economic rungs (for a German example, Kemmerling 2009). Here, too, (input) congruence of democratic rule may be violated if and to the extent that people are confused about policy choices (compare Zürn 2000, 90). 5.6.1 The Good Old Days 5.7 Misunderstanding Tax 5.8 A Vicious Cycle These two crises — an ineffective and/or inefficient welfare state and a confused democracy — may partly reinforce one another into one perfect storm of societal reaction by stealth: The two issues, the crisis of egalitarian politics and the trivialization of democracy, are not necessarily the same. Egalitarians might say that they do not care how manipulative of democracy a government is, provided it divides society’s wealth and power more evenly. A conservative democrat will point out that improving the quality of political debate need not necessarily result in more redistributive policies. But at certain crucial points, the two issues do intersect []. My central contentions are that while the forms of democracy remain fully in place — and today in some respects are actually strengthened — politics and governments are increasingly slipping back into the control of privileged elites in the manner characteristic of pre-democratic times; and that one major consequence of this process is the growing impotence of egalitarian causes. — Colin Crouch (2004, 6) 5.9 Critique of the Literature &lt;!-- ### TINA &gt; *&quot;There \\[...\\] is no alternative.&quot;*\\ &gt; --- Margaret Thatcher (London, 1980) TINA is hardly observed directly in the literature, if only because, as a political strategy, it aims to &quot;veil the essentially political character of political decisions&quot; [@Bluhdorn-2007-aa 314], and therefore, will not even explicate itself. The purported lack of alternatives, mostly, asserts itself not by negating existing alternatives, but by ignoring them. --&gt; Welfare regimes are, as (Esping-Andersen 1990) has said, systems of stratification in themselves, and they have been, from the very beginning socialist demands in the 19th century more than epiphenomenal income replacement. They are tools to socialize the costs of painful, but welfare-enhancing (???) economic transformations as well as individual hardship and serve as engines to redistribute this very welfare as the legislator wishes. The indeed quintessential and highly political question for globalization and welfare entrenchment is then, whether the state can still (or ever could) socialize and redistribute as it wants, with no limitations resulting from the behavior of other states. Welfare state sustainability then has to be pitted not against past or current performance, but against a hypothetically desired welfare regime under global trade with global redistribution, both within and between countries. In game theoretic terms, to gauge how badly welfare-depressing defection is, you first have to calculate the payoff for mutual cooperation. In the global context, this certainly requires quite a bit of political imagination, something that political scientists, it appears, like to shy away from. Even leaving normative considerations aside, in this case, academic rigor alone requires such exercise. If welfare states can be well or poorly designed mixed economies, achieving different outcomes, we should also judge its prospects by comparing actual or evolving regimes to such hypothetical, but possible and desirable configurations. That is a very different question than testing whether welfare states en- or retrench, let alone its spurious correlates of income replacement (Swank 2005) or even spending (Kleinman 2002, 24), and a question that would deeply unsettle Pangloss. As Offe (2003) reminds us: “The mode in which welfare state institutions change can be explicit reform and retrenchment. But it can also be inconspicuous and gradual decay. For instance, people may defect from public health and pension systems, trade unions see themselves forced into single-employer concessions bargaining, workers resort to unprotected forms of pseudo self-employment in order to avoid social security dues, if not to illegal (”black“) forms of employment.” — Offe (2003, 364) Genschel (2005), too, reminds us of what Pangloss would rather have us forget: “The effect \\[of globalization\\] is not so much to force change upon the tax \\[and thereby, welfare\\] state as to reduce its freedom to change.&quot; (2005, 53). This is why political scientists such as (Pierson 2001, 1996) that assume institutional constraints (for example, veto points, Tsebelis (2002)) to only work to prevent retrenchment, are wrong: the very same constraints that may prevent or delay nominal cuts will also make it harder for welfare states to adapt to, rather than just to recede in the face of changing economic and demographic circumstance. Nondecision does not”generally favor the welfare state&quot; (Pierson 1996, 174). While the welfare state might nominally have retrenched only “cautiously” (Pierson 1996, 174), the ground underneath it has shifted, leading to a much graver de-facto change of positions: it can no longer expand or react, relies on unsustainable deficits or real dissavings and must make do with growing inequality, and sometimes, structural unemployment, non of which Pierson (1996) even mentions. To shake off Pangloss, and see clearly the demise of the welfare state, we must ask three new questions, that so far, much of the retrenchment literature has shirked: What, given a hypothetical, intact mixed economy, would welfare states be capable of, if their democratic sovereigns wanted it? This is a question that, absent natural experiments on the matter, cannot be subjected to straightforward positive test, because precisely such a hypothetical, intact mixed economy does not exist. Still, we must compare actual to hypothetical regimes, to find out just how constrained actual welfare states may, or may not be, by whichever second-order process we subsequently propose. What is the highest possible trade-off between equity and efficiency that an intact mixed economy can offer their democratic sovereigns? As I have argued in the above, better-designed mixed economies face less harsh — or even no — trade-offs between equity and efficiency than worse-designed mixed economies. The price for an additional increment of equity in efficiency (or vice versa) not only varies at the margin,115 but it also varies depending on the set-up of the mixed economy. For example, a highly progressive tax on consumption may allow same or greater equity at a much lower price than a compressed tax on labor income.116 More broadly, Ganssmann (2010) has proclaimed the Scandinavian welfare state as the “winner” to achieve higher levels of both equity and efficiency (2010, 343). Amongst these higher trade-offs between equity and efficiency, there may, additionally, be local optima of equity-efficiency mixes, complemented by quite distinct institutional constraints and — somewhat related — path dependencies, ranging as wide as educational systems and industrial relations. I have here mostly ignored these Varieties of Capitalism (Hall and Soskice 2001)117 They, too, are important detail. They strongly suggest that there may not be one universal mixed economy design, but that quite different designs might coexist and specialize. Still, also within each of these varieties, there are again different trade-offs between equity and efficiency. Interestingly, the institutions that Hall and Soskice (2001) identified as markers of cmes and lmes do not mention variants of tax, social insurance or any of the other key welfare state institutions. While cmes may correlate with, and are often conflated with Bismarckian welfare states, there is nothing about cmes that would make them necessarily rely on, for example, labor income-backed social insurance. Of course, the variant of capitalism will be reflected in the nitty-gritty of welfare state institutions: for example, the statuses originally maintained by Bismarck are, arguably, related to the categorical groups that cme educational systems create, or cme industrial relations are organized around. This institutional spill-over notwithstanding, I would hypothesize, that cmes and lmes might be able to achieve equally high trade-offs between equity and efficiency, even if the institutional implementation may vary: for example, cmes might continue to sport extensive job protection, while lmes will allow quick “hire and fire”, potentially complemented by generous unemployment benefits (as in “flexicurity”). Allocative results, either way, may be very similar, which is my point here. Even carefully crafted positive research into changing welfare states currently looks, at best, at cross-sectional or longitudinal variation in social transfers as a percentage of output (for example, Ravenhill 2005, 249). This is much better scholarship than the naysayers who like to look only at absolute spending or income replacement, but still, it does not tell us how good a trade-off we are getting. To gauge the level of trade-off between equity and efficiency available to democratic sovereigns, the retrenchment debate has to look at allocative results, not at transfer flows, that is, at inequality (for example, Gini-coefficients) and growth (preferably measures more comprehensive than gdp). Out of logical necessity, if nothing else, welfare state retrenchment, inequality and growth are one question. The compartmentalization of these into different academic areas allows not, as one would hope, greater theoretical clarity but instead confuses and waters-down concepts. If “welfare” is to mean anything, surely, it must be the ability of states to alter allocative results, and to do so at a minimum, or democratically acceptable price in efficiency. Consider the alternative research designs, that currently predominate. If income replacement stays the same (Swank 2005), but, realistically, incomes become more unequal and states more indebted, is that evidence of a non-retrenched welfare state? If transfer volumes rise absolutely, or stay constant relative to output (Ravenhill 2005), but, realistically, ever more people rely on ever smaller transfers, all paid for the labor-incomes of an already squeezed middle class, is that evidence of a non-retrenched welfare state? Surely, just external validity requires more extensive operationalizations. The best, theory-driven operationalization of a non-retrenched, welfare state is the intact mixed economy. How well does the welfare state work as an entire system of production and distribution, that is, as a mixed economy? This is a very different question from those based on a traditional, more limited definition of the welfare. Offe (2003), for instance, takes pains to remind readers that welfare states have nothing to do “with equality of outcomes’, neither normatively nor positively”, that “the guiding principle of principle (…) is the security and protection of workers, not equality” (2003, 450). This is a historically accurate definition, but it is no longer an externally valid conceptualization of welfare states, if the term is to be more than an empty hull devoid of positive reality and economic possibility. “Welfare states as worker protection” is not mece any more. By this definition, an economy, or rather, sectors thereof would be classified as a “welfare state”, in which poorly qualified workers are nominally protected, but either structurally unemployed because their gross wages are higher than their productivity, or live in working poverty, ever unable to participate in the riches of the wider economy. To the people working in cleaning or security in Germany today, such a definition would not have a lot of face validity. The labor market for poorly qualified workers in Germany is then, at the same time, evidence of a welfare state and evidence of a non-welfare state. Conversely, by the traditional definition, an economy, or rather a sector thereof with no nominal protection, but generally high compensation and little economic hardship, would be classified as a non-welfare state. For example, freelancers in management consulting, earning handsome but unsteady labor (!) incomes, but equipped with enough assets to weather rainy days, surely do not enjoy welfare state protection. Still, at face validity, they also do not exactly suffer from Manchester style laissez-faire. Under the traditional definition, they reality is neither welfare-state, nor non-welfare state. Offe (2003) might, asked about the plight of German cleaners, point to a leak in the “Keynesian” roof of full employment, protecting the lower storeys of welfare states. Today, full employment is only a necessary, but not a sufficient condition for an intact roof. In fact, full employment might always have been merely necessary, and we were just lucky that in the past, all other necessary conditions were mostly met. To stay in Offe (2003)’s elegant metaphor, the welfare state house is facing much harsher weather. For example, severe crosswinds of rising income inequality (for example, winner-take-all, efficiency wages) diverging factor returns (for example, Stolper-Samuelson trade), threaten to further drive apart the different economic strata making up the house, threatening to tilt the building. In addition, international tax competition, but also home-made dysfunctions are eating away at some of the load-bearing walls, putting enormous stress on the few remaining walls and the (already struggling) people making it up. If all we care about in this house is whether the roof is still tight against cyclical unemployment, the structure will not stand much longer. If the house of the welfare state is to survive the throes of economic transformation, it needs strong cross-beams, to re-balance the load of its stories. These cross-beams are progressive redistribution, and we measure their solidity by looking at overall inequality. Today, if not always, the ability of a mixed economy to efficiently curb runaway inequality is the sine qua non of welfare states, too. This is not new to (Offe 2003), who also includes “monetary, fiscal, trade and economic policies” in the roof (2003, 543). However, he seems to neglect that consequently, the different stories of welfare protection cannot be organized (financed) irrespective of overall inequality: if, for example, the fiscal shingles are to remain intact, the protection schemes must charge those most who can best afford it and in a way that will least affect them.118 Surely, Pangloss would already despair over (Offe 2003)’s insistence on a full-employment protecting roof. But with inequality, we can and should ask him an even harder question that might reveal his unreasonable optimism in starker colors. In addition to these functional reasons, there are normative and empirical reasons to demand of welfare states worthy of the label to, at least, be able to curb inequality. Normatively, it seems questionable to constrain the surely emancipative agenda that once endowed welfare states to worker protection. That’s quite little to ask of Pangloss. Empirically, we know that people care about relative differences in access to resources (Frank 2005a), that they suffer from relative inequality (Wilkinson and Pickett 2009). If we are welfare state researchers and, as humanists, care about the human outcomes of institutions, maybe more than evident at Bismarck’s time, today inequality is that relevant outcome, even if and to the extent that absolute material security is achieved. To do meaningful second-order, sociological or political science work on the welfare state, you first have to do some first-order, economic work, if nothing else, because it lets us talk about welfare as if people mattered: “In the first instance, we are interested in the welfare state because we are interested in human welfare.” — Haggard and Kaufman (2009, 236) &lt;!— 5.10 Why it Matters: The Welfare State as Mixed Economy At this point, a reader may ask: why would all, or even any of this, matter to the welfare state? Materially possible and normatively desirable welfare states are, and must be thought of as, well-designed mixed economies, for at least three reasons: Engaging Complexity. In a welfare state, (p. ). For example, a (p. ) and the (p. ). To eradicate today, as Lord Beveridge promised 150 years ago, the five ‘Giant Evils’ of want, disease, ignorance, squalor and idleness, you have to anticipate this interplay of market and government and choose accordingly. The mixed economy provides us with a toolset to analyze the complexity of welfare states, including the dynamics presented here. For example, the mixed economy suggests we should always check the dwl (p. ) and (p. ) of any redistribution, a welfare state may undertake. To answer the first-order question of welfare state design, as I have here tried to do, we need the abstractions of the mixed economy to know what is materially possible in a scarce world, filled with (at least some) homines oeconomici. For example, price controls may not be possible (without grave losses), but a well-designed, personal, almost arbitrarily progressive taxation is indeed possible in a closed economy. Denaturalizing Market Allocations. When we consider welfare state programs in isolation from the markets which they supplant, we easily end up naturalizing whatever markets have allocated. In fact, any given market exchange which a welfare state may seek to correct, is already and always contingent on the institutions, dynamics and distributions under which it occured. For example, rather than “fighting poverty” — as if that were an objective reality — welfare states must consider overall allocative dynamics (such as , p. ) and distributions (such as , p. ), and counteract them, as is seen fair. Markets do not make some people below an arbitrarily defined threshold “poor”, and leave others ok or even untouched. Instead, markets allocate incomes across the entire spectrum contingent on a host of institutions, dynamics and initial distributions. If government pursues a particular minimum standard of living for everyone, it might not only transfer income to those who fall below it, but may need to counteract those dynamics under which people slipped below the minimum standard in the first place. Market allocations, in short, are — and should be — no less subject to enlightened, collective human choice than remedial welfare state programs: “Increasing dependency is no law of nature but the result of socio-economic changes, which in turn react to human intervention” (Esping-Andersen et al. 2002 x). Caring about Outcomes. Haggard and Kaufman (2009) ((2009, 236)) remind us about “\\[...\\] the importance of pushing the research on the \\[Eastern European\\] welfare state down the causal chain towards its social consequences. \\[...\\] \\[A\\]ny meaningful strategy of comparison of the welfare state must ultimately engage its consequences for a variety of outcomes, from poverty and inequality, to physical quality of life measures, to economic outcomes such as the efficiency of labour markets, competitiveness and even economic growth. In the first instance, we are interested in the welfare state because we are interested in human welfare.” — Haggard and Kaufman (2009, 236) The abstractions of the mixed economy I have summarized here synthesize a lot of what we need to know about the material, and therefore social consequences of a capitalist welfare state. When we care about social consequences, the mixed economy suggests a great deal more to consider than just nominal welfare programs. For example, welfare states should not only provide social insurance, but also redress failing (p. ) and (p. ). When we care about social consequences, the mixed economy also implies that not all welfare programs are created equal. For example, welfare states interventions should (p. ) and use (p. ). When we care about social consequences, the mixed economy suggests that governments and markets are better at different things, and it ties welfare state interventions to specific justifications. For example, welfare states should nationalize or regulate utility markets if and to the extent that they are (p. ), but there is no reason to (as Germany presently does) redistribute within (p. ), who was supposed to only save the risk pool from (p. ). Caring about social consequences also means to leave markets alone, if they will likely serve material human need best. When we care about social consequences, the mixed economy reveals that efficiency and equity are not always opposed, but often go hand in hand. Inefficient is Inequitable. Titmuss (1974) urged welfare states not to exclusively concentrate on poverty relief because such “residual services (…) often become poor services for poor people” (1974, 134). This intuition is supported by the abstractions of the mixed economy: as the rich are forced or allowed to take the inefficient — but for them, affordable — exit route from government provision, a retrenched welfare state will offer only inferior provision, or none at all, to those too poor to exit. This problem is particularly acute in health or disability insurance: as the rich and healthy exit the risk pool, coverage becomes ever more expensive, driving even more people out until it (p. ). Similar distributive effects occur in a wider class of market failures, too. For example, a failed (p. ) of global climate or local public safety will not only be wastefully inefficient, but it will also hit hardest the poorest regions and people, who can least afford substitutes, such as building a levee or hiring private protection. Inequitable is Inefficient. On the other hand, the abstractions of the mixed economy also imply that sometimes, slicing the pie unequally, will also make it smaller: “(…) there is a very good argument that equality of opportunities and life chances is becoming sine qua non for efficiency as well” (Esping-Andersen et al. 2002 ix). For example, people may not be able to align the when collateral is not widely available (p. ), and overly taxing low and middle (labor) incomes may contribute to (p. ). To answer the first-order question of welfare state design, as I have here tried to do, we need the abstractions of the mixed economy to know what is normatively desirable in a scarce world, filled with (at least some) homines oeconomici. 5.10.0.1 Higher Equilibria. The first-order conflict about the best, possible welfare state is about the trade-offs, contradictions and uncertainties of the mixed economy explained in the above. My aim here was not to resolve that conflict: we may not know for sure, let alone agree on what the best welfare state looks like (even though I offered some well-informed hunches). A mixed economy can — in principle and for my purposes here — make arbitrary trade-offs between equity and efficiency, presence and future, or any of the other dimensions of human material need. But given any preference set for these (sometimes!) competing goals, there are still more and less efficient institutional configurations for the mixed economy. More efficient, in this broadest sense, means that these configurations achieve more on all goals. These preferable mixed economies will still trade off preferred goals for less preferred goals, but the trade-off will be less harsh. For example, a nit may achieve the same, if not more equity than a minimum wage, but at much lower cost in growth, or unemployment. Of course, you still have to break eggs for the proverbial omelette, just fewer of them. A Production Possibilities Frontier of Equity and Efficiency in Different Tax Regimes The competing goals of human material need, in other words, are not always in a pure and immutable zero-sum relation, where, for example, one increment more in equity means one less in efficiency, one more in present consumption one less in future consumption. Depending on the institutional design, these conversion rates will differ: sometimes, for example, one increment in equity will cost only a half-increment in efficiency. Alternative mixed economies, more often than not, are a positive or — equivalently — negative sum proposition. Just entering the preferences between purposely competing goals does not yield a single configured mixed economy, but many different blends of command and exchange production and distribution. Again, we may not know or agree what the globally optimal configuration is, given our preferences, but checking and of a mixed economy, we can know better from worse configurations, or local optima. A better welfare state is that mixed economy which elegantly combines and components to , , , and . That is, a welfare state that offers one of the higher possible trade-offs of growth, individual security, equality, saving and convergence. It does so without effectively borrowing from the future through , or hidden, but . This view of the welfare state differs markedly from other perspectives: Full Employment or Growth. A popular variant on the purposive trade-off between equity and efficiency, is that between the policy goals of full employment or growth, sometimes supported by “demand-” or “supply-side” economics. Offe (2003, 2003, 453) finds a similar controversy about how to best keep up the full employment “roof” over the metaphorical Keynesian welfare state house, protecting the lower floors: market liberals (or supply-siders) want to deregulate so that growth leads to more employment and social democrats (and, sometimes, demand-siders) want to sustain welfare state protection so that more employment stimulates growth. These contenders both once had a (somewhat overstated) point, but as market failures grew and monetary policy improved, they are now both increasingly wrong. Market liberals are wrong because not all deregulation, or any amount of it, will stimulate growth, and conversely, not all redistribution or other intervention will depress growth. The abstractions of the mixed economy suggest that sometimes, command production is more (p. ). Social democrats, or more accurately, demand-siders are wrong because, of course, in the long run, only supply determines prosperity, and the depressed aggregate demand they always seem to suspect is clearly defined as a monetary phenomenon and unlikely to persist for very long. If it occurs, monetary expansion and fiscal stimulus should smooth it out, but that does not make for a roof. Social democrats are also wrong to believe that all redistribution and regulation is cost-free: there are dwls. The indicators of activity that both market liberals and social democrats usually obsess about — gdp and full employment respectively — are both emphatically not related to greater (p. ). And so, they both sometimes fall for (p. ) and ignore (p. ). So how can we rescue full employment? By not fighting this last war, for at least two reasons. Pushing macroeconomic policy to full employment risks overheating the economy and building up inflationary pressure. More fundamentally, full employment no longer is (if it ever was) a necessary, let alone sufficient factor for anything we might consider socially desirable outcomes for a welfare state. Full employment in itself cannot, as the house metaphor suggests, provide any shelter for depleted , lemon-market , let alone address intertemporal failures or bubbles, to name just a few. More dearly to social democrats, full employment will also be increasingly unable to — as the proponents of power resources had hoped — level the playing field between capital and labor, simply because some of the major inequities no longer are between capital and labor, but also within labor incomes as (p. ). Even if the “reserve army of the unemployed” is fully activated, the resulting upward pressures on low (or all) wages will be no match for the governing dynamics of inequality enveloping the postindustrial economy (such as Baumol’s cost disease). Full employment, and (properly defined) growth are still desirable — because they are an efficient outcome — but neither serves as a powerful instrument to reduce poverty or economic insecurity: “Promoting labour market participation is no substitute for income redistribution and the fight against poverty: more work does not necessarily mean less poverty” (Esping-Andersen et al. 2002 ix). Offe (2003)’s metaphorical house of the Keynesian welfare state, in other words, does not need a few new shingles, but an altogether new roof. That new roof is the ability of the mixed economy to redistribute, efficiently and progressively as the democratic sovereign wishes. With that ability, welfare states can subsidize any (however small) minimal labor market income to any desired minimum standard of living (for example, through a nit), and can provide any level of social protection desired without counterproductively burdening low incomes (for example, by paying social insurance out of general tax revenue). Progressive redistribution could, if desired, dampen or counteract any existing income dynamic, including markets. Offe (2003) may be right that the welfare state did not “have much to do with ‘equality of outcomes’”, but that does not make it tenable or desirable today (2003, 450). If you are serious about even just “security and protection of workers, not equality” (ibid.) you have to care about inequality and redistribution, at least a little. Without it, you will not have the resources to guarantee even such minimal, unequal outcomes for workers. I further develop this argument in my critique of the literature (p. ). Decommodification still serves well to distinguish different welfare states (Esping-Andersen 1990), but it, too, is no strategy for a possible and desirable mixed economy. Of course, welfare states may still “decommodify” (or subsidize) health care, education and replace (or insure) the incomes of the sick, disabled, old and unemployed, but such programs may be better described in the language of the mixed economy: as specific market interventions and redistribution. The difference is not merely semantic, but at least three substantial misunderstandings easily follow from this choice of terminology: Decommodification suggests — misleadingly — that welfare states could take some aspects of life, and some people off the market. But, because command and exchange modes of production and distribution always interact, that cannot be done. For example, decommodification of disability income is easily misconstrued to mean that disability would no longer be affected by the market, and that markets would no longer be affected by disability. That is not so. When welfare states support the disabled, they not only exempt them — as intended — from earning a market income, but they may also make it cheaper, and therefore more likely for labor markets to produce, say, burn-out, depression or back pain. Thinking in the abstractions of the mixed economy helps us to avoid such pitfalls. In this case, we know that insurance of risks is prone to moral hazard, and that (Pigouvian) co-payments can save the commons of a prudent risk-pool. We can, if desired, slap a Pigouvian tax on risky or strenuous employment and activities, to make sure it is more costly, and is avoided. Decommodification easily morphs from description to prescription, as for example, when income replacement becomes a measure for welfare state re- or entrenchment. Caring about outcomes, there is nothing inherently desirable or essential about decommodification, for four reasons: To decommodify someone out of the market also means to exit someone out of the central institution, that aside from providing the substituted material sustenance, mediates much human cooperation, generates self-worth to many and allows people to shape the world around them, however marginally. Instead, we should “enable all citizens to participate in the mainstream of social and economic life” (Esping-Andersen et al. 2002 ix). Complete decommodification is a last resort response to hardship. Mixed economies have several alternative, and less intrusive, approaches to, for example long-term unemployment. Instead of generous, universal and unconditional — and therefore decommodifying — benefits, a mixed economy may subsidize low market wages with a continuous and regressive nit. The availability of a specific tool, may not much matter to the desirability of a welfare regimes: social outcomes do. Decommodification, conceptually, if not always in reality, is ignorant of potential welfare losses, as some person is provided, or some activity done under command instead of exchange — without a respective, justifying market dysfunction. Thinking instead, in the abstractions of the mixed economy, always ties a market intervention to a particular market failure or broader dysfunction, and considers the potential welfare losses. Decommodification is both radical as a prescribed tool (exit the market) and strictly limited in its reach (only a set of included people or activities). To be sure, sometimes a complete move to (entitled) command provision and distribution may be necessary or desirable. Likewise, a democratic sovereign may choose to only care a lot about some market outcomes — and decommodify them —, and not care about others at all. But there is no reason that all welfare states should do so, let alone that we carve this specific vision of a social compact into our conceptual toolbox. 5.10.0.2 It follows: for a better welfare state to strike any such optimal balance, even given arbitrary preferences, it needs an intact set of (p. ), (p. ) and (p. ) of a mixed economy. Of these, tax is the elephant in the room: it is by far the most versatile, precise and powerful tool of command production and distribution in the mixed economy. None of this is a merely abstract or academic concern. Much lies this balance of command and exchange: everything we materially value, and with that, a great deal of the life chances of us all depends on an intact mixed economy. This is also not a revolutionary project. It does not ask for a new man, but it accepts, and timidly merely reforms homo economicus, the civilized version of our selfish demons. It does not ask for a new institution, it carefully compromises existing ways of exchange and command. The mixed economy, by any historical standard, is not a radical proposition. This much, I hope, is widely agreeable. “Taxes are the price we pay for a civilized society.” — Oliver Wendell Holmes, Jr. (1904), Washington, DC Macroeconomic Regimes and Political Priorities Forms of Rule and Welfare State Regimes Trilemma of the Service Economy Welfare Provision and Political Ideologies 5.11 Tax Taxation in the eu, for the most part, remains an exclusive competence for ms. Under the acquis, only indirect taxes (VAT) are harmonized — at ineffective, minimal levels — with very limited cooperation in other fields (for example, European Commission (2009), Bratton and McCahery (2001)). Per its treaties, the eu can harmonise only indirect, always proportional taxes — such as vat — and only by unanimous decision of the Council on proposals by the Commission (Article 113, Treaty of Lisbon, 2009 / Article 93, Treaty Establishing the European Economic Community, 1957). Union members do not even fully cooperate in collecting existing, national taxes: instead of full reporting of all incomes, some incomes (for example, dividends) and some countries (for example, Luxembourg) are exempted and instead levy a proportional, much lower withholding tax. It is, in short, no exaggeration to say that the eu has no fiscal institutions or even coordination to speak of. In the eu, there is no match between the scope of economic activity — the union-wide common market — and the scope of taxation. With no union-level taxation, what does this mismatch do to ms-level taxes? As the abstractions of the mixed economy suggest, tax competition results. For example, (Genschel, Kemmerling, and Seils 2009) find that tax competition is different and greater within the eu than outside of it, and that it accelerates with time and enlargement. eu tax competition is a pd, where states (strictly) dominantly prefer low taxes over high taxes and (Nash) equilibriate in suboptimal, mutual low taxation () on ). If, and to the extent that such a race-to-the-bottom is at play in the eu, it will affect both levels and schedules of national taxation. 5.11.0.1 Levels. Straightforwardly, ms will be strictly limited in the overall level of taxation they can sustain. Governments will no longer be free to set a level of taxation, or, equivalently, determine the command-exchange components of the mixed economy. In some cases, tax levels may even fall, as has been shown for cit rates (Piatkowski and Jarmuzek 2008). 5.11.0.2 Base. Moreover, and more importantly, competition will also alter the base composition of taxation. To avoid large dwls, as they should, governments will turn to bases that are relatively price inelastic, that is, economic transactions that cannot be altered to escape taxation. eu integration opens up a lot of new escape routes, especially for newly mobile capital, and, to a lesser extent, high-skilled labor: they can relocate their economic activity to wherever the tax burden will be lowest. This causes welfare-depressing distortions in the high-tax economy: rather than face a now voluntary tax, these pareto-optimizing exchanges will not be made at all, and instead happen elsewhere. For example, a rich entrepreneur otherwise willing to open a new factory in high income-tax Germany, may, faced with the new alternative of building the same facility in a low-tax location, forego his original plan. Germany unambiguously looses welfare, both because the investment is not made, and also because it does not even generate any fiscal revenue. Faced with these dynamics, governments will, again rightly so, shift their taxation to bases that are less prone to dwls, or equivalently, bases that are relatively less mobile. Relatively less mobile bases in the eu will be consumption and labor incomes, because consumers and workers cannot easily do their shopping and working in another country. Other — partly dysfunctional — taxes traditionally used to raise revenue for mixed economies will be rolled back or falter altogether. This applies especially to — anyway defunct — national cits that large corporations can often evade easily, in part because nailing down the locale of any particular increment of income of a multinational firm will always be conceptually difficult. For example, the German holding of Deutsche Bank AG can easily reassign a particular income stream to a Luxembourg-based subsidiary, arguing that a crucial business process occurred there. Tax administrations will always, and necessarily, be unable to argue where any particular value was created (Ganghof (2006), Ganghof and Genschel (n.d.), Ganghof and Genschel (2007, 5)). Similarly, higher brackets of progressive pit will also cause large dwls or, more likely and wisely, disappear, as high-income individuals change residence or citizenship, offshore their income-generation to other countries, or at least shelter it in foreign corporations no longer affected by high, backstop cits. For example, a rich German entrepreneur can establish a new holding in Ireland to buy up his German-based firm, and have it retain most if not all of the earnings, effectively escaping german income taxation. If and to the extent that Pigouvian taxes, or even fees fall on mobile bases, these will also either cause excessive market distortions, or, more likely, disappear. For example, a German steel producer may, (hypothetically!) faced with the German ecotax, relocate to Poland, avoiding the higher energy price, without, as the Pigouvian tax intended, raising the price of steel. Overall energy intensity will remain the same, steel production will simply fall below equilibrium levels in Germany. 5.11.0.3 Schedule. Crucially, by shifting the base, eu ms will also alter the schedule of their tax regimes. By relying more on taxing labor incomes, schedules will become more regressive: most large incomes in developed capitalist economies are not labor, but capital incomes. By relying more on (pre-paid) consumption and other indirect taxes, schedules will become regressive or — at best — proportional: even if rich people, just as others, eventually spend all their income, they will only pay the same percentage in vat or similar taxes. If union member governments wish to avoid, as they should, excessive dwls they will have to sacrifice progressivity in tax. The already impaired, but lone vestige of progression, the pit together with its ugly, but necessary backstop,119 the cit will either disappear altogether, or, largely equivalent, depress their schedules and degenerate into effective labor income taxes, with, at best, some residual but proportional taxation of capital. 5.12 Dysfunctions What kind of an economic reality results from this open, but heterogeneous eu, with unbounded trade, mis-configured currency union and rampant tax competition? It is, and must be, a deeply dysfunctional design, boxing the European household in unattractive policy dilemmas, wasting its communal resources, ever building new imbalances, harboring new crises, and, ultimately, fracture the social contract. 5.12.1 Underfunding Straightforwardly, the strictly limited revenues of eu ms confine them to structural underfunding, or at least, constrain the command-exchange that mixed economies are otherwise free to make (p. ). By subjecting taxes to competition, any increment in more command production and distribution must be bought at an increasing price in lost economic activity, or dwl. In this scenario, governments rebalance their Haig-Simons identities — as they always must — by cutting public consumption or dissaving out of their wealth. This can take many, but entirely equivalent forms. For example, governments can save on public goods, such as road maintenance, or it can reduce transfer payments, such as welfare benefits. It can also dig into its savings, and take on new debt, or, let infrastructure fall into disrepair. Here, too, government is faced with unattractive choices: to either cut public spending to suboptimal levels, to go into debt or to otherwise dissave. 5.12.2 Unemployment The neoliberal agenda promised that if states cut their spending, at least their economies would greater economic growth. Under a dysfunctional mixed economy, that is not necessarily so. In the eu, mixed economies cannot have the cake and eat it, they cannot even do one of the two. Instead, its welfare states are faced with a twin crisis that is mutually reinforcing: one of structural unemployment, and one of structural underfunding, as illustrated in (p. ). Structural underfunding, aside from causing (p. ), in the long run may also diminish the kind of public and common goods that drive future economic growth, such as basic research or infrastructure, and especially, education. Over the long haul, structurally underfunded states will ill-equip workers for a global marketplace, and leave them with comparatively poor labor productivities. In addition, structurally underfunded governments are increasingly unable to transfer low- and middle-income workers, or, at least, exempt them from taxation. In particular, the greater tax burden on immobile labor, and the constrained progressivity of tax under competition will make it even harder for workers to make ends meet at any given market income. They have to pay more — not less — to the state, or, misleadingly named “social insurance” and keep even less for their personal consumption. As a result, at least some workers will be relatively unproductive and face high taxes on their already low or middle market incomes. If and to the extent that eu welfare states maintain some minimum socially acceptable living standard, either through a minimum wage, or, equivalently, welfare transfers, these low and some middle income earners will find it increasingly difficult to earn enough on the market to meet this standard. Any — already diminished — income will be further depressed by a tax wedge driven between the gross and net disposable incomes. Both in a minimum wage, and a welfare transfer regime, those workers with productivities too low to make the minimum income on the market will exit the market, and collect welfare instead — not out of laziness, but out of necessity. The ensuing structural unemployment, in turn, reinforces the structural underfunding of the mixed economy government. First, it creates greater needs for transfers, putting further strain on public transfers. Secondly, it also depresses growth, and in the long run, diverges the economy from its long-term growth path, as segments of the labor force lie needlessly idle. Alternatively, of course, governments can lower effective price floors by cutting welfare benefits, minimum wages or by raising work requirements. By lowering minimally acceptable social standards — frequently euphemised as “structural realignments” or “labor market flexibility” — states will have to abandon central welfare tenets and create and accept, once again, widespread working poverty. That is, eu welfare states can brake the vicious cycle of underfunding and unemployment if they cease to be welfare states, a configuration that (Streeck and Mertens 2010) have aptly called a “permanent austerity regime”. Governments of dysfunctional mixed economies, here, as always, are faced only with equally unattractive options: to either save social standards at the price of structural unemployment and depressed growth, or to abandon them and risk widespread working poverty. This dual crises, and the uneasy choices it forces, will only be exacerbated in a modern and open economy. Modern economies already produce highly unequal returns, as winners take all and, equivalently, Baumols cost disease looms. A modern economy will, by its very structure tend to produce people whose productivities are much lower than the overall productivity of their host countries. Trade, migration and capital mobility add even more pressure. As countries specialize even more according to their factor endowments (think: Romanian Nokia, German Management Consulting), remaining, relatively scarce factors (think: unskilled laborer in Germany) may find their market wages fall even further below the respective socially acceptable minimum income. Especially rich states may then be forced to redistribute income to these individuals, but find themselves unable to raise the necessary revenues (progressively) without further reducing their competitiveness. 5.12.3 Inequality Lacking any union-level fiscal institutions and marred by tax competition between the ms, the eu mixed economy lacks effective tools to redistribute market outcomes. Both within and/or between member states, rampant inequality will remain unchecked, or even further widen. At home, the mixed economy has lost its ability to dampen (possibly accelerating) winner-take-all dynamics, and to compensate the losers from trade and economic transformation.120 The great U-turn back towards more inequality, in Europe as elsewhere in the oecd is well under way (Alderson and Nielsen 2002). As tax competition both erodes the base and depresses the progressivity of taxation, market allocations, increasingly, are final. Moreover, structural underfunding and associated public squalor will also hit hardest the lower and middle income earners, further widening the divide in living standards. Rich people can afford to exit from public provision, for example by paying doctors out of pocket, by sending their children to private schools or even walling their gardens and gating their communities. Lower and middle income earners have no such exit option, but are stuck with decrepit public provision. Between ms, too, inequality will remain unchecked, as member and union level governments have no instruments to alter distributive dynamics of trade, that may — or may not — lead to fast convergence of productivities, and related, living standards. What is worse, the poorer mixed economies are especially constrained: at low productivities, they can least afford to burden mobile capital, and other mobile, high-earning factors, such as professionals with any, let alone progressive taxation. With open borders, but without coordinated tax or union-level transfers, these poorer ms currently can only take the hard, unmitigated route to economic convergence: they tax mostly (low-productivity) labor, proportionally if not regressively, and at low overall public spending levels (for example, Dauderstädt 2008, 88:267). By contrast, in the higher-productivity, rich ms, corporatist arrangements, strong trade unions and substantial, if increasingly dysfunctional welfare regimes can still eek out pockets with sometimes generous welfare provision: in always capital-intensive, often high-value add and sometimes oligopolistic — not commodity —production, these economies can, at least in some sectors, afford welfare. For example, a Bavarian specialist engine builder with high capital and relatively low labor inputs and maybe a handful competitors on the world market, may, faced with strong unions, accept above-equilibrium, possibly efficiency wages. Not so in Romanian manufacturing: Producing low-margin commodities, with little capital but hundreds of competitors and easy access, firms have to compete tooth-and-nail on labor costs. And so, ms may not only fail to converge as quickly, or as closely as they could, or hoped to, but the very conditions for economic development will diverge widely. The rich, high-productivity ms can still, if inefficiently and incompletely, dampen and distribute the pain of whichever economic shock hits or transformation sets in. In the poor, low-productivity East and South, it will be bare-bones laisséz-faire capitalisms.121 In the eu, Kuznet’s and Keynes’ grand hopes, that welfare would — and should — always follow growth, are dashed (as cited in (Galbraith 2002, 22). If they have any choice at all, it is a very unattractive one for the governments of the union: they can either stay in the common market and reap the gains from trade and abandon all or some welfare, or they can exit the union, stall economic integration, save their welfare regimes and retreat to autarky and recession. 5.12.4 Imbalances and Crises 50 Ways to Leave Your Lender The problem is all inside your head, she said to me, You can’t pay back 200 percent of GDP, You have to negotiate, if you want your country free, There must be 50 ways to leave your lender. You really don’t want the IMF to intrude, Furthermore, they’ll force austerity for the interest that’s accrued, Imagine your middle class, subsisting on cat food, There must be 50 ways to leave your lender. Fifty ways to leave your lender. You just stretch out the loan, Joan, Cut the creditors’ hair, Claire, Or boost GDP, Lee, Just listen to me. Print more money, honey. No need to pay back, Jack! Structure a default, Walt. And get yourself free. — Planet Money / National Public Radio, 2012 Democratic governments, firms and households alike will be under great strain from the underfunding, unemployment and inequality that a dysfunctional mixed economy creates. They may take on any possibility to temporarily relief the pressure they are under, even if it will not solve, or even exacerbate the situation in the long run: here, too, humans and the institutions they man, suffer from time inconsistency. In modern financial capitalism and complex societies, there are some powerful painkillers to numb the effects of a dysfunctional mixed economies. As painkillers go, they treat the symptoms, not the disease, and have serious side effects. And so it is with the macroeconomic temptations in the eu: as powerful drugs, they seemingly let economies transcend their material means, spread euphoria and frenzy. Only their cure is, ultimately, delusional, their treatment addictive. While under the charm of such chimerical boom and prosperity, economies keep building pressures and imbalances, that, one day, will unload in financial shocks and systemic crises, that, if sufficiently large, can disturb or bring down entire markets. After this kind of ecstasy always comes a day of reckoning, with a catastrophic hangover. Just when an economy is in such drug-enduced delusion, and living beyond its long-term growth path is, as always in uncertain markets, hard to tell. National balance of payments accounts provide as in (p. ) an intuitive, if rough-and-dirty first indication. Between economies, too, an identity akin to Haig-Simons and the conservation of matter, holds: for any good or service that leaves the country, there must ultimately be imports of equal value, or, a change in ownership of foreign assets, that is, the promise of future imports of goods and services. Conversely, any import must be matched by exports of equal value or it will be offset in foreign ownership of domestic assets, that is, claims against future domestic production. As all the most important economic abstractions, this one is simple: balance of payments accounts are double-entry bookkeeping, only at the economy level. The components of balance of payments accounts, as the Haig-Simons identity, break down over households, firms and governments. For example, in a fictions German account, households can import olive, or firms can import particle filters as semi-manufactured inputs, or governments can import commuter trains for public transportation (, p. ). In these accounts too, positions of one owner are offset by positions of other owners in the same economy. For example, German household exports of home-made cuckoo clocks can be offset by said firm imports of particle filters in a roundabout way, when clockmakers buy French-particle-equipped German cars, or following some other chain of exchanges. Positions also offset across the equality sign. For example, German household imports of olive oil may be offset by government issues of German bonds to Greek oil producers, with the government channeling the revenue to oil-consuming welfare recipients, or through a myriad of other transfers. A (German) Balance of Payment Account with Examples Balance of Payment accounts are easily misunderstood or oversold, for a four reasons: Trade deficits and surpluses between any pair of countries are frequently reported, but meaningless and entirely unproblematic, just as shoppers need not worry about a trade deficit with the local supermarket. Trade deficits — as consumer debt — are potentially worrying only if they are net of all exchanges with all trading partners. Conversely, balance of payments accounts do not apply only between countries, as is easily assumed, but is, in fact a meaningful and true identity between any group of market participants and the rest of their trading partners, all the way down from nations to households. For example, a trade deficit may also arise between laggard regions, impoverished demographics or even generations, and the rest of an economy, with much the same possible problems. In the short term, even such trade deficits may not be problematic, but, in fact, help to stabilize economies from exogenous shocks. Even in the medium and long run, persistent trade deficits may be ok if and to the extent that the resultant capital inflows can reasonably be expected to currently, or in the future, earn whichever factor income was promised. For example, emerging economies may well experience persistent trade deficits for some time, while machinery is imported to equip the workforce, if and to the extent that the resulting, now capital-deepened production pays off as expected. Still, balance of payments accounts are an immensely enlightening abstraction, without which trading mixed economies cannot be well understood: Trade deficits are not a sufficient, but still a necessary condition for building macroeconomic imbalances. Not every trade deficit will betray an economy living beyond its means, but every economy artificially held above its long-term growth path by said financial drugs will leave a grave trade deficit in its wake. The balance of payments identity shows, as Keynes argued forcefully, if somewhat ineffectively at the Bretton-Woods conference in 1944, that these macroeconomic imbalances know no one culprit. The loaded language notwithstanding, both deficit and surplus economies are, equally, at fault. One parties excessive imports are another parties dumping exports. To get to equilibrium, where imports equal exports, either of the two parties, or both, must change its prices. To pride oneself, as German leaders frequently do, in being an export champion — but not an import champion — is but a mindless return to the folly of beggar-thy-neighbor, and, before that, mercantilism. Financial flows always track flows of tangible goods and services, as well as vice versa. It does not much matter who — households, firms or government — in an economy creates the trade deficit. Only the deficit net of all economic actors in a given region matters. How, then, do we know the acceptable trade deficits, from the unsustainable ones? We look at the offsetting changes in the capital account, and check whether these are intertemporally efficient, or whether they were enabled by failed markets. In the eu, as in any other mixed economy, we must beware of these smoke and mirrors, that only forestall and worsen the inevitable day of reckoning: credit bubbles, asset bubbles, inflationary pressure, and nominally invisible, but real dissavings. Credit Bubbles &amp; Default. Trade deficits can precipitate in capital inflow, as new foreign-held debt. To receive their extra imports, deficit economies issue different forms of IOUs, including government bonds, corporate debt and household credit, sometimes backed by physical collateral, as in a mortgage. If the sum of these debts, is sound, so is the trade deficit. If debts, sour, or were overly optimist to begin with, the trade deficits cannot stand. Consider the two scenarios: The loans are performing as long as, if, and to the extent that whichever projects they financed generate sufficient earnings to pay back interest and principal. For example, if the extra, imported surplus production the IOUs enabled were transformed into a competitive factory that now churns out export merchandise, the loan can be paid be back out of these exports and revenues. In the bop, the initial trade deficit is first offset by the loaned capital inflow, which later flows out again as the loan amortizes, offset by foreign factor payments and exports of the produced merchandise. In effect, the loan has, as efficient credit should, inter-temporally balanced past trade deficits with future trade surpluses and/or foreign payments. It matters little whether, and in which proportion the amortization on the capital account is offset by either foreign payments or equivalent actual exports, and whether the factory’s merchandise is actually for export or domestic consumption. In the balance of all economic transformations and exchanges, successful factories and other projects can always honor their loans without curtailing the living standard of the population. Interest, and maybe even collateral, are paid back out of extra production that would not have otherwise occurred. We need not worry about this kind of trade deficit: because it moves everyone closer to the long-term growth path, is an inter-temporal Pareto, or at least Kaldor-Hicks optimization. The loan goes bad as soon as, if, and to the extent that whichever projects they financed do not generate sufficient earnings to pay back interest and principal. For example, if the factory is not competitive, or — more to the european point — no one needs or can afford the airports, malls and mansions into which the extra imports were coagulated, there are no revenues or exports to pay back the loan. In the extreme, but conceptually similar and now plausible case, the extra imports were not meaningfully coagulated into capital at all, but were simply consumed away at present. Come the day of inevitable reckoning, the deficit economies have two choices: If the loan in question carries effective recourse, the deficit economy has to return the loaned capital in other, material ways. As when a leasing company repossesses a car on which payment the lessee has fallen behind, deficit countries must return the surplus production in some form. For example, the deficit economy may ship back the foreign-financed machinery in the project, or, more likely, return the same amount of surplus production transformed into some other good or service. This is the hard way of rebalancing the bop: the inevitable, promised outflow of capital on the capital account (reflecting net changes in the ownership of, but not generation of, assets) can be balanced only with an often painful trade surplus, because there are no factor incomes to be otherwise offset on the current account, reflecting a nation’s income. Either way, a failed investment enforces a later, and often painful trade surplus to return principal and return. Alternatively, if and to the extent that debtor economies (can) forego recourse and exert sovereignty vis-a-vis creditor economies, they (partially) default on their commitments and simply refuse to return the coagulated surplus production. In that case, creditors are stuck with their claim. By fiat, the original loans become full, or partial transfers from the debtor to the creditor economies. Here as always, the two sides of the bop identity cancel out: the original trade deficit is matched by a later, ex-post, enforced, foreign payment in the form of debt forgiveness, haircut or default. No matter the choice, this kind of trade deficit is never an optimization, but an unavoidable redistribution, either from surplus future to deficit present if and to the extent that debtors pay, or from creditors to debtors, if and to the extent that debtors default. Crucially, it matters very little who in the deficit economy — households, firms or government — initially took on debt. These non-performing loans will redistribute from future to present, or creditor to debtor no matter who signed them. In many cases, government will be forced to act as the lender of last resort and take on, or guarantee all the bad loans, both to counteract adverse selection and, often to save an exposed banking system from systemic crash. Even if and to the extent that government, or, equivalently, future taxpayers, can avoid to take on the bad loans, the redistribution is merely concentrated on whoever remains nominal debtor. Domestic policy can force only some people — ideally those responsible — to repay, but, short of default — another redistribution — it cannot void the need to repay. Here, as always, something akin to economic conservation of matter reigns: when credit bubbles burst, someone will have to pay back the future, either some debtors, all taxpayers, some creditors, or any combination thereof. In addition to these mere inter-temporal distributions, credit bubbles and associated mass defaults or austerity, of course, also waste economic welfare, because of the turmoil they harbor, not to mention the hardship they imply. As the business cycle fluctuates wildly in such crises, the economy diverts from the long-term growth path, leaving resources either depressively idle, or manically scarce. Credit bubbles are a market failure that may plague any economy — not just the eu — but the european, defunct mixed economy is particular prone to them, for at least three reasons: The eu, until at least 2012, exercised most macroprudential oversight and otherwise mostly regulated financial markets at the ms level. Here, even the regulatory arm of the mixed economy was impaired, and regulations might have been arbitraged to suboptimal levels. Monetary policy drives bank lending. The ecb, because it can set only one monetary policy, was unable to react to credit bubbles in individual markets or regions, such as Spain or Greece. Equivalently, if there were in the emu, or ever hoped to meet the its nominal convergence criteria, deficit and credit-crazed ms also could not devalue their currency through monetary interventions. Asset Bubbles &amp; Crashes. Broadly similar, and often concomitant to credit bubbles, asset bubbles can also fuel trade deficits. As some assets in the deficit economy are persistently overvalued, foreign investors buy up these domestic assets, offsetting the trade deficit on the current account with a capital inflow on the capital account. Real estate, stock or some other asset that did not previously exist, or belonged to domestic investors, changes hand to foreign investors, expecting an ex-post unreasonable return. Come the day of reckoning, asset prices plunge, and much the same process sets in as when credit bubbles burst, only in asset bubbles, the default incidence is on the foreign investor, because she will usually, if not always, have taken risk-bearing equity in the asset. Asset bubbles, too, are a redistribution from a future day of reckoning to a manic present, and, in that future, a redistribution from foreign investors to the domestic economy. In addition, asset bubbles also waste welfare: when they burst, they spiral downwards, often cause grave systemic risk and generally divert the economy from the long-term growth path. Asset bubbles, too, are a universally looming market failure, but the eu is particularly vulnerable, again, because of likely regulatory arbitrage and ill-fitting monetary responses to local business cycles. Monetary Expansion &amp; Inflation Overly expansive monetary policy can also enable unsustainable trade deficits. In this scenario, central banks simply inject more fiat money into the economy to offset the current account deficit with a Potemkin inflow of capital. Fiat money, of course, never creates capital, and if, when and to the extent that this bluff is called, inflation ensues: money supply and demand equilibrate at new, higher price levels. This problem is widespread in the emu with asynchronous business cycles, a single interest rate target and no transferred stimulus to speak of. In those regions where a low interest rate pumped too much money into the economy, as now appears to have been the case in Ireland, Spain, Portugal and Greece preceding the 2008ff crisis, loose money silently credit and asset bubbles, and might have already built yet-to appear inflationary expectations. Inflation, here, as always, wastes resources and redistributes arbitrarily. The middle class and older people, with frequently nominal denominated assets (pensions), but real denominated liabilities (rents) will be particularly vulnerable to whichever level of inflation this crisis might, eventually, bring. Inflation, too, as the other temporary diversions from the long-term growth path, redistributes from the future to the present. Even inflation does not spiral to double-digits or more, any additional increment in medium-term inflation and expectations is costly, as disinflating to previous levels is painful and often causes prolonged unemployment. Dissaving &amp; Depletion Trivially, economies can also go into unsustainable trade deficits by real dissaving. Instead of, say, selling shares in domestic companies, the deficit economy can just burn more of strictly limited fossil carbon, diminishing its real, if not its nominal assets. Because such real assets, including an economies infrastructure, demography, environment or CO2e levels as unresolved commons have no defined ownership rights, they do not nominally show up in the capital account of an economy. Whatever this assets are transformed into, however, may well show up as an export in the current account. For example, a deficit economy can dig into its coal and iron ore resources, transform them, and export them as steel, offsetting other imports. Because the dissaving in natural resources is not usually recored, and thus triggers no change in the capital account, the steel export revenue will erroneously be attributed as a domestic income in full, when in truth, much of the revenue comes from dissaved domestic assets, that ought to be recorded on the capital account. Such dissavings — by definition — redistribute from the future to the present. As dissaving most easily and nominally invisible occurs out of vulnerable commons, it also wastes the welfare of some of our most precious, communal resources. If, when and to the extent that they are depleted to sustain trade deficits, we may never or only at great cost be able to restore them. I cannot marshall evidence here to show how each of these dynamics caused the 2008ff Euro, let alone the broader sovereign debt crisis. Nor can anyone, in 2012, reliably predict which of imbalances may yet turn out to be unsustainable, and why. What I do claim here is that whatever the actual imbalances and crises of the embattled emu and eu are, or will be, they will, underneath it all, follow these scripts. Using some economic imagination, these are imbalances and resulting periodic crises we would expect to plague any such internally open, but dysfunctional mixed economy. While this european mixed economy may be of its own kind, the market failures that enable these imbalances and trigger the resulting crises are in no way sui generis. The herding and information externalities that inflate asset and credit bubbles, the systemic chain-reactions that loom on large defaults and the tragic commons depleted by real dissavings are the kind of market failures that plague all real-existing capitalism. As such, they must be meet the appropriate regulatory, fiscal, and — to a lesser extent — monetary responses. Similarly, loose money tempts governments of all market economies, not just — in fact, probably, least of all — in the eu. It, too, must everywhere be curtailed by policy: a constitutionally-enshrined monetary governance, preferably a robustly independent central bank, bound to a well-defined goal. Because as dangerous drugs endemic to capitalism, these problems are not European problems, they also do not require a European solution. Still, the deficient european acquis exacerbates the imbalances and crises looming everywhere, in at least three ways ((echoed by Bordo, Markiewicz, and Jonung 2011, 25)): Without a monetary union or nominal convergence criteria thereto, trading economies can intervene in their exchange rate, or, at least, let their currencies depreciate freely. As adjustment mechanisms, none is as fast as currency devaluation to get out of trade deficits. In an instant, imports become more expensive and exports become cheaper, ideally, until import and exports equilibrate at the free exchange rate. Alternative — and ultimately equivalent — domestic readjustment of (higher) prices and (lower) wages often takes longer, maybe too long to avert a bop crisis. Discretionary exchange rate interventions are difficult to get right, and easily deteriorate into competitive devaluation, or beggar-thy-neighbor by a fancy name. Freely fluctuating exchange rates, in turn, are costly as the second theory of optimal currency areas reminds us, and they, too, might be the result of herding or otherwise failing global currency markets. As drug addiction therapies goes, the methadone of devaluation is, at best, a mixed blessing. And still, it is a prescription, the european economy has to do without, no matter the indication. Within the emu, everyone in the eu who ever wants to join, currencies cannot fluctuate. To readjust, member economies can only hope their wages will not be too downwardly sticky. Creditors and debtors alike will anticipate the systemic risk and spillovers that the monetary union bestows on all its members. They know that other members too, would suffer from defaults or, related, emu-exit, and, therefore, will likely bail them out. With systemic default risk as a union-level commons, but decisions in individuals, firms and, at best, ms-hands, credit everywhere, but particularly in the high-risk economies, will be too loose. The eu, in other, metaphorical words, is not only plagued by powerful and addictive drugs, but dealers and addicts alike can reasonably expect to be saved — as the should — if they overdose. Lastly, and familiarly, the european mixed economy lacks the fiscal means to otherwise rebalance internal demand, that intact mixed economies use to mitigate regional imbalances, including public works, industrial policy, or even straightforward transfers. The imbalances that have built up over the last years of European integration, and the crises in which they now seem to erupt, tell of the market failures of our capitalist economy. But they also betray the underlying dysfunctions and unfairness of an impotent mixed economy, that built these pressures in the first place. To bemoan only the market failure, and to seek to redress it is as naive as it is dishonest. Even worse, to simply wish away the crises, and to blame someone (“the banks”) — anyone (“’the markets”) for our misfortune, is to shoot the messenger, rather then to heed her warning. In drug policy, if you are faced with a rampant substance abuse, you have to follow (Mills 1959), and sociologically re-imagine the saddening observation of an overdosed corpse: you have to ask how, and why, people socially turn to harmful drugs in the first place, and then, if you can, cure this anomie, whatever it may be. If you only wage a war on drugs, they will always win. And so it is with the imbalances and crises facing the eu today: we have to use our economic imagination to explain how, and why, the european economies turned to delusional market failures in the first place, and, if we can, strengthen them to resist any siren call. Our anomie, now, should be clear enough: it is the underfunding, unemployment and inequality left untouched by an impaired command arm, that slowly, but steadily, unravels the social contract of the mixed economy. Faced with such pressures, it is little wonder that individuals, firms, states and the household-writ-large they collectively make up turn to the sirens of delusional growth. Boxed in, as it is, the dysfunctional mixed economy, and especially its poorer constituents, find ways to relieve such economic pressure somewhere, to postpone such austerity to somewhen and disguise such anomie somehow. To now, as many do, deplore only the failing markets,122 to demonize investors or politicians is an act of exorcism. It was we, who made a Faustian bargain with these devils: to let them reign free, if only they could numb the economic pain. They obliged us. But no one, as Doctor Faustus, should be surprised if some day, there is hell to be paid. 5.13 An Old Deal “We can’t start another new deal.” “How about fighting for the old one \\[...\\]?” — The West Wing (Season 5, Episode 5), created by Aaron Sorkin. To insist on an intact mixed economy is not so innovative. The mixed economy is, in fact, a very old deal, prepared by the social reforms of Chancellor Bismarck, forged by Presidents Roosevelt and Truman, institutionalized by Lord Beveridge and, with miraculous success, reactivated in war-torn Germany, by Chancellors Adenauer and Erhardt. If there is such a thing as a European social model, or really, any capitalist social model, it is the mixed economy. To insist on an intact mixed economy is also not radical. The mixed economy, is, at heart, a compromise of exchange and command, of market and state, of individual freedoms and duties, of efficiency and equity. The mixed economy hopes not for an end of history, nor harbors any overhaul of society: it makes amends with capitalism. Tax — the cornerstone of the mixed economy — in particular, is a reformist, never a revolutionary project. Good taxation, especially of consumption, accepts private property as given, even legitimate and desirable, and merely adjusts the sticks and carrots that people reap for their personal enjoyment. Minimizing their dwl, good taxation maximizes the freedom of all market participants to do as they would absent the tax. Today, Europe is reneging on this old deal. Without union-level taxation to speak of but with full factor and goods mobility, tax schedules are compressed and levels lowered. Without fiscal complements, the common currency allows imbalances and lets diverging business cycle fluctuate widely. Even regulation is yet incomplete, such as in labor market legislation, and is arbitraged away by competition. On these institutions rides it all: that we can efficiently produce public good cancer research or preserve our environmental commons, that we can pool the risks of the healthy and the frail, that we can restore some fairness between market winners and losers, that our children will receive at least what we have received, and that our neighbors prosper, too. Tax especially, and together with regulatory and monetary institutions, are the social contract of modern capitalism. European regional integration does not rewrite the social contract, but sins by blithely omitting much of passages on tax, monetary and regulatory policy. Absent them, the european polity is no longer free to choose between command and exchange, but — without explicit popular consent — defaults to ever more market, and ever more present consumption. Our household-writ-large now occupies a greatly constrained coordinate space (), boxed in by chronic underfunding, looming unemployment and rampant inequality, shaken by recurring imbalances and crises. Constrained Coordinate Space of a Dysfunctional Mixed Economy By its dysfunctional design, the european mixed economy yields ever more to markets while the other part of the mixed economy, the state, is on the retreat. Bereft of their old, flexible and capable social contracts, the acquis will, nay, already has — however fortuitously — remade european society in the neoliberal, consumerist image. “Neoliberalism” and “consumerism” are, in this case, not catch-all labels of disaffection, but I choose them with equal anger and care, and they apply precisely. The aquis is: Neoliberal, because, eviscerating the state, it morphs markets from one of several means, to inescapable reality or even ultimate end, neither of which it is, nor should be. Consumerist, because, it cannot set a positive savings rate, and leaves austere members no choice but to loot real savings, and give into the temptations of bubbles. As a result, much of the resources of the communal household will be devoted to near-term consumption.123 Just how angelic the postwar mixed economy really was, I do not know.124 Still, relative to 19th century mass poverty (Marx and Engels 1848), 20th century “mob politics” (Crouch 2004, 158) and the spectre of 21st century neo-laissez-faire, the mixed economy, and the welfare state it has enabled, appears as singular achievements of the modern era. The economic institutions of Postwar Western Europe have brought about unseen prosperity and equity, all in relative peace and freedom. That is an old deal worth fighting for. 5.14 Why Tax Matters to Democracy “But who can say how much is endurable, or in what direction men will seek at last to escape from their misfortunes.” — John Maynard Keynes (1936) 5.15 The Social Contract Crucially, we must, again, understand that economic integration must always beget more economic and political integration, that, production at great economic scale implies solidarity at the same scale. That is the story of economic integration and material progress. Living on a scarce and harsh planet, for which, alone, we are ill-equipped, we escaped the (???) curse of overpopulation and starvation by scaling up our production. That is, to this day, how we remake an arithmetically growing material world, to feed our geometrically growing hunger: we join forces to bend upward the production curve, and wrest from it above-linear returns. To reap such economies of scale, in agriculture (Diamond 1997), in the production of violence (Tilly 1985), or in europe-wide automotive engineering (Krugman 1980), we have to master the feat of cooperation in the face of atomistic incentives. Akin to ever-increasing entropy, the arrow of pre-historic organic life, and history of human life on earth progresses by increasing complexity, specialization, but also, always, mutual dependence. Wright (1994) thus describes the transformation of individual cells into higher organisms: they become richer, more resilient, but they must also sacrifice, and will do so only to the extent that they have successfully merged their genetic instructions (1994 Chapters 7, 8). And so it is with european, or other regional economic integration: opening up commerce to one common market makes everyone richer by the magic of comparative advantage — if not necessarily by the same amount. But, if this association is to remain stable, some other elements of human organization have to follow to that higher level, if they are not to perish and destabilize. In the eu, continent-wide commerce also requires continent-wide taxation, regulation and monetary policy, if the formerly stable and re-productive system of a mixed economy is to remain operative. The genius of the otherwise hyper-federal, and hyper-liberal US Constitution is that it foresees this functional necessity in its commerce clause.125 It stipulates that, precisely as commerce traverses the otherwise autonomous states, the federal polity asserts itself. The German constitution knows a similar norm: “The Federation shall have the right to legislate on matters \\[...\\] if and to the extent that the establishment of equivalent living conditions throughout the federal territory or the maintenance of legal or economic unity renders federal regulation necessary in the national interest.” — Basic Law for the Federal Republic of Germany: Article 72, Paragraph 2 (Bonn, 1949)126 It too, displays the same genial insight into the relationship between economic and political integration: as the commerce clause, it does not compel any particular social or other policy, but, if economic integration has occurred, endows the democratic sovereign to rule at the newly emerged, higher level of societal organization. Optimists, not just of the Panglossian kind, are always inclined to see glasses half-full, and so many argue that any, even imperfect or negative European integration is better than none. That is not so much optimistic, as it is hopelessly naive. The metaphor is flawed. Societies are not just glasses half-full or half-empty, they are dynamic systems, residing within, and reproducing the walls of this glass: half-built glasses, as the eu defunct mixed economies, are not merely waiting to be filled, they can also drain of all remaining water, or topple and fall over. The democratically governed mixed economy is the institutional achievement of the century, and as a social compact, it is fragile. As Offe (2003) points out, “it is much more likely that a European-styled \\[mixed economy\\] capitalism transformed itself into a liberal model” than the other way around, or, appropriating Lech Walesa, “it is easier to make fish soup out of an aquarium than the other way around”, because it — like the mixed economy — depends upon “supportive dispositions of a cognitive as well as moral kind” (2003, 446). A mixed economy, as the democracy that governs it, is “easily lost, but never finally won”, as the first African-American federal judge William H. Hastie ominously warned. We have already lost much of the mixed economy, and we are now risking to harm or loose democracy, too: not just democratic rule of the eu, but democracy in Europe, too (Scharpf 1997b, 19). Over time, the hardship that this arrangement inflicts on people, especially in the poorer, more austere ms (Greece), and when the imbalances and crises unload (Spain, Ireland), will corrode the inputs of popular rule, too, and fray the social contract. Offe (2003) has already seen the grave-diggers of liberal democracy, capitalizing on the resulting popular discontent of such “permanent austerity” (Streeck and Mertens 2010): &quot;Also, a third voice, luckily with much less resonance, is making itself increasingly heard in European politics, a voice which claims that the social security of workers (as well as the protection of citizens from violent crime), on the one hand, and efficiency of production and competitiveness, on the other, can only be reconciled if national borders are sealed to the influx of foreign people, foreign workers, foreign goods, and those praying to ‘foreign’ gods. Since the mid-1990s, integrating Europe has seen the sometimes sudden and spectacular rise to electoral success of figures such as Pia Kjaersgaard (Denmark), Umberto Bossi and Gianfranco Fini (Italy), Pim Fortuyn (Netherlands), with Jean Marie Le Pen (France), Jörg Haider (Austria) and Carl Hagen (Norway) being among the pioneers of this new field of populist political entrepreneurship. Le Pen has described himself in the 2002 French electoral campaign as being a leftist in social affairs, a rightist in economic affairs, and a nationalist for everything else. This formula, which is designed to resolve the tension between liberal market freedom and welfare state status rights by ethno-nationalist, xenophobic, and anti-European appeals, is applied by his rightist populist colleagues as well.&quot; — Claus Offe (2003, 454) The ground of popular resentment will only grow more fertile as the Euro crises goes on. The crippling economic depressions of the south, and the ever-insufficient, multi-billion bail-outs from the north are political dynamite, waiting to be ignited by fringe populists. If the extreme right, or extreme left, and/or nationalists start playing with this fire, the moderate voices of social democracy, liberalism and conservativism will have little to douse the flames, because in fact, both the polities in the north and south are faced with thoroughly unattractive alternatives. In the now often wretched south, policy makers must either accept the conditional straightjacket imposed by the eu and its rich sponsors, or jump the cliff of secession and sovereign default, all but ensuring all-out economic collapse under autarky. In the still largely isolated north, policy makers must either continue to periodically support often (but not always) failing governments in the south to alleviate the gravest imbalances, or splinter the union and forego the years, maybe decades of economic growth that wide integration brought. European democracies thus are also no longer input congruent (Zürn 2000), in addition to the already grave, but homemade democratic deficit of a heavily intergovernmentally-biased institutional setup. Faced with such non-alternatives, the people of Europe are no longer subjected only to decisions in the making of which they had a say. For the past two years, every couple of months, German or Greece executives, legislators and voters are asked to choose between bail-out or break-up, austerity or abyss, always at gunpoint, with the entire european project held hostage (for example, Grexit, followed by Portugal, Spain, Italy, followed by doomsday). If the people of Europe merely get to choose between a rock and a hard place, popular sovereignty becomes a farce. Offe (1998)’s (1998) intuition is, tragically, fully borne out: “\\[...\\] that every interim solutions between the extremes of intact national sovereignty on the hand, and complete european supranationalty of a European Federation will, inevitably, violate both the reference point of welfare state protection and that of democratic legitimacy.” — Claus Offe (1998, 41)127 The political fringes will prosper, as they exploit these violations, both from the extreme left and the extreme right. The extreme left will fan the flames of societal disintegration by pitting liberal freedoms — especially those of property and contract! — against welfare protection and democratic sovereignty. Merely a change in tone, the extreme right will push societal regress by pitting all cosmopolitan integration and solidarity against always national welfare and sovereign rule. This new game is rigged against liberal democracy and the moderate voices (or cross-cutting cleavages, Lipset and Rokkan (1967)) on which it so thoroughly depends. What the glass-half-full-optimists forget is that societal, political and economic integration can reverse course, too. The self-reinforcing dynamics of integration operate in both directions: more integration begets more integration (as Monnet had hoped, according to (Schmitter 1999, 948), but regress also begets more regress. As a matter of fact, the neo-functionalists are right, if one-sided (Tranholm-Mikkelsen 1991 as cited in Bieler (2003, 1)) The course of economic disintegration is quite clear: literally the milli-second that an over-indebted ms is thought to leave the emu, anticipating massive devaluation, all holders of cash in, say, Greek institutions, will take their money out.128 Faced with such an economy-wide bank run, the Greek government will be forced to install capital controls, effectively leaving the common market. Without international finance, the remaining trade, too, will be greatly constrained. Just the credible rumor of Grexit could, thereby, almost in an instant, unravel 21 years of integration since the country joined in 1981. We tread the course of political disintegration if — outflanked by populists — our ms democracies increasingly re-embrace the supposed trade-off between national interest and political unification, and again entertain the nationalist politics that the union was built to overcome, in a scenario of decay similar to that painted by (Beck and Grande 2007, 339ff), or (Schmitter 1999, 947). Writ large on society, this is the disintegration that Offe (1998) presciently feared in (1998): “\\[...\\] one could suspect a downward descent on the ladder that T. H. (Marshall 1950) suggested as a model for european political modernization. The three rungs on this latter are the cumulative expansion of liberal, democratic and social rights. The question is, whether during the course of european integration, we will pass the democratic and welfare state stages on our way down, and, as a result, european citizens will be equipped merely with the rights of a (neo-)liberal market participant.” — Claus Offe (1998, 41)129 References "],
["design.html", "Chapter 6 Research Design 6.1 Open Second-Order Questions 6.2 Hypotheses 6.3 Methods 6.4 Expected Results 6.5 State of the field 6.6 Research Questions 6.7 Hypotheses 6.8 Public Finance and Welfare State Research 6.9 Public Choice and Political Economy 6.10 Conclusion", " Chapter 6 Research Design Between these two At this intersection of (pct) taxation and (deliberative) democracy, two related research questions arise: \\[itm:resolve-better-tax\\] Can democratic rule resolve — and if so, how — any remaining disagreement about a priori desirable and doable taxation? \\[itm:prove-deliberative-democracy\\] Can deliberative democracy address — and if so, how — the vastly complex, macro-level abstractions facing our polities? This dissertation investigates the intersecting set of these two research questions. In one sense, deliberative democracy serves as the method to resolve a priori disagreement about taxation, and — equivalently — to test whether pluralist democracy may be partly to blame for the absence of better taxation. Deliberative democracy is a good method for research question \\[itm:resolve-better-tax\\] on taxation, both because I axiomatize it to maximize democratic legitimacy, and I hypothesize it to reveal some of the dysfunctions of pluralist aggregation plaguing tax choice (see below). Conversely, taxation is a good case to test deliberative democracy in research question \\[itm:prove-deliberative-democracy\\], because it affects everyone, but is shrewd in the kind of macro-level abstractions which deliberative fora have so far avoided130 and which may strain the cognitive ability of deliberators. Good deliberation is: Metaconsensus (or well-structured ordinal preferences) Good deliberation is: Intersubjective Consensus (IC) or Intersubjective Rationality People who share the same beliefs and hold the same values express the same preferences and vice versa. As a result of deliberation, metaconsensus in beliefs, values and preferences will be higher. Factors will be fewer, and loadings higher. As a result of deliberation, intersubjective consensus will be higher. People who share the same beliefs about the economy, and hold the same allocative values will express the same tax preferences and vice versa. This is open also to factors of fundamental disagreements about the role of rationality in democratic decisions (as compared to, say, power, practice). –&gt; 6.1 Open Second-Order Questions These hypotheticals flow, for the most part, from a priori knowledge. They are grounded in reason, but not — as hypotheticals can never be — in experience. I have, as much as possible, supported a posteriori assumptions with robust empirical findings — but I cannot provide experiential evidence for the complete hypotheticals. In welfare and tax, comprehensive a posteriori knowledge about hypotheticals is hard to come by. Inframarginal reforms as those I suggest here strain the limits of economic models, all of which must be grounded in only marginally known behavioral data. For example, we would need to know the price elasticity in the demand for luxury goods, or saving and agree on (as I have shown) irreduceably normative judgments of diminishing returns or an optimal savings rate. Natural experiments are, of course, unavailable and the external validity of any laboratory setting will be strictly limited. There is, in other words, not much else beside a priori knowledge that we might learn about welfare and tax. My reasoning here is hopefully as widely agreeable as any comprehensive account of welfare and tax, but it will surely not go uncontested. Absent empirical clarification, we must then resolve such disagreement in a democratic process. Taxation, in other words, probably cannot be reduced to a first-order question, but it, too reverts to a second-order concern on how to democratically resolve any disagreement we might have about it. In democracy, we have some, if limited a posteriori knowledge about the deliberative hypothetical I here suggest. Deliberative democracy has been successfully implemented mostly in local settings on policies of limited scope (Fung 2003; Hendrickson and Tucker 2005). In their proposal for “Empowered Deliberative Democracy” Fung and Wright (2001, 17) explicitly suggest what underlies much of the current designs: A focus on specific, tangible problems Involvement of ordinary people affected by these problems and officials close to them The deliberative development of solutions to these problems. Few attempts at deliberating regional issues of greater complexity have been made, including a ‘Citizens Assembly’ on electoral reform in British Columbia, Canada (confer Ratner 2008). On the one hand, deliberative theorists as Fung and Wright (2001, 17) have criticized this limited scope of deliberative attempts, focused on the immediate, tangible and “confined to the realm of neighborhood and locale”, as much out of necessity, as out of conviction (Boggs 1997, 759). The associated anti-authoritarian, anti-bureaucratic, anti-functional-differentiation impetus may, as (de Sousa Santos 1998) hopes for participatory budgeting in Porto Allegre, Brazil contribute to a “counterhegemonic globalization” by concentrating on special issues like land rights, urban infrastructure and drinking water in urban settings, but may by the same token remain “Bean n’ Rice works” (de Sousa Santos 1998, 479). Faced, as our polities inevitably are, with macro-level abstractions, such cherishing of “human-scale democracy” above all else may, suspects Boggs (1997), move democracy “in a defensive and insular direction, laying bare a process of conservative retreat beneath a facile rhetoric of grassroots activism” (Boggs 1997, 759). On the other hand, political psychologists as (Rosenberg 2002) have problematized the cognitive demands deliberation poses. Recent research in political psychology suggests, that — contrary to the bounded rationality assumption — imperfect human reasoning may not only stem from remediable cognitive scarcity, but may be developmentally determined. Rosenberg (2002) has suggested a threefold developmental sequence and typology of sequential, linear and systematic reasoning. His empirical accounts suggest that if any, only systematic thinkers will be able to meet the cognitive demands for reasoned arguments, and egalitarian free speech of deliberative democracy. Moreover, this cognitive competence was found not to be domain specific, and while people may regress to lower levels of competence under high loads or appropriate cues, they are unlikely to easily, if ever, achieve higher than developed levels. “Structurally (more and) less developed reasoning adults” make “not only the adequacy of citizens’ reasoning, but also their equality” a problem for deliberative democracy (Rosenberg 2007, 12, emphasis added). Few empirical work has been done to address both these ontological and empirical reservations towards deliberative democracy. We know very little about if and how deliberative fora do in the face of macro-level abstractions and unequally limited cognitive ability of deliberators. 6.2 Hypotheses Both research questions \\[itm:resolve-better-tax\\] and \\[itm:prove-deliberative-democracy\\] can be rolled up in one set of hypotheses: 6.3 Methods I test these hypotheses by subjecting people to a , pioneered by James (Fishkin 2009). This ‘Gold Standard’ of deliberative fora (Mansbridge 2010, 55), combines skillfully moderated small and plenary group discussions of randomly sampled citizens, rigorously vetted, balanced briefings and experts with pre-treatment, post-treatment and control group opinion surveys into a quasi-experimental design (as in , p. ). Method of the Deliberative Poll This design suits me well, because it combines the electoral part of democracy with the talking part (Chambers and Kymlicka 2002, 308) and thereby offers the kind of quantitative data needed to test these hypotheses. Both as a research method and as a second-order hypothetical, it also enjoys high external validity: it delivers a collective choice and has been shown to work in actual policy making. 6.4 Expected Results 6.4.1 Hypotheses \\[itm:knowledge-gain\\].x are informed by prior research on the “Heuristics and Biases in Thinking about Tax” (McCaffery and Baron 2003), supplemented by systematic misunderstandings specific to broad choices between income, consumption and asset taxation. My hypothesized misunderstandings flow from anecdotal experiences I have had with laypersons and political leaders. They also shine through in some of the welfare state research, and contrast with my synthesis of an (, p. ) and (, p. ). 6.4.2 Hypothesis \\[itm:attitude-change\\], if confirmed, suggests that in fact, remaining a priori disagreements about tax can be resolved, and that hypothetical taxes really are desirable. 6.4.3 Hypotheses are related to public choice and opinion research in political science, showing that popular beliefs often violate vnm-consistence and can fall prey to aggregation dysfunctions. Opinions about taxation, in particular, may reveal anomalies at the individual and aggregate level, because tax is highly complex and demands highly structured choices. Past research has shown that deliberation can alleviate preference structuration anomalies (Farrar et al. 2003). A dp on tax choice might replicate those findings and extend them to another policy area. 6.4.4 Hypotheses \\[itm:interaction-effects\\].x inform the political psychology, both of tax and of deliberation. If the above effects do not, in fact, interact with equity beliefs (hypothesis \\[itm:interact-equity\\], tax choice must be affected by something other than allocative preferences. In (p. ), equity preferences are moves along a curve, and shifts or moves of curves reflect preferences orthogonal to equity. If, as hypothesized, the above effects do not interact with equity preferences, deliberation may reveal (or bring about) popular understandings in line with the suggested institutional of (p. ): If given some thought, people agree that they can shift the trade-off curve between, say, equity and efficiency outwards, and they prefer those institutions that do. Hypothesis \\[itm:interact-ses\\] is broadly related to the post-democracy thesis (Crouch 2004), by which late oecd liberal capitalism and democracy not only disenfranchise the social contract, but do so at a socio-economic gradient. The poor (and middle classes) may be too systematically confused to effectively act on their supposed self-interest, reinforcing a mutual crises of political and economic equality. Lastly, hypothesis is informed by the political psychology of deliberation, and the empirical research on unequal cognitive ability (Rosenberg 2002). If, as hypothesized, people with greater cognitive ability will display greater changes along the aforementioned hypotheses, deliberations on tax, too, may have to consider such unequal abilities in their theory and practice. More broadly, a dp on tax choice can inform several fields, including research on public finance and the welfare state, public choice and political economy. So, &quot;who dunnit&quot;? Because this dissertation is, at bottom, positivist, I cannot show what — let alone who — prevented a better tax, or a better democracy. Non-events such as as land or consumption taxation and deliberative democracy are always causally underdetermined, just as history is always overdetermined. For a detective, that is a bit of a let-down, but there is still work to be done. I might not find the perpetrator, but at least, I can absolve wrongly accused democracy, and, during the hearing, spread word of the crime. 6.4.4.1 Specifying the PCT. As is argued in the above, the PCT still requires a lot of grunt work detail. In particular, monetary dynamics of the PCT must be addressed, its formulae and schedule must be specified, an elegant design and administrative process must be drafted and a its implementation must be planned. Specification of the PCT will depend on cardinal quantification of its effects and require extensive economic modeling as well as econometric data. 6.4.4.2 Informing the International Political Economy of Progressive Taxation. If the PCT is indeed superior, cardinally and inductively, this verifiably desirable and doable hypothetical will put new questions to the literature on welfare retrenchment. Given that a better configuration is possible, political science must answer whether denationalization and tax competition are to blame for the present misery. Two formal approaches apply here. 6.4.4.2.1 Multi-Level Game Theory. One is the multi-level game theory of international tax harmonization (Scharpf 1997a). Here, considerable effort should be devoted to understanding the strategic imperatives of governance both at the national and international level to adopt or avoid the PCT. An econometrically informed modeling of the distributive effects within and between countries will be required to gauge this multi-level game131. 6.4.4.2.2 Veto Playing. Closely related, international tax harmonization should be modeled as a veto-player problem (Tsebelis 2002). Given the anarchic nature of the international realm (taking on a realist view), veto playing appears to be a reasonable approximation of international tax harmonization. In particular, a modeled comparison between tax harmonization at the international and European level with their different veto rights may be instructive to estimate the overall significance of the model. 6.4.4.3 Modeling the Introduction of the PCT. Lastly, the chain reaction dynamic hypothesized in the above will need deductive, as well as inductive verification and econometric quantification: just how much costs will the PCT early adopter incur to the non-adopters? And how quickly will these costs materialize? How large would a critical mass of countries have to be to introduce and proliferate the PCT? Which countries would be suitable to ignite the chain reaction? –&gt; Such a difference \\(\\Delta\\) between “raw” and “enlightened” thinking, beliefs (Caplan 2007) and preferences (Fishkin 2009) is worth knowing for several reasons: There may be agreeable, doable but hypothetical taxes, which may (Pareto)-improve over current taxes (Harberger 1974) or may be fairer (Rawls 1971) and more sustainable (Solow 1956), including a (Mill 1848; McCaffery 2002; Frank 2005b; Seidman 1997), a (???; Buiter 1988) and a (Friedman 1962). By comparison, existing taxes appear unnecessarily wasteful, inequitable and unsustainable.132 These suboptimal tax regimes may present democratic sovereigns with needlessly harsh tradeoffs between efficiency, equity, sustainability and other competing ends. The means of an intact mixed economy (Musgrave 1959; Stiglitz 2011) may consequently erode or even force retrenched welfare states into “permanent austerity” (Pierson 2001; Streeck and Mertens 2010) as it becomes harder for them to reconcile planned allocation and market exchange through taxation. Public confusion about taxation may be a contributing factor, or even a sufficient — but not a necessary — condition for the non-existence of these supposedly superior taxes. Progressivity in tax, especially, may hinge upon an enlightened, “new understanding of tax” (McCaffery 2005), as the historic mainstay of redistribution — the pit — withers away in the face of its inherent contradictions (McCaffery and Hines 2010). People may understand tax systematically different from how they would under (more) ideal speech, which may cause them to choose less effectively progressive taxes than they otherwise might, or otherwise inadvertently act even against their material self-interest. Additionally, the difference between raw and enlightened thinking about tax may, in itself, depend on the socio-economic status of citizens. Taken together, such socially structured and structuring differences in understanding tax may contribute to further, or perpetuate existing social inequality, even under nominal political equality. The aggregative and representative institutions of liberal-pluralist democracy may be partly outmatched by the vexing complexity (Merton 1968) and tightly concentrated interest (Olson 1971) of late market economies, such as in taxation, violating both their input and output legitimacy (Scharpf 1997b) as well as basic democratic prescriptions for “enlightened understanding” (Dahl 1989). Empirical political sociology bears out such popular dissatisfaction, not with democracy it self, but with incumbents, enamored by special interest, and inattentive to the public good (???; Norris 2011; Putnam, Pharr, and Dalton 2000). If such a “post-democratic” constellation (Crouch 2004) were to plague the polity in its thinking about tax, it may well affect other policy areas too. 6.5 State of the field This positive aspect of this thesis sits at the intersection of two fields of empirical research: Practical experiments with different deliberative fora on varying policy areas, Experimental and survey research about popular understandings of tax. 6.5.1 Empirical Deliberation 6.5.1.1 Deliberative Experiments Deliberative fora have been tried on a number of policy areas and places, including renewable energy in Texas (USA) (Lehr et al. 2003), participatory budgeting in Porto Allegre (Coelho, Pozzono, and Montoya 2005) (Brasil), nanotechnology (Powell and Lee Kleinman 2008), electoral reform in British Columbia (Canada), policing and education in Chicago (Illinois) (Warren and Pearse 2008), and city planning in Philadelphia (Pennsylvannia) (Sokoloff, Steinberg, and Pyser 2005). They also vary in their institutional design (reviewed in Fung 2003). These include several small-n, multi-day designs, such as Citizen Juries (Smith and Wales 2000) or Planning Cells (Dienel 1999), where a few randomly selected citizens hear expert witness and deliberate amongst themselves for a couple of days, all facilitated by trained, non-expert moderators and prepared by a balanced briefing book to draft a Citizen’s Report or similar consensus recommendation. The otherwise similar Consensus Conferences (Einsiedel and Eastlick 2000) are often (much) longer, feature extended learning periods and are typically held on highly technical issues.133 Deliberation has also been tried in large-n, shorter designs with a more quantitative methodology in dps (Fishkin and Farrar 2005), where hundreds of randomly selected citizens receive issue books, deliberate in small, moderated groups and plenary sessions, hear expert witnesses and — instead of reaching a consensus — cast a secret vote or fill out a questionnaire, after the typically daylong deliberation concludes. Often, these survey results are compared to answers provided by participants before the event, and/or to a control group of non-participants, allowing quasi-experimental within-subject or between-subject statistical inferences.134 There is quite limited experience with long, large-n designs, including — to my knowledge — only the (Warren and Pearse 2008) that spanned several months, and included several hundred participants who were offered stipends and accommodation, participated in extended learning sessions, attended public hearings and ultimately agreed to recommend a new electoral system (a stv) for the province (British Columbia Citizens’ Assembly on Electoral Reform 2004). Across these different designs and purposes, deliberative fora have been shown to improve knowledge of the subject matter in question, to change — but not bifurcate — beliefs (Fishkin 2009), to yield more orderly and orthogonal preferences structures (Farrar et al. 2003) and to boost a sense of political efficacy or mutual trust even among disempowered citizens (Karpowitz, Raphael, and Hammond 2009). These encouraging findings on the capacity of deliberation to strengthen democratic rule notwithstanding, much of deliberative practice may be criticized for often focusing on the immediate, tangible and remaining “confined to the realm of neighborhood and locale” (Fung and Wright 2001, 17). This includes not only the overtly “human-scale” democratic efforts (Boggs 1997, 759) of participatory budgeting (de Sousa Santos 1998), city planning (Sokoloff, Steinberg, and Pyser 2005) or public stewardship of forests (Cheng and Fiero 2005), but also the much-lauded dps, which have, even when they were about clearly large-scale issues such as the eu (Fishkin 2009) or renewable energy use (Lehr et al. 2003) mostly shied away from the very structured choices and remote abstractions which policy must engage. These dps have, for example, demonstrated that more people prefer eu enlargement and more people would be willing to pay a premium for renewables (Fishkin 2009, 157), but did not ask — and probably not deliberate on, either — whether the eu was an oca, and if not, what fiscal complements might have to be implemented (for example, Mundell 1961), or by which process competing renewable energies should be chosen, especially if they depend on economies of scale and network effects (for example, Krugman 1990). Clearly, these are only two haphazardly selected examples of abstractions (oca) and choices (infant industry regulation) relevant to these topics, but equally clearly, someone, somewhere in the democratic process has to make these calls. Some of these more technocratic considerations may be rightly relegated to experts, but for others — including the abstractions of tax — this may not be possible or desirable as discussed in the above. In terms of Landwehr and Holzinger (2010), dps may not be sufficiently “coordinative” to be fully deliberative, and not only because they do not enforce a collectively binding decision (2010, 377). Absent abstractions, “coordinativeness” suffers because the policy options (for example, “faster eu expansion”) in which preferences are expressed (2010, 375) are ill-defined (for example, what kind of european integration?). If such expressed preferences are allowed to detach from ontologically given policy options (for example, currency union with our without fiscal complements), people can effectively “exit” from substantial agreement (Landwehr and Holzinger 2010, 377): “deliberators” can just talk, which may still be desirable, but stretches the concept too thin (Thompson 2008, 502). To show its stripes — or limits (!) — deliberative democracy must be tried on such highly structured choices and abstract issues, too. So far, with the notable exception of some Danish (small-n) Consensus Conferences and the (large-n) ca (on its complexity, Blais, Carty, and Fournier 2008), such applications are lacking in empirical experiments with deliberation. If deliberative practice continues to blank out governing abstractions and structured choices — the very stuff of the modern world and its political rule — it risks moving “in a defensive and insular direction, laying bare a process of conservative retreat beneath a facile rhetoric of grassroots activism” (Boggs 1997, 759).135 6.5.1.2 Academic Reflection These and other deliberative fora have also received ample academic reflection. One strand of positive research treats deliberation as the independent variable, investigating its effects (including Bächtiger (2005) on legislatures, Hibbings and Theiss-Morse (2002) on power differentials or Jackman and Sniderman (2006) on better-grounded judgements). In a similar vein, (Steiner et al. 2005) studied in how far deliberatively reached decisions “correspond to criteria of social justice” (Steiner 2012, 13), by which they seem to mean a utilitarian maxim (Bentham 1789) or the (???) difference principle (Steiner 2012, 95).136 In other work, deliberation is the dependent variable, investigating its causes (including Steiner et al. (2005) on institutional aspects or Steiner (2012, 13) on issue polarization). Similarly, (Landwehr and Holzinger 2010) compare how the discursiveness and coordinativeness of parliamentary debates and citizen conferences on regulating embryonic stem cell research impacts resultant preference change, their proxy for deliberation. Employing a more sophisticated and theory-grounded operationalization than most, Landwehr and Holzinger (2010, 377) analyze speech acts to code for arguing (for example, “to claim”) and bargaining (for example, “to offer”). Lastly, researchers simply seek to operationalize deliberation. For instance, (Stromer-Galley 2007) suggests a coding scheme, including “reasoned opinion expression” when people provide “a definition, a reason for holding the opinion, an example, a story, a statistic, or fact, a hypothetical example, a solution to the problem, further explanation for why the problem is a problem, an analogy, \\[...\\] or any further attempt to say what they mean or why they have taken the position that they have”. These academic reflections all try to abstract away the substantive issues at stake in the respective deliberative encounters. Social scientists, here, as often, are loath to get caught in first-order conflicts over, say, school policy: “\\[...\\] measuring the factuality or quality of reasoning was deemed unnecessary” (Stromer-Galley 2007, 10). This impulse is understandable, even commendable: Social scientists are, by definition, in the business of explaining the second-order social conflict over normative and positive first-order questions (compare Gutmann and Thompson 2004, 12) and must generalize from the particular to build or test social theory. Yet, since deliberation is a discourse ethic (Gutmann and Thompson 2002, 153), it is difficult to analyze without the actual, domain-specific discourse. The operationalizations employed vary widely, ranging from the crudely statistical (Steiner 2012)137 to the quick-and-dirty semantical (Steiner 2012)138 to promisingly speech-act linguistic (Landwehr and Holzinger 2010). The dqi — even in Steenbergen et al. (2003)’s more exacting formulation — and other such operationalizations “can be applied easily and reliably to a wide range of deliberative contexts” (Steenbergen et al. 2003, 22), but that very decontextualization makes it hard to adjudicate whether really, universal validity was discussed, let alone reciprocally acclaimed. I share the uneasyness of “critical theorists and postmodernists” (Steiner 2012, 11) with this measure of quality of deliberation — though I do not think you have to be either to object, nor for the same reasons.139 Rather than how many justifications were given in total, or how often people referred to counterarguments (Steenbergen et al. 2003, 29), I want to know whether these reasons corresponded in substantive terms to those they disagree with. I provide a hypothetical exchange from deliberating taxation to illustrate my point: Foundationalist140 I believe that everyone should be entitled to incomes generated from uncoerced exchange with others. Social Democrat141 Yeah, obviously we are not going to take away all of a person’s income, they should be entitled to it, but maybe we’ll take some of it because we need that for education. That’s not going to hurt them. Georgist142 I agree there may be something inviolable about owning the value added in free exchange with others. But, Foundationalist, do you think that should also extend to owning unimproved land and natural resources? It seems to me that no one has really created these values, and whoever reaps rents from them does so only because of the coercive powers that be. Formally speaking and under dqi coding, both Social Democrat and Georgist refer to Foundationalist’s argument, yet their speech acts are of strikingly different quality. Social Democrat, rather sloppily, reiterates the argument and proceeds to balance it with another good (education) and suggests a compromise. We can see how the dialogue would proceed; Foundationalist — presumably deontologically bound to inviolable property — would refuse to trade off property against opportunity. Talking past one another, Social Democrat and Foundationalist may not have exchanged, let alone agreed on universal validity claims. By contrast, Georgist — maybe provisionally — embraces the foundationalist’s argument and points to an unexpected implication in foundationalist terms. It is much harder to see how their dialogue would proceed; presumably, Foundationalist would not — ex ante — have been open to large-scale land reform or taxation (as Georgism implies), and may not much warm to the idea. Still, she may find it necessary to either clarify her position further, or to concede Georgist’s point, because his may just be the “forcelessly forceful” better argument. Foundationalist and Georgist are grappling with the formulations and implications of what may become a universally acceptable moral claim, and in so doing, have crept a little closer to communicative rationality. Crucially, this kind of Habermasian adjudication cannot be done in the abstract; researchers have to look for ideal speech in substantive terms as in this case, a foundationalist theory of property. By abstracting away the actual arguments from a normative theory vested in their force, these researchers, as Steiner (2012) concedes, “take the position that large parts of the Habermasian version should be relaxed” (Steiner 2012, 150). For Steiner (2012), “the crucial point should be that participants in a political debate acknowledge that others also have reasonable arguments \\[...\\] If this is accomplished, the other side is humanized” (2012, 150). This may well be true and important research, especially for divided societies on which Steiner (2012) here reports, but it stretches the concept of communicative action too thin. To be sure, there may be such a loosely-defined thing as “deliberation” — distinct from “debate”and “discussion” (Landwehr and Holzinger 2010, 384) — and we should study its antecedents and consequences, as much of the above-cited research does. In this vein, deliberation raises positive questions: What “middle-range theories” associate antecedents and consequences (Mutz 2008)? To the extent that this deliberation is a normative project at all, it espouses a consequentialist outlook: deliberation is good, because it leads to “more public-spirited attitudes; more informed citizens; greater understanding of sources of, or rationales behind, public disagreements; a stronger sense of political efficacy; willingness to compromise; greater interest in political participation; and, for some theorists, a binding consensus decision” (Mutz 2008, 524). Habermas prescription for communicative action, again, is different. Communicative action is not a positive statement about causes and effects open to falsification, because it poses a regulative principle (Kant 1781), which “is unachievable in its full state, but remains an ideal to which, all else equal, a practice should be judged as approaching more or less closely” (Mansbridge et al. 2010, 80). Communicative action is also not a consequentialist ethic, but deontological, because “it has the force to legitimate” (Habermas 2008, 147) — or even teleological, because “reaching understanding is the inherent telos of human speech” (Habermas 1984, 287). As such a normative political theory, communicative action cannot be operationalized into a set of treatments. Just because you subtract power, inequality, disrespect (Steenbergen et al. 2003), even “irrationality”, or any other combination of procedural criteria from communication does not necessarily make it ideal speech — even though the inverse may well be true, as these and other criteria may be necessary conditions for ideal speech. Legitimating collective choice through ideal speech is the irreducible, final reason of Habermas (1984)’ political theory whose achievement can only be found in substantive arguments. Still, to thereby “simply define” deliberation as “intrinsically good” does not, as Mutz (2008, 524) suspects, render “empirical research irrelevant”. As Steiner (2012, 13) reminds us, the “level of deliberation” must be “measured and not merely assumed”. By definition, the normative impetus of communicative action cannot be falsified, but we may find that at any given time, any given people under any given circumstances, fail to meet Habermas’ exacting standards (Thompson 2008, 499). Such failures can serve “as indicators of contingent constraints that deserve serious inquiry and \\[...\\] as detectors for the discovery of specific causes for existing lacks of legitimacy” (Habermas (2006, 420, emphasis in original), compare also Thompson (2008, 502)). If we accept this more demanding definition of deliberation as the venue in which our approximating of ideal speech is ascertained, as I here wish to do, our research designs must be focused on argumentative substance. Others have done this, and have produced encouraging insights. (Ratner 2008) interviewed ca members after the fact, and found that most members considered Habermas’ validity claims honored — especially compared to the proper tragedy that was the following referendum campaign in which many ca alumni participated. While the analysis is based on interviews, not process data, it maintains the Habermasian standard and does not spare the reader qualitative data, including snippets recounting substantive disputes over electoral design. (Gutmann and Thompson 2004), too, reporting on deliberations over medical rationing, stick to their (quasi-Habermasian) standard, argumentative reciprocity, and offer examples of how it is met or violated in substantive arguments over health care. Some may object that this approach collapses the procedural theory of democracy into a substantive theory of justice, and that it consequently requires researchers to take sides. This neither need, nor ought to be so. Habermas (1984)‘Communicative Action, much like Rawls (1971)’ contemporary Theory of Justice is neither purely procedural, nor substantive, but elegantly posits the discursive conditions under which substantive claims ought to be made. The contrast between procedural and substantive theory, here, as elsewhere, is misleading: This kind of public philosophy would avoid the dichotomy that has come to dominate contemporary discussions of political theory, which poses a choice between basing politics on a comprehensive conception of the good, on the one hand, or limiting politics to a conception of procedural justice, on the other. We can and should avoid choosing either of these approaches exclusively. — Gutmann and Thompson (2004, 90) Consequently, researchers need not take sides, nor follow (dubiously legitimate) expert rule (Blok 2007; Haas 1992). Emphatically, “alternative conceptions of the common good” (Cohen 1989, 23) are possible and desirable. Researchers must, however, familiarize themselves deeply with the issue to be deliberated, penetrate the discursive fluff and distortions to relentlessly unearth causal and moral disagreements which reasonable people may have over the issue. Researchers can then analyze their rich, qualitative, ideally process data in terms of these expected disagreements, to assess in how far the observed deliberation lives up to the standards of communicative action. Ideally, deliberators will struggle to formulate universal validity claims to express and resolve these very disagreements. In short, researchers need not have a position on the issue, but they must have a stake in the abstractions, disagreements and uncertainties plaguing the issue. Communicative action on taxation, in particular, must be analyzed in substantive terms for these reasons. Tax is fraught with deep-seated causal and moral disagreement. To test whether people in any given deliberation on tax really have exchanged universal validity claims to — potentially — resolve these disagreements, their actual communication must be compared to some ideal standard of expected arguments, tentatively identified by researchers. Pluralist and Deliberative Democracy Pluralist Democracy Deliberative Democracy Knowledge Miracle of Aggregation Wisdom of Crowds Schools as the Operative Metaphor Legitimacy Special Interests (Alternative!) Common Goods Equal Participation Representation Veil of Ignorance Speech Power Communicative Action Preferences Pre-Social Intersubjective Hypothetical Examples of More or Less Valid Claims Concerning the Optimality of Taxation Comprehensibility Truth Truthfulness Rightness desirable-tax, tax-optimality “A deadweight loss occurs if otherwise pareto-improving exchanges do not occur”.“Taxes can sometimes prevent people from trading things on the market, which they otherwise would have exchanged to at least mutual benefit.” “Taxes are anti-growth”. “Under some circumstances, taxes can depress market activity without raising revenue”. “Lower taxes benefit everyone, because growth trickles down.” “If and to the extent that people react to incentives, and that we can really compare their utility, perfect markets make some people better off without hurting others, compared to how things were before”. “No one would get any work done, if all their rewards get taxed away!&quot;”It seems that markets do a good job in solving some problems, and we can, to that extent and in those domains accept that people will also be influenced by threats and rewards doable-tax, tax-incidence “The tax burden is distributed according to the relative price elasticities of sellers and buyers in any given taxed transaction.” “The ultimate burden of a tax may differ from who nominally pays it.,Taxes are almost always levied on some trade, and their ultimate burden is distributed among the traders according to how sensitive they are to price changes.,The less you can react to a price change, the more of a tax you bear.” “With a Financial Transaction Tax (FTT), we can take back the money from those who caused the financial crisis.” “To the extent that a FTT discourages short-term trades, it does not raise any revenue.,To the extent that a FTT raises revenue, behavior does not change.,The burden of a FTT will be complexly determined by relative elasticities.” “Employers already pay half of social insurance”. “Social insurance is a payroll tax and falls entirely on labor, no matter the nominal distribution.,The burdens are shared between employers and employees according to their relative price elasticities.” “Big corporations should pay their fair share.” “Corporations are not moral subjects; they should not be liable for redistributive or general revenue taxes.,However, the rich people who hold a lot of stock in corporations should pay taxes on such income, or wealth.” Hypothetical Examples of More or Less Valid Claims Concerning the Incidence of Taxation Less Valid More Valid Comprehensibility The tax burden is distributed according to the relative price elasticities of sellers and buyers in any given taxed transaction The ultimate burden of a tax may differ from who nominally pays it.,Taxes are almost always levied on some trade, and their ultimate burden is distributed among the traders according to how sensitive they are to price changes.,The less you can react to a price change, the more of a tax you bear. Truth With a Financial Transaction Tax (FTT), we can take back the money from those who caused the financial crisis. To the extent that a FTT discourages short-term trades, it does not raise any revenue.,To the extent that a FTT raises revenue, behavior does not change.,The burden of a FTT will be complexly determined by relative elasticities. Truthfulness Employers already pay half of social insurance Social insurance is a payroll tax and falls entirely on labor, no matter the nominal distribution.,The burdens are shared between employers and employees according to their relative price elasticities. Rightness Big corporations should pay their fair share. Corporations are not moral subjects; they should not be liable for redistributive or general revenue taxes. However, the rich people who hold a lot of stock in corporations should pay taxes on such income, or wealth. 6.5.2 Misunderstanding Taxation Survey and experimental researchers have investigated how people misunderstand taxation and its abstractions. (McCaffery and Baron 2003), tracking the heuristics and biases research program (Kahneman, Slovic, and Tversky 1982) suggest that voters fall short of (???) in their choice of tax: they incorrectly believe that taxation has a trivial, “flypaper” incidence (McCaffery and Baron 2004a), they favor (indirect) taxes not labeled or not visible, they fail to aggregate different taxes — especially the proportional or regressive payroll and vat — confuse progression in absolute and percentage terms and inconsistently prefer bonuses over penalties. Worried, (McCaffery and Baron 2004b) suspect that politicians may exploit these and other misunderstandings and that current tax systems may be suboptimal as a result of voter irrationality. They also suggest that people may overestimate effective progression, and that tax regimes may, as a result, be less redistributive than voters intend and believe them to be (McCaffery and Baron 2004b). In a similar vein, (Sausgruber and Tyran 2011) show that people mistake the nominal incidence of a tax with its effective incidence on the buyers and sellers in a taxed market transaction.143 Under the resulting “tax-shifting bias” (Sausgruber and Tyran 2011), or, equivalently “flypaper theory of taxation” (McCaffery and Baron 2003), participant buyers often choose suboptimally high taxes on sellers, which they end up paying (partly) themselves. Sausgruber and Tyran (2011) furthermore find that participants are less likely to tax-shift if they are given accurate information on tax burdens exogenously and if they have experienced its income-reducing effects in the laboratory marketplace. Troublingly for deliberative democrats, they also find that (albeit minimally defined) initial deliberation does not reduce tax-shifting, but that it “makes initially held opinions more extreme rather than correct” (2011, 164), corroborating concerns for (deliberative) group polarization (Sunstein 1991). These are rigorously researched and disconcerting findings that have not received nearly the attention they deserve from welfare state scholars and other political economists. These and other misunderstandings of tax may constitute partial, possibly sufficient — but not necessary — causes for changing tax regimes and constricted, or retrenched welfare states. These are also — much like the sponsoring heuristics and biases program — no definitive, comprehensive list of misunderstandings, but an evolving, loosely connected set of relatively low-level cognitive effects. More research is needed: how do people (mis)understand some of the broader and more complex issues bearing on taxation, such as economic growth, savings rates or government and market failures? how — if at all — do the suggested low-level cognitive heuristics and biases filter up to the very structured choices of base, schedule and timing of taxation asked of the oecd polities? As these pressing questions are addressed to develop a coherent account of social change from misunderstanding taxation, research designs as those by McCaffery and Baron (2004b) or Sausgruber and Tyran (2011) may well face two kinds of limits: The complexity of higher-order considerations may overwhelm both designs. Laboratory models in behavioral economics as those by Sausgruber and Tyran (2011) have the distinct advantage of directly testing the misunderstood causal effect in question (in this case, tax-lse), and rendering participants with an immediate experience of the effect (here, lower incomes). As the causal effects in question become more complex — for example, liquidity or employment decisions under different taxes — laboratory models face ever harsher tradeoffs between internal and external validity (for a review and dissenting opinion, see Jiménez-Buedo and Miller 2010). Trivially, economy-wide choices — for example, between a pit and pct — are very hard to model in the laboratory; if they were any easier, economic policy would be a non-issue. The within-subject designs of cognitive psychology as those by McCaffery and Baron (2004b) also offer great internal validity when the heuristics and biases are low-level. Participants rate their agreement with, or the truth, of several statements they are presented with. Experimenters vary the context or wording of substantively identical statements to identify contradictions or mental shortcuts within the answers of individual participants. McCaffery and Baron (2004b) provide supposedly “de-biasing” information as within-subject treatments. The problem with higher-level concerns — say, whether to tax income or consumption — is that equivalent de-biasing treatments would eventually need to be an introductory economics course, covering at least the circular flow in the economy, some macroeconomics of aggregate demand and the Haig-Simons identity of income. As the required syllabus grows, it not only becomes impractical for participants with limited time and patience, but it also ignites a combinatorial explosion of necessary treatments to figure out just which (of supposedly many) insights from the de-biasing did the trick. More problematically yet for the deliberative-minded researcher, both designs cannot problematize, let alone resolve, any remaining substantive disagreements over taxation and its economic abstractions. Both behavioral economics and cognitive psychology stipulate that there is a (vnm-)rationality, and merely chronicle whatever human deviations from it they find. Again as with the heuristics and biases research program in general, researchers and deciders must be aware of these deviations from rationality in tax, but these deviations do not add up to a theory, much less an account of social change in taxation or democracy. We know that von Neumann and Morgenstern (1944) say one (internally consistent) thing, and other homo sapiens mostly something else, but we learn little about the conditions of this divergence, let alone ways to reconcile it. Things get even thornier for a heuristics and biases research program of tax, when higher-level concerns are introduced and — as they likely will — turn out to be controversial. McCaffery and Baron (2004b) or Sausgruber and Tyran (2011) posit expert rationality against their participants, and, whenever they differ, find their participants at fault. This works well enough for disentangling percentage and absolute tax burdens, and tolerably for demonstrating a tax-shifting bias144, but leaves the researcher empty-handed when faced with genuine disagreement between experts, or challenges as the legitimacy of expertise. To be sure, there may well be the vnm-rational, human-nature-compatible optimal taxation, that renders all other proposals regrettable misunderstandings — but this hypothesized agreement and its communicative conditions must be tested, not axiomatized. The heuristics and biases research program can inspire a sociological account of misunderstood taxation, but it cannot bring it to its conclusion. When faced with inevitably controversial higher-level issues such as the choice between a pit and a pct, both cognitive psychology and behavioral economics will smack of expertocracy and remain fruitless because they allow no recourse to a second-order theory (Gutmann and Thompson 2004, 125) — such as Habermasian deliberation — to resolve disagreement. 6.6 Research Questions The deliberative experiments on taxation proposed here builds on these two strands of research as much as it probes its limits: Taxation as a Case. A deliberative forum on taxation, including its structured choices and abstractions, adds another — yet rare — experiment on a technocratic and complex policy issue to the empirical record. From this perspective, deliberation is the research topic, and taxation is a case to test the limits and capacities of this prescription for political rule.145 Deliberation as Method. Lastly, deliberating taxation will provide data to better understand possible misunderstandings of tax and, thereby, clarify remaining controversies over it. From this perspective, optimal taxation and popular deviations therefrom are the research topic, and deliberation provides the method to investigate, or even resolve these differences in a normatively attractive manner (Rawls 1971; Habermas 1984). These two research topics all fold into the aforementioned research question: how will different people understand taxation differently, if they participate in a deliberative process closer to the ideals of communicative action than the status quo of representative democracy? 6.7 Hypotheses Both research questions, about 1) the remaining disagreements about tax and 2) the ability of deliberative democracy to cover abstract and structured issues can be rolled up into one set of hypotheses. Fishkin (2009) suggests that deliberation increases knowledge, and changes preferences as a function of such knowledge gain. Similarly, I hypothesize that people will gain in knowledge about taxation, including resolving some systematic misunderstandings of concepts and causal relationships, some variants of which are already documented (McCaffery and Baron 2003; Caplan 2007). I furthermore hypothesize that as a function of these knowledge gains, people will change their preferences on the desired base and schedule of taxation. Additionally, deliberation will increase self-assessed autonomy and competence of participants, and they will find the presented alternatives of base and schedule more meaningful. Learning interventions will further drive knowledge gain and improve self-assessments, but — if it is sufficiently balanced — will not interact with attitude change. \\[itm:think-different\\] If ordinary citizens are given the possibility to deliberate welfare and taxation, they will think differently about it. Specifically, \\[itm:knowledge-gain\\] Knowledge Gain. People will gain in knowledge. Specifically relevant for tax choice, \\[itm:bastard-keynesianism\\] Bastard Keynesianism. People will understand that an economy can have an arbitrary (sub-@Solow1956) savings rate, and that — equivalently — if lowered slowly, aggregate consumption will not depress aggregate demand. \\[itm:real-myopia\\] Real Myopia. People will understand that the future prosperity of an economy is, in part, determined by present net investment, and that — equivalently — real, not nominal (for example, gdp) indicators track the savings rate of an economy. \\[itm:bastard-hayekianism\\] Bastard Hayekianism. People will understand that an economy can adopt an arbitrary government quota (if, possibly, at a cost), and that — equivalently — not all (but some) taxed resources are lost. Taxation is not exclusively or unconditionally a negative-sum proposition. \\[itm:flyper-theory\\] Flypaper Theory. People will understand that ontologically and empirically, only and always natural persons bear the burdens of taxation, and that — equivalently — the flypaper theory of tax incidence is false. This hypothesis is similar to, but not identical to McCaffery and Baron (2003)’s findings of tax aversion, by which people mistakenly prefer taxes not labeled as such and/or indirect taxes and a disaggregation effect, by which people fail to integrate taxes on same bases. \\[itm:ordoliberal-mess\\] Ordoliberal Mess. People will understand that Pigouvian and general-revenue taxes follow opposing logics, and that — equivalently — a Pigouvian tax should not raise revenue. This hypothesis is similar to, but not identical to McCaffery and Baron (2003)’s findings of a disaggregation effect, by which people fail to integrate different taxes even on same bases. \\[itm:false-omnipotence\\] False Omnipotence. People will understand that taxation will often (if not always) cause unintended consequences in markets, or that — equivalently — taxation can cause a negative-sum dwls. Taxation is not exclusively or unconditionally a zero-sum proposition. \\[itm:false-omniscience\\] False Omniscience. Relatedly, people will understand that taxable income and wealth (but not consumption) cannot be measured independent of market-pricing, or that — equivalently — any taxation based on fiat evaluations may have unintended consequences in markets. \\[itm:universal-validity\\] Universal Validity People will find the above abstractions meaningful and accept a universal validity of underlying causal and moral arguments. \\[itm:attitude-change\\] Attitude Change. Partly as a function of the above effects people will prefer a PCT, conditionally supplemented by a WT, LVT and NIT over the present tax regimes of PIT, VAT and Payroll. \\[itm:meta-assessment\\] Meta Assessment \\[itm:autonomy\\] Autonomy. People will consider themselves more effectively autonomous in discussing and choosing basic tax regimes. \\[itm:competence\\] Competence. People will consider themselves more competent in discussing and choosing basic tax regimes. \\[itm:meaningful-choice\\] Meaningful Choice. People will find the choices of tax base and schedule to be more meaningful to express their political autonomy. \\[itm:learning-interventions\\] Learning Interventions. A learning interaction targeted at one of the above-listed knowledge gains will further that knowledge gain. \\[itm:interaction-effects\\] Interaction Effects. The above effects will: \\[itm:interact-equity\\] Not interact with people’s equity beliefs, or — equivalently — people will not change their thinking about tax as a function of their allocative preferences. \\[itm:interact-ses\\] Interact negatively with people’s socio-economic status, or — equivalently — people with greater socio-economic status will change their thinking about tax the least. \\[itm:interventions\\] Intervention-interaction Of the above effects, meta assessments will interact positively with learning interventions. Of the above effects, attitude changes will not interact with learning interventions over and above the effect mediated by knowledge gain. Variable Metaconsensus in Taxation (x3) Values Beliefs Preferences Subject What makes a tax desirable? What makes a tax doable? Which tax do we want? Existing Work axiology, desirable-tax ontology, doable-tax better-tax Example Statements “People should pay taxes on what they earn on their wealth” “A tax can be easily collected right at the source, in the firm” “We should tax the income of corporations.” Example Extracted Factors Efficiency vs. Equity Withholding Taxation Corporate Income Taxation Meta-Consensus High(er) loadings on relatively few(er) factors. People understand that people can disagree on efficiency vs. equity. High(er) loadings on relatively few(er) factors. People understand that one of the choices would be to tax at the source. High(er) loadings on relatively few(er) factors. People understand different taxes, and recognize it in different formulations. Example Extracted Secondary Factor Indirect Taxation (all columns) Intersubjective Rationality People who value wealth taxation, and think withholding taxes are easier to collect will prefer a Corporate Income Tax. (all columns) Delta Before and After Deliberation Tax Misunderstanding: Flypaper Theory (all columns) Variable Metaconsensus in Taxation (x5) Values (specific) Values (General) Beliefs (General) Beliefs (Specific) Preferences Subject What is desirable? What makes a tax desirable? What is doable? What makes a tax doable? Which tax do we want? Existing Work axiology desirable-tax ontology doable-tax better-tax Example Statements “Growth is good” “A good tax should encourage people to save for the future.” “Markets are a reasonable way to organize much of human activity.” “It is more feasible to tax things whose value is known.” “People should be taxed on what they spend.” Example Extracted Factors Consequentialist Ethic Time-Discounted Stable Incomes Pro-Market Liquidity Taxation Progressive Consumption Tax 6.8 Public Finance and Welfare State Research If the above hypotheses (especially \\[itm:attitude-change\\]) are confirmed, welfare research has a lot more to explain. If under a legitimate democratic process, people resolve remaining a priori disagreements to select a different tax than we currently observe, welfare state research must provide a second-order theory of the observed (illegitimate) social choice of suboptimal taxation. I cannot provide, let alone test that second-order theory of tax choice in this dissertation, but — if hypotheses are confirmed — can at least suggest that the democratic forum may be important. Pluralism may not be the only reason why we have no better tax, but it might well be one of several reasons. 6.9 Public Choice and Political Economy Public choice has long specified how individual preferences can be aggregated, what it requires and how it can fail (for an overview, Mueller 2003). Conversely, (classical?) political economy has formulated theories of how (material?) interest and power can corrupt and alter preference aggregation (for an overview, Robbins 1976). Both those fields, too, can benefit from deliberating hypothetical taxes. Public choice, for once, is mostly an a priori and often an insular enterprise, merely chronicling — but not explaining — the inconsistent preferences and structuration anomalies it faces. For example, portfolio theory clearly differs from observed loss (not risk!) averse strategies (Kahneman 2011), but we know relatively little about how culture or institutions mediate this gap. Deliberation is one (possibly attractive!) institution to narrow the gap between a priori public choice and a posteriori policy preferences. The hypotheses tested in this dp on taxation problematize the institutional link between optimal and actual choice. Public choice is also hard to bring to bear on real-life political choices, because it deals in such high abstractions and the very consistency in preferences it demands are often subject to (second order) controversy. It can be difficult to argue how any particular policy may be explained by, or differ from a theory of public choice, as long as there is disagreement over possible alternatives and ontological concepts. This dissertation synthesizes and develops consistent preferences on taxation, and puts them to a deliberative test. If successful, it may provide an insightful, practically relevant case study of policy and help resolve the controversy over preferences. Conversely, (classical) political economy can gain from a dp on taxation, because it, too, facing disagreement over alternatives and material ontologies (Keynes (1936) vs Hayek (1931), as in Wapshott (2011)), too often reverts to second-order theory, sometimes getting lost in an unproductive muddle of ideas and interests, rather than one or the other as independent variable.146 This dissertation develops a — hopefully — agreeable material ontology for the mixed economy (), and synthesizes a desirable and doable alternative which is then subjected to a specific, legitimate second-order process: deliberation. By comparing cognition — or ideas (!) — as a function of this second-order process, and by relating it to socio-economic outcomes and conditions of deliberators, this dissertation also tests one specific theory of the political economy. Based on transparent, indeed quite orthodox assumptions, I hope to show that material interest may, through whichever agency or structure, have found a clever way to corrupt democratic rule without resorting to crude power. If confirmed, my hypothesis suggest that — for whatever reason — people systematically misunderstand tax in a way that favors the rich, and more generally, the benefactors of the status quo. 6.10 Conclusion In Schumpeter’s thundering words, taxation and democracy are the two sides of social contract (, p. ). They, above much else, balance contradicting social integration and individuality in modernity and moderate its key antagonistic institutions: states and markets. Taxation and Democracy Surely, such a broadly defined research agenda risks hubris, but it also promises to formulate and maybe, tentatively answer some important questions for the social sciences. I may well — figuratively — be biting off more than I can chew, and it sometimes shows (for example in ). All I can do is to tentatively mull over a mouthful at a time, and spit out again that which I cannot address. This — forgive the metaphor — rumination takes a lot of pages to write and read, but at least, it is a relatively transparent way of reasoning. Maybe, in the spirit of Umberto Eco’s anti-library (as recounted in Taleb 2007), explicating what I do not know or cannot address in this dissertation will make for better questions.147 Still, to digest anything in full, I must concentrate. Counterintuitively, the often economic or even technical abstractions of taxation and public choice provide a great guide here. This mostly a priori knowledge, too, requires a lot of pages to write and to read, but, akin to enzymes in the digestive tract, it helps me break down complex issues to ever simpler, fewer and clearer questions (as in and ) that can be handily metabolized into a new, and hopefully original argument that I put to the test. Admittedly, too, underlying this research are normative convictions about political and economic opportunity, and even human destiny. Somehow, such a prescriptive impetus has become unacceptable in much of (German) social science on the welfare state. I must indeed avoid “politics-based evidence making” (The Economist, 2012) or any other conflation of what should be, with what is. However, even from a merely positive perspective, and even to readers who may not agree with these convictions, the hypotheticals they raise pose analytically worthy questions: If there is indeed another, superior fiscal configuration on which we could democratically agree — but do not — this absence needs explaining. If anything, finding that which needs a social explanation, must be a worthwhile attempt for the social sciences. It, too, is how social sciences can contribute to human progress, by politicizing the status quo. “Politicization is the realization that established social norms, social practices, and social relations are contingent rather than sacrosanct, that things could also be different and that citizens, individual and collectively, have political agency by means of which alternatives can be explored and implemented. This recognition that things could also be different has always been the igniting spark of the emancipatory-progressive movements, and politicization has always been their key strategy.” — Blühdorn (2007, 313) References "],
["civicon.html", "Chapter 7 The CiviCon Citizen Conference 7.1 Reciprocity 7.2 Remedial Deliberation 7.3 Format 7.4 Implementation", " Chapter 7 The CiviCon Citizen Conference I here propose to investigate a supposed difference in current, and (more) ideal speech thinking about tax by subjecting a group of diverse citizens to a deliberative forum on the topic, which, in one succinct summary (Steenbergen et al. 2003, 25ff), is marked by open participation, including setting the agenda and deciding on procedures ((???), Chambers (1995), Cohen (1989), Habermas (1992, 370ff)) mutual justification of assertions and validity claims (Habermas 1992, 370), orientation on the common good (Rawls 1971), respect for one another, as well as one’s arguments (Gutmann and Thompson (1996), Macedo (1999)), other-directedness, empathy and solidarity, constructive politics of a widely acceptable decision over well-defined policy alternatives, ideally — though not necessarily — by consensus (Cohen 1989, 23), authenticity in sincere, not strategic communication (Habermas 1984, 149) 7.1 Reciprocity Deliberation is no such catalogue of first-order considerations of what would make a good (efficient, fair, sustainable) decision in any given policy field, but instead, a second-order prescription to resolve disagreement over those very the moral and causal arguments (Gutmann and Thompson 2004, 125). Still, and in contrast to pluralism, it places more than just procedural claims, but posits substantive requirements for how agreement must be reached. Succinctly, Gutmann and Thompson (2004) demand that “your fellow citizens must give reasons that are comprehensible to you” (2004 K177). More than merely intelligible, permissible arguments to reach deliberative agreement must raise validity (Habermas 1984) and moral (Rawls 1971) claims universally acceptable to everyone. Logically, it must then be theoretically possible for any given argument to fail that test, and to be impermissible — including the arguments brought forward by experts. In fact, deliberation reserves no special place for nominal experts at all, except if and to the extent that these experts have arrived at, and present their agreement under the standards of deliberation. As noted in the above, any research design that axiomatizes any disagreement between experts (or ex-ante logic) as misunderstandings on the part of the non-experts fails the deliberative standard, no matter the supposedly enlightening de-biasing treatments. The desiderata of taxation and deliberation are seemingly in conflict. Taxation requires people to consider an exogenously given catalogue of abstractions, and deliberation implies that no such set of arguments can be unconditionally accepted. This is a general contradiction of procedural and substantively epistemic formulations of deliberative democracy (Bohman 1998, 402). This impasse has real repercussions for the proposed research design: apparently, it can serve only one master. Either, participants misunderstandings can be revealed by treating them with introductory economics, or they can deliberate any which arguments they themselves find comprehensible. As with so much that deliberative theory is up against this is a false dichotomy.148 Argumentative reciprocity implies that expert knowledge be neither presumed, nor negated, but that these arguments — as all others — be allowed to demonstrate their universal causal and moral validity. Expert- and non-expert (as all other) arguments must not merely be balanced in terms of airtime or affect, but “the considerations offered in favor of, or against, a proposal, candidate or policy \\[must\\] be answered in a substantive way by those who advocate a different position” (Fishkin 2009 K550, emphasis added). Deliberation is, in other words, when non-experts answer expert arguments on their own, expert terms, and vice versa, too. Surely, the relation of this ideal to practice is “aspirational”, too, as all in deliberation (Fishkin 2009 K2679). So far, democratic theorists have failed to show “how to incorporate the need for expertise and technical administration in a deliberative democracy” (Thompson 2008, 515) and “\\[a\\] great deal of work on deliberative theory focuses on conflicting values, religious toleration, identities and so on, and relatively little on conflicting facts” (Moore 2011, 2). Yet, for the deliberative experimenter, this aspiration to argumentative reciprocity is the ultimate hypothesis in need of falsification, including reciprocity with expert arguments. If expert knowledge on taxation is presumed valid, and any dissent chalked up as misunderstandings the aspiration of communicative action could never be falsified. Likewise, if expert knowledge on taxation is not given adequate opportunity to be reciprocally considered on its own terms, communicative action will always be violated. Neither of those formats would provide construct-valid deliberation. Expert knowledge is then “a special case of the general problem of convincing those who are not ‘in the room’ to accept the results of a deliberation in which they did not directly participate” (Moore 2011, 2). This problem can only be resolved by a “democratic conception of expert authority” (Moore 2011, 2), that is, if deliberative standards are extended to expert arguments, too. A good deliberative format brings ordinary citizens into this business: they must be enabled to the farthest extent possible to argue expert as well as non-expert claims on their own terms, including the possibility to reject expert opinion if it is found insufficiently reciprocal in argument. Because bringing ordinary citizens into this business, and equally so, is, as Rosenberg (2002)’s so difficult, deliberations must “be regarded as remedial institutions” (2007, 12) and more, “they must be sites for political education and development” (2007, 13). 7.2 Remedial Deliberation If their democratic rule is to be enlightened, deliberators must also engage a set of abstractions concerning its optimality, justice and sustainability as well as problematize the view of human motivation, utility and rationality implied by these (economic) abstractions. Deliberators must also understand the means and ends of an (ideal) mixed economy within which taxation reconciles allocation by plan and by exchange, including respective government and market failures. Warren (2008 K1513), reporting on the ca, notes that such learning phases are “crucial to its ability to render a decision, and indeed” and that it can be done: “the decision was a learned and sophisticated one”; “Most members transformed themselves from lay citizens with little knowledge of electoral systems into experts over a period of several months (…)” (2008 K1513). Any deliberation that fails to engage such technical but broadly consensual — if not entirely coherent — concerns, must be considered unenlightened. It would ignore some of the causal relationships and moral alternatives that we must assume exist, if and to the extent that we accept the economic ontology of coexisting states and markets. The fora, even though featuring learning phases, must not regress to a (traditional) classroom setting in which the economics teacher knows best. Still, this is the inescapable challenge of deliberation in the modern world, as Moore (2011) points out: “the ideal of citizen equality to deliberate issues that affect them is in tension with the inequalities of knowledge that are inherently governing complex societies”, but “the question is not whether expert authority is part of the deliberative system, but how it is integrated and whether this integration is itself subject to deliberative standards” (Moore 2011, 14, emphasis added). 7.3 Format Consequently, a good format for deliberating tax includes a strong learning component, where participants are introduced to the structured choices and relevant abstractions suggested by experts, including their disagreements, conditions and uncertainties. Deliberators are unlikely to cover these extensive grounds by themselves, or pick them up from (very limited) briefing books and will benefit from well-planned lessons. On the other hand, a good format must also feature extensive small-group discussion, where participants talk amongst themselves, shielded from the authority of expertise. Deliberators may benefit from a trained, non-expert moderator and/or scribe to assist, structure and document their discussion. Most importantly, participants must be encouraged to adjudicate the validity claims of economics, to contrast and possibly relate those to their own, and other claims. As noted earlier, most deliberative formats include — often moderated — small-group discussion, but few add to that a substantive learning component.149 Among those that do are the small-n Danish-developed Consensus Conferences (Grundahl 1995) and especially the large-n, one-off ca in British Columbia, Canada (British Columbia Citizens’ Assembly on Electoral Reform 2004). Consensus Conferences have been held on very technical issues, including nanotechnology (Lee Kleinman et al. 2007) or genetically modified foods, and the Citizen Assembly was tasked to recommend an alternative electoral system for the province. Much like taxation, technology policy, and especially electoral systems design also demand highly structured choices (for example, a seat allocation rule or regulation) and raises a set of abstractions (party systems or risk distributions). In a Consensus Conference, 12-15 self-selected but diverse citizens discuss a technical matter of political relevance for three or more days (!) and issue a report on their discussions (Lee Kleinman et al. 2007). In preparation for the conference, participants read quite extensive background material, which they discuss during the first of several daylong sessions. At later sessions, they (publicly) share their questions with experts and finally draft a report for the sponsoring body and/or the media. Lee Kleinman et al. (2007) report that a positive atmosphere, good organization and skillful facilitation are necessary for a successful Consensus Conference. They also recommend that participants tell their stories, which moderators then weave into themes, to help citizens relate their experiences to expert knowledge (2007, 159). The ca expanded on this model. Over the course of a year, 160 randomly selected British Columbians read and learned about electoral systems, held public hearings, deliberated amongst themselves and, after several intermediate votes, recommended a (quite complicated) stv-variant and drafted a report (British Columbia Citizens’ Assembly on Electoral Reform 2004). The extensive learning phase, spanning six (!) weekends stands out among deliberative experiments. In addition to a university-level textbook on electoral systems, Citizens received interactive lectures accompanied by small discussion groups. An expert panel vetted the learning phase for impartiality and accuracy of information. Participants also developed a set of shared values during the learning phase, both (procedurally) for how they wished to deliberate and (substantively) for a desirable electoral system. 7.4 Implementation The ca provides an ideal blueprint for a deliberative forum on tax; however, limited resources and lack of experience do not permit such a large undertaking for this project. Instead, the project proposed here is a scaled-down hybrid, combining the ambitious learning phases of a Citizen Assembly, with the more intimate, small-n interaction and participant empowerment of Consensus Conferences.150 A small, self-selected sample of 15-24 diverse, ideally randomly-sampled, citizens will attend around 6 days of learning and deliberation spread over several weeks. A provisional schedule is included in (p. ). After introducing the format and framing the issue, deliberators are invited to tell stories about experiences with taxation, following Mansbridge et al. (2010, 67)‘s recommendation: ’Stories can establish credibility, create empathy, and trigger a sense of injustice, all of which contribute directly or indirectly to justification’’ (2010, 67). Deliberators also share opinions and grievances about tax as well as identify relevant questions and issues, which will be taken up in, and structure the later learning phases. During these learning sessions, taking up less than half of the time, participants receive information about the structured choices and relevant abstractions of taxation from the author. These lessons are organized around themes and values identified by deliberators in an initial session, and attempt to relate the subject matter to participant experiences. Learning sessions alternate with extensive small-group deliberations, during which participants reflect on the lessons, relate or contrast it to their own experiences and claims and adjudicate the validity of the presented moral and causal arguments. Small-group deliberations are facilitated by a trained moderator, who has no expertise in taxation or economics. At the end of the process, participants must agree on a tax and write a report outlining their recommendation, which can be released to the public. If resources permit, participants will also hear a panel of expert witnesses to discuss their questions. This will produce (small-n) data, that may not lend itself to a conventional quantitative DP-research design (essentially a within-subjects Likert-type data). A small sample may not be statistically powerful enough to produce significant effects. Instead the same quasi-experimental design could be run with qualitative data, including, open-ended questionnaires and/or semi-structured interviews, or even a record of the deliberations. Such data could be content-analysed (or rather: coded) to distill the (mis)understanding before and after the treatment. Alternative Initiators of Deliberative Formats Self-selected participants Randomly-selected participants Stakeholder-selected participants Initiated by a civic association School-based deliberation - National Issues Forum Chicago PD deliberation (…) Study Circles (…) ———————————— ——————————- —————————————– ————————————- Initiated by an NGO National Issues Forum Deliberative Poll National Issues Forum AmericaSpeaks Citizen Juries (aka Planning Cells) Participatory Budgeting online dialogues (…) (…) ———————————— ——————————- —————————————– ————————————- Initiated by government Town Hall meetings Deliberative Poll Deliberative city planning local councils Citizen Juries participatory budgeting (…) (…) (…) ———————————— ——————————- —————————————– ————————————- Alternative Deliberative Formats Short duration Long duration Large sample size Deliberative Poll Citizen Assembly (BC: Electoral reform) Small sample size Citizen Jury / Planning Cells Consensus Conference (Australia: GMOs, NZ: Biotech) Limited and/or positive knowledge Knowledge-intensive and/or contested Unstructured ex-ante choices National Issues Forum (race in Philadelphia, PA) Deliberative Poll (Texas: Energy) Consensus Conferences (Denmar, Canada, Aus: GMOs) Structured ex-ante choices Deliberative poll (Jacobs community service) Citizen Assembly (BC electoral reform) Planning Cells Consensus Conferences &lt;!-- %\\cite{Johnson1998} %161: %``Would-be political reformers of various persuasions urge deliberation upon us. %Yet in their pleadings such theorists and reformers frequently invoke deliberation in an uncritical manner. %They proceed as though the ways in which deliberation and the effects we can expect of it are not just obvious, but attractively so.&#39;&#39; %\\cite{Azmanova2010} (also \\cite{Fishkin2009}: %529) %49: %thoughtfulness and reflexivity (as per Fishkin) require 1) reasonably accurate information 2) substantive balance 3) diversity, 4) conscientiousness, 5) equal consideration %\\cite{Citizen-2004-aa} %11: %one guy did the learning sessions %1) has a selection phase %2) has a LEARNING phase %also develop &quot;shared values&quot; about the process %3) has a PUBLIC HEARING phase %4) has a DELIBERATION phase %go to church with group, maybe. %\\cite{Dienel-1999-aa} %&quot;To make a Planning Cell project go effectively, one needs a process of more than four days. %We can distinguish five phases: %design, preparation, implementation, compiling the documentation, follow up. %After the joint formulation of the task, two problems have to be solved in the second phase: %The issue has to be translated into a program of 16 work units and the jurors have to be selected and invited. %Building the program is crucial for the Planning Cell preparation. %Each working unit has to have its clearly defined remit. %Problems have to be reduced to comprehensible alternatives. %All this is done by the programmer who is much more important than the moderator&#39;&#39; %\\cite{Fung-2003-ac} %very good summary, get back to this %\\cite{Fishkin2009} %democracies are supposed to fulfill two values; %political equality and deliberation (K84) %``a democracy in which we all had substantive information [and] [\\ldots] substantive opinions would seem to take too many meetings.&#39;&#39; %what&#39;s wrong with unenlightened opinions (all K113ff) %\\begin{enumerate} %\\item they may be volatile (cite other sources %\\item people may be manipulated by foregrounding some information (clean coal vs.\\ dirty coal, forgetting about natural gas -- compare this to tax choice) %\\item misinformation (savings rate) %\\item favor favorable, true arguments over others %\\item manipulation may ``prime&#39;&#39; one aspect of policy. %\\end{enumerate} %K260: %``the hard choice, in other words, is between debilitated but actual opinion, on the one hand, and deliberative but counterfactual opinion, on the other.&#39;&#39; %K369: %table with raw/refined opinion, and different kinds of sampling. %K438: %``The idea is that if a counterfactual situation is morally relevant, why not do a serious social science experiment -- rather than merely engage in informal inference or armchair empiricism -- to determine what the appropriate counterfactual situation might look like? %and if that counterfactual is both discoverable and normatively relevent, why not then let the rest of the world know about it? %Just as John Rawls&#39;s original position can be thought of having a kind of recommending force, the counterfactual representation of public opinion identified by the Deliberative Poll also recommends to the rest of the population some conclusions that they ought to take seriously.&#39;&#39; %K390: %self-selected samples will be very limited in what they can achieve. %K401: %citizen juries use quota samples, consesus conferences use self-selected samples, then with some quota sampling %K550: %notes that positions must not merely be balanced in terms of airtime or affect, but ``whether the considerations offered in favor of, or against, a proposal, candidate or policy are answered in a substantive way by those who advocate a different position.&#39;&#39; %K561: %three categories for such considerations %``the benefits or burdens of a policy or political choice, % the causal arguments about whether those benefits or benefits or burdens will actually result from one choice or another, %and the values by which those benefits and burdens might best be evaluated.&#39;&#39; %K1424: %``the problem is that any microcosmis deliberation taking place in a modern society will be one in which there are significant social and economic inequalities in the conduct of ordinary life in the broader society. %It seems difficult or impossible to `bracket&#39; these inequalities -- for participants to behave `as if&#39; they do not exist. %Indeed the problem goes deeper. %The possibility of doing so is the challenge of the ``autonomy of the political&#39;&#39;, namely, whether or not equality can hold sway in politics in a world in which inequality rules in economic and social relations. %The viability and legitimacy of the liberal-democratic process may turn on the answer.&#39;&#39; %notes that the relation between ideal theory and actual practice is ``aspirational&#39;&#39; \\citep[K2679]{Fishkin2009} %\\cite{GutmannThompson-2004-aa} %loc177: %`your fellow citizens must give reasons that are comprehensible to you&#39;&#39; %K188: %they introduce first and second-order theories, too! %K919, writing about Fish: %``Giving reasons is the chief way of academics to exercise power in democratic politics. %All the talk about deliberation, like deliberation itself, is merely a cover for power politics.&#39;&#39;. %cite the sausgruber tyran stuff. %iris marion young, especially notes that people of lower status may have a hard time getting listened to, or that others may be particularly accustomed to orderly forms of reason-giving arguments that weigh with other participants -- and this may be particulary problematic the more substantive the topic is. %note that %groupthink! %tax allows only very limited choice: %income, consumption or assets; %a couple of schedules, plus some pigouvian taxation. % the CIT, notably, is just a special way to raise the PIT. %Otherwise, only natural persons. %Tax demands these choices. %Also, these choices \\emph{are} as I explain in the below, political, so they must be made legitimately, and we may not be able to simply outsource them to elites. %argue exactly why small sub-issues of tax do not work; %they violate the real choices. %there remain problems: %you can&#39;t just go about this as if it wwere not controversial; %it is controversially maongst experts, but more importantly, controversial whether experts have in fact authority and the right context. %It can&#39;t just be an experiment, or a treatment intervention where ordinary citizens must necessarily become more like experts, and if they are not, then the teaching has failed. %It must be possible for people to disagree with the abstractions they ar epresented with, see the criticism of it. %tax is very technocratic, simply because the instition is like that. %this will have to be qualitative %\\end{enumerate} --&gt; References "],
["q.html", "Chapter 8 Q Methodology", " Chapter 8 Q Methodology &lt;!-- %Mutz (2008) is all about how it is important to make deliberative democracy falsifiable. --&gt; Q methodology and deliberative democracy share epistemological and normative roots in American pragmatism: that meaning is intersubjective, that communication is human action and that we can – and should – reach some mutual understanding on the objective and moral world, even if contingent and preliminary. This study illustrates and explores how this common legacy makes Q methodology and and deliberative democracy are a good fit for political psychology. On the one hand, deliberation posits the regulative ideal under which individuals can freely constitute and express their political subjectivity: a mutual and egalitarian give-and-take of reasons. On the other hand, Q may serve to measure the quality of deliberation, occupying a sweet spot between quantitatively constrained interpretations for researchers and qualitative leeway for citizens’ viewpoints: increased consistency or structuration of Q sorts might be falsifiable meta-standards without substantive (and circular) prejudice as to which one viewpoint would be “deliberative”. The study concludes by suggesting further extensions and applications of Q methodology and deliberative democracy. Factorial Concourse Sampling Scheme Values / Axiology Beliefs / Ontology Preferences / Taxes General Efficiency Individuals vs Groups Corporate Tax “Actions are good if consequences are good”. “People are motivated by incentives”. “Unfettered markets”. Specific What makes a tax desirable? What makes a tax doable? Which tax do we want? “A good tax gets at the yield to capital”. “A doable tax is a withholding tax.” “Corporate Income Tax”. "],
["field.html", "Chapter 9 Field Report 9.1 Recruiting 9.2 Schedule 9.3 Field Data 9.4 Press Conference", " Chapter 9 Field Report Never doubt that a small group of thoughtful, committed citizens can change the world; indeed, it’s the only thing that ever has. — Margaret Mead After filling out the questionnaire, the topic of the deliberation will be introduced by two moderators, both of which are non-experts in the field. The moderators again introduce a simplified version of the above table, and walk participants through the combinations of base and schedule — including presently used pit, vat and payroll — but abstain from causal, or normative statements about these taxes. The table will also be prominently displayed on premise for easy reference by participants. Participants are then tasked to deliberate the basic choices of base and schedule. Deliberation alternates throughout the day between small-group settings of 5–8 persons and larger plenary sessions. Moderators assist the participants and encourage the norms of deliberation — including respect, equal participation and argumentative reciprocity — but do not clarify or comment on substantive questions. Moderators also make sure that no pressure for a collective decision is exerted. Participants are invited to collect questions for a question and answer session held mid-day with a balanced panel of economic experts. The deliberation concludes in a plenary session where participants are invited to reflect on the experience, as well as to identify further questions for deliberation. Participants fill out a questionnaire again, including some of the questions from the initial questionnaire, as well as some additional items concentrating on the deliberative experience, especially the perceived autonomy, equality and competence of participants. are recompensed for their time and thanked by the convener. 9.1 Recruiting 9.2 Schedule A preliminary (very rough) draft schedule Tuesday Wednesday Thursday Friday Saturday Sunday Monday 09:00-09:30 Arrival Learning: Ends of the Mixed economy Learning: Tradeoffs, Smoke/mirrors real dissavings Learning: Optimal Tax Learning: Incidence of Tax/Elasticity Free Time / Church Service Learning: Tax Choice 11:00-11:30 Introduction Small Group: Ends of Mixed Economy Small Group: Tradeoffs, Smoke/mirrirs, dissavings Small-Group: ptimal Tax Small-Group: Incidence Plenary: Preparing Questions Small Group Lunch Break 14:00-15:00 Small Group: Experience and Questions with Tax Learning: Means of the Mixed Economy Plenary: Summary Learning: Just Taxation Learning: Schedule, timing/Neutrality Experts Panel: Q&amp;A Plenary: Write up of Report 16:00-17:30 Plenary: Report on above Small Group: Means of the mixed economy Learning: Overview of existing taxes Small Group: Just taxation Small Group: Schedule, Timing &amp; Neutrality Plenary: Reflection see above. Learning: Distilling guiding questions Dinner Break Farewell Dinner 20:00ff Movie: 12 Angry Men tba. Games Night tba. tba. tba. Departure 9.3 Field Data The first CiviCon Citizen Conference on taxation produced a wealth of data, including: 2x 18x Q-Sorts (before, after) 2x 18x Item Feedback (before, after) 17x Socio-Economic &amp; Demographic data 15x General written feedback 315x Notes, Posters, Illustrations 1601x Photos 100hrs Audio 80hrs Video (1080p) Additionally, process data from the deliberation will be gathered, comprising of the claims, themes and questions raised by participants and summarized by the moderator and/or scribe at the end of each deliberation, as well as the final report on taxation and other documents or notes produced during the deliberation. Deliberations will also be audio-(visually) recorded, although comprehensive transcription and analysis of this data will probably not be possible. This process data and select recordings will then be subjected to a qualitative content analysis, both to verify that participants could make arguments reciprocally comprehensible (and otherwise valid), and to record which of several arguments deliberators accepted as based on universal validity claims. 9.4 Press Conference "],
["subjectivity.html", "Chapter 10 Deliberative Subjectivities 10.1 Administration 10.2 Data Import 10.3 Import 10.4 Participant Feedback 10.5 Missing Data 10.6 Q Method Analysis 10.7 (No) Descriptives 10.8 Correlations 10.9 Factor Extraction 10.10 Loadings 10.11 Factor scores 10.12 Interpretation 10.13 Discussion 10.14 Factor changes", " Chapter 10 Deliberative Subjectivities He who knows only his own side of the case knows little of that. His reasons may be good, and no one may have been able to refute them. But if he is equally unable to refute the reasons on the opposite side, if he does not so much as know what they are, he has no ground for preferring either opinion […]. He must be able to hear them from persons who actually believe them […] he must know them in their most plausible and persuasive form. -–- John Stuart Mill (1859) 10.1 Administration “Factor analysis (…) is concerned with a population of n individuals each of whom has been measured in m tests or other instruments. The (…) correlations for these m variables are subjected to (…) factor analysis. But this technique (…) can also be inverted. We may concern ourselves with a population of N different tests (or other items), each of which is measured or scaled relatively, by M individuals.” – Stephenson (1936a: 334) “factor analysis with the data table turned sideways”; persons are variables and traits/tests/statements/abilities are the sample or population. looks at 10.2 Data Import All citizens as well as the researcher and the two moderators completed two Q sorts each, one at the beginning and one at the end of the conference. At its heart, Q methodology involves factor analyses and similar data reduction techniques, procedures that are widely used and available in all general-purpose statistics programs. However, Q methodology also requires some specialized operations, not easily accomplished in general-purpose programs. The transposed correlation matrix, flagging participants and compositing weighted factor scores in particular, are hard or counter-intuitive to do in mainstream software. Counter to many Q studies, this research features several conditions (before, after), groups of participants (citizens, moderators, researcher), types of items (values, beliefs, preferences) as well as an extended research question, all of which will need to be analyzed systematically. Running and documenting all these variations will be next to impossible without programmatic extensibility of the software used. 10.3 Import 10.4 Participant Feedback 10.5 Missing Data 10.6 Q Method Analysis Results chapters in quantitative research do not usually recount and justify every mathematical operation from raw data to final interpretation. In reporting mainstream statistics, say, a linear regression (OLM) much in the way of axioms and preconditions is often taken for granted, though perhaps, sometimes too much. Powerful computers, confirmation biases and time pressure for writers and readers alike may conspire to occasionally stretch thin the argumentative link between elementary probability theory and the results drawn from it. Q methodology is different, and requires a more careful, patient treatment. While its mathematical core — different methods of exploratory factor analysis (EFA) — are well-rehearsed in mainstream quantitative social research, its application to Q is still strange to many. Transposed Data Matrix. Because Q method transposes the conventional data matrix, positing people as variables, and items as cases, all of the downstream concepts in data reduction, from covariance to eigenvalues, take on a different, Q-specific meaning, even if the mathematics stay the same. For this reason alone, it will be worth tracking every operation and grounding it in the unfamiliar epistemology of Q. Marginalization and Controversy. Almost 80 years after William (???) suggested this inversion of factor analysis in his letter to Nature, Q is still an exotic methodology. It sometimes invites scathing critiques (Kampen and Tamás 2014), but more often, is outright ignored in mainstream outlets. As the dynamics of marginalization go, the community of Q researchers, may, on occasion, have turned insular — though not into the “Church of Q” Tamás and Kampen (2015) fear. Q methodology may have sometimes shied away from exposing itself from rigorous criticism and disruptive trends in mainstream social research, though probably often out of sheer frustration with persistent misunderstandings, and because of genuine disagreement. In epistemological squabbles, too, crazy people151 sometimes have real enemies — or opponents, at any rate. Happily, within Q, too, considerable disagreement remains (for example, on the appropriate factor extraction technique), though legacy procedures and programs sometimes hamper intellectual progress. Unfortunately, misunderstanding and marginalization is sometimes compounded by a lack of deep statistical understanding, though rarely digressing into glib ignorance of “technicalities” or outright mistakes (“varimax rotation maximizes variation”, in the otherwise fine textbook introduction by Watts and Stenner (2012)). What may easily appear as articles of faith (“thou shalt not use automatic rotation”), are in fact thoroughly argued reservations, based on a deep mathematical and epistemological understanding, as evidenced in both the works of William Stephenson and Stephen Brown, though at least some of this orthodoxy now appears obsolete. These norms become hermetically sealed ideologies (“reliability does not apply to subjectivity”) only when detached from their epistemological underpinnings, as, I suspect, would be the case for other methodologies. Experimental Design Literate Statistics. Not just when, or if, a method is new or controversial, as in Q, mathematical operations and argumentative prose should not be let to diverge too widely. The intuition of literate programming holds for statistics, too: we do not use unintelligible algebra to give us a comprehensible result to be explained. Neither mathematics nor prose are primary, algebra and language are both the explaining intellect at work, just in a different registrar. I suggest then, that at least in the following, initial analysis, some verbosity has its virtue. Readers will be relieved to learn that I intend not to reproduce the entire mathematical apparatus of Q: Steven Brown has accomplished that, and much more in his authoritative, insight-laced “Political Subjectivity” to which my own work is deeply indebted. Mathematics have their place, but formulae alone, in spite of their veneer of rigor, need analogy, intuition and impression to cover the distance to the deliberative subjectities on taxation and the economy under study here — which themselves are not mathematized in this dissertation, nor much convincingly in the broader field, and maybe never will. One may add, that if science is also bound to a discourse ethic of reaching understanding, it too needs to relate its abstractions to the human lifeworld, from which all meaning eminates. At the same time, some of the details of this chapter may raise the ire of some established Q methodologists who never tire of stressing the substantive analysis over statistical sophistication. They are right: the key to understanding human subjectivity lies in iterative abduction, in the thorough going back and forth between informed hunches about what might make sense, and what the data will bear out. The following statistical groundwork is a worthwhile, but merely necessary — not sufficient — condition to a scientific study of subjectivity (emphasis added, though intended by the ISSSS). The following pages will hopefully dissuade the Q skeptics, and take along newcomers to the method. To make sense — as we must — of the shared viewpoints on taxation and the economy among the participants of the first CiviCon Citizen Conference, we must first be sure what they are, and if they are, in fact, shared. 10.7 (No) Descriptives It is common to reproduce descriptive statistics before turning to more advanced analyses. Common summary statistics often include measures of central tendency (mean, median) and dispersion (standard deviation, range) on variables across cases. As mentioned before, in Q methodology, variables are people, and cases are items. The mean rank alloted by Q sorters (as the variables) across all items (as the cases), \\(\\overline{x} = \\frac{\\sum{x}}{N}\\), is then, unsurprisingly, 0 for all 18 participants. The average position of items has to be zero, because the forced distribution was symmetrical: since participants placed an equal number of cards at -1 / 1, -2 / 2 and so on, item ranks will always cancel out. The most frequent median value, also by definition, will be 0 for everyone, because the center value allowed the greatest number of (11) cards per the forced distribution. By the same token, the range extends from -7 to 7 for everyone, because those extreme values are defined by the forced distribution. The standard deviation, too, is the same for everyone. \\(s_N = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}\\), gives the square root of the average squared differences of item scores for a participant, 2.79 for everyone, because the “spread” of items across the mean is defined by the forced distribution. Conventional R type descriptives are, then, quite meaningless (Nahinsky 1965 as cited in Brown 1980, 265), because they are the same for all participants and are defined ex-ante by the forced distribution.152 In search for descriptives, one may be tempted then to revert back to the R way of looking at data, and to treat Q-sorted items as variables, and Q-sorting people as cases. One may ask, for example, which items were, on average of all participants, rated the highest (all-people-own-earth at 2.67), the lowest (poll-tax at -4.11), which items were dispersed the most (pro-socialism at 3.68), the least (tax-outmoded at 1.28), or — most offending to Q methodologists, as well as spurious —, which items were correlated the most positively (poor-central-planner, markets-pragmatism at 0.79), the most negatively (no-tax-corporations, corporate-income-tax at -0.88), and the least (pigouvian-tax, sic-parity at 0). This kind of exploration is fascinating, and it invites seemingly inductive hypotheses: Do people respond most strongly to poll-tax and simple-tax, because these — and other, supposedly similar items — are easy to comprehend and relate to? Do people respond quite differently to pro-socialism, and quite similarly to land-value-tax-resource, because the former obviously aligns with political identities, while the latter does not? Do people feel similarly about exchange-value and natural-market-order because of a pervasive anti-market bias, and very dissimilar about a wealth-tax and a corporate-income-tax, because they fall for a flypaper-theory of tax incidence? While these survey-type approaches to the data are intriguing, they are inadequate for the data gathered here. It may appear arbitrary to categorically rule out a range of broadly-applicable techniques, let alone summary statistics on the grounds that the data were gathered for a different purpose. For example, Likert-scaled data collected for a (R-type) factor analysis may, if conditions apply, be subjected to a regression analysis. Q, however, is not just another statistical operation, it is a methodology, and while the gathered data bear a superficial resemblance to R methodological research, “[t]here never was a single matrix of scores to which both R and Q apply” (Stephenson 1935: 15, emphasis in original). This postulated incomparability applies to this study, too, and plays out in several ways: Generalizeability &amp; Small N Design. In R, the participants are usually a sample of cases from a broader population, and the generalizeability of the results hinges on the quality of this ideally large, random selection. In Q, participants are the variables, on which supposedly shared, subjective viewpoints are measured. Sampling theory does not apply to Q, just as one would not ask random questions in a survey (or so one hopes). Instead, Q method requires researchers to broadly maximize the diversity of participants to capture all existing viewpoints. Generalizeability in Q, if applicable at all, concerns the representativeness of the sampled items of a “population” of statements, though no straightforward sampling theory applies to this concourse either, as it is both infinite and non-discrete. The P-set of this study, the participants in the CiviCon Citizen Conference, are self-selected and probably not representative of the broader population, though recruitment was inclusive and diverse. This flagrantly non-random sampling alone, however, may not yet rule out generalizeability to a broader population of citizens willing to participate in deliberation. As I argue elsewhere, both self-selection and diversity of point of views are crucial for meaningful, concept-valid deliberation, and such non-random sampling may therefore be required for even R-methodological research into the effects of deliberation. Given the haphazard recruitment and great demands placed on citizens, the group of CiviCon participants must probably be considered excessively non-random even by the more charitable standards of deliberation, but an even greater problem lies in the small number of participants. A “sample” of \\(N=18\\) people (including 2 moderating “confederates”) simply does not admit of generalizations toward a broader population, even of would-be deliberators. R statistics, even the cursory summary statistics in the above, implicitly rely on this kind of generalizeability: If we concede that, say, the low average score of poll-tax at -4.11 is, to a large extent an artifact of the people who happened to show up at a locally-advertised, five-day sleepover conference for EUR 50 compensation, it becomes dubious why we should care about this factoid at all — other than to characterize the bias of the group.153 Given limited funds, and little experience the CiviCon Citizen Conference was designed for a small group of people, and this constraint was — admittedly — operational in the choice of Q methodology. The small number of participants (even by Q standards) restricts the following Q analysis, too, but in a different, provisionally acceptable way. Recall that in Q, people serve as variables. 18 variables will still be considered quite crude to extract several latent concepts. Consider, for example, the data reduction in Inglehart’s and Welzel’s human development theory: to arrive at two latent value concepts, they condense 35 variables (2005). The issue here, however, is one of resolution, not generalizeability. With relatively few people-variables, Q method will be able to extract only a few, blurred shared viewpoints. But the exercise is not moot: given a decent Q-sample of items, potential additional participants are exceedingly unlikely to render the factors extracted here null and void — that scenario is later rejected as a Null-Hypothesis of sorts. By contrast, adding more people to this study may very well render the high average position of, say, all-people-own-earth at 2.67 a product of randomness or bias (a high standard deviation of 3.56 would appear to bear this out). The now familiar analogy of LEGO bricks serves to illustrate this difference in concepts of generalizeability once more. Tasking, say, 15 people to build something out of a given set of LEGO bricks may seem reasonable to get a preliminary idea of the kinds of objects (cars, houses) that people build — though we will probably miss out on some rarer constellations (spaceships?) and nuances of existing patterns (convertibles?). Adding more people may increase the resolution to tease out these details, but it’s unlikely that something as basic as “car-like”, or “house-like” will completely disappear. By contrast, again, it seems much less reasonable to take the ratings of individual LEGO bricks by these 15 people as anything but random flukes: there are bound to be some people who like red bricks, more so if the study listing displayed red bricks. Holism &amp; Non-Independence. R methodological opinion research often proceeds by analysis, then synthesis; a construct is first dissected into its constituent parts (say, items), then reassembled into some composite (say, a scale or an index). Truth, in principle, flows from inter-individual differences on the smallest measurable unit of meaning. Inglehart&#39;s and Welzel&#39;s work with the World Values Survey, again, serves to illustrate [-@InglehartWelzel-2005-aa]. Items are constructed with some view to a broader construct (say, civicness), and then narrowed down to number of smaller concepts and items. The reconstruction of latent concepts happens through a very deliberate, almost literate *synthesis*: you take $x$ units of item $A$, $y$ units of item $B$, standardize the result, and out comes a theory of human development to be subjected to confirmatory factor analysis. &lt;!-- FIXME really need to go back to the source and figure this out. This is probably BS --&gt; Here, too, the latent constructs (say, traditional vs secular-rational values), are of greater interest than any individual item, but the relationship is merely *additive*, or *triangulatory*, at best: you synthesize several variables to cancel out bias, and to get at a common concept that might unite them all, but these bigger pictures can never be more than the sum of their parts, let alone *different*. &lt;!-- TODO also add some Brown quotes to all of this --&gt; Q, by contrast, has a *holistic* outlook on human subjectivity. What matters are not the individual items, but there overall constellation, that is, how (groups of) participants value these items *relative* to one another. Summarizes Brown: &gt; In moving from R to Q, a fundamental transformation takes place: &gt; In R, one is normally dealing with objectively scorable traits which take meaning from the postulation of individual differences between persons, e.g. that individual $a$ has more of trait $A$ than does individual $b$; &gt; in Q, one is dealing fundamentally with the individual&#39;s subjectivity which takes meaning in terms of the proposition that person $a$ values trait $A$ more than $B$. &gt; &gt; [@Brown1980: 19] These epistemological differences have practical research implications. In R, items are supposed to narrowly measure *one* concept and should not invite *multiple* interpretations. A typical item from the World Values Survey, `How would you feel about your daughter(son) moving in with a person from a different ethnicity?` was crafted to elicit a predefined, unambiguous scenario in the minds of respondents. &lt;!-- TODO find example for/against double-barelled items in Q --&gt; In Q, an item as open-ended as `labor-no-commodity` (`Labor is not a commodity.`) is suitable *because* it invites a variety of different interpretations on what labor or a commodity are. &lt;!-- TODO need to add more here - there are limits to Q openennes of interpretation too; refer back to the section on item discussions --&gt; R survey design, and drafting (or collecting) Q statements may both be more craft than science, but they are a very different crafts that limit what can, and cannot be done with results. Given the confidence we place in the common understanding of an R-type item, it is reasonable to present summary statistics on this *individual* item. Conversely, given the openness to interpretation in Q, single item summaries make little sense, precisely because they were chosen to mean very different things to different people. &lt;!-- TODO notice that Q items are not always crafted as in this case; that makes the difference maybe even starker --&gt; In R, items are (mostly) measured *independently* from one another, that is, the choice of a participant on one item should not influence or restrict her choice on another item. This independence is not only a requirement for some of the statistical procedures frequently used, it also follows from the analytic-synthetic epistemology: if the synthesizing operation is subject to hypothesis-testing, any relationships between individual items produced by the data gathering method would be considered an undesirable artifact. &lt;!-- TODO find statistical concept for non-independence--&gt; *Rating*, rather than *ranking* measurements are therefore the norm, and some survey research goes as far as randomizing the questionnaire to avoid ordering effects. &lt;!-- TODO find source for randomized order --&gt; In Q, by contrast, items are evaluated *only* relative to one another and participants are reminded that the absolute scores assigned (-7, 7 in this case) merely imply *ordinal* (better *than*), not *cardinal* valuation. &lt;!-- TODO notice that at least I stressed that; also relate to later discussion --&gt; Relative item rating in Q has a technical reason, too: Only because all items are evaluated relative to all other items, all item scores come in the *same* unit (valuation relative to the remaining items), a transposed correlation matrix becomes possible (a statistical summary, of, say, height and weight would be nonsensical). &lt;!-- TODO is this analogy exactly right? Have I really gotten this? --&gt; But strictly relative valuation has an epistemological dimension, too: if meaning can be derived from the *entire* item constellation only, participants must always choose *between* items. *Rating* and *ranking* measurements, taken to these extremes, are not interchangeable and strictly limit the meaningful presentation of results. An item from the World Values Survey can be summarized in isolation, because it was measured independently: the agreement with, say, a son-in-law from a different ethnicity by any one participant does not preclude her equal agreement with another item. An item from a Q study as this cannot be summarized in isolation, because it was measured in a dependent way: a participant ranking, for example, `labor-no-commodity` at the top, will be precluded from ranking any other item in the same way. By extension, it also makes little sense to present a mean valuation of `labor-no-commodity` (0.61) in isolation because, in absence of the means of *all other items*, we have no idea what that value *means* (it might mean very different things, for example, if all other items were supremely agreeable). The analogy of LEGO bricks only slightly overstates the absurdity of ignoring the holism aspired to by, and non-independence implied by Q. Recall that participants were instructed to assemble a given set of bricks into an object of their design. It would clearly be nonsensical to take the low *absolute* (x-axis) position of, for example, a small red brick as an indication of disliking, without relating its position to the overall design: depending on the position of other bricks, it might be part of an exposed car fender, or a hidden structure in a house basement. It would be similarly perplexing to harp on the higher average (x-axis) position of ramp-shaped bricks, compared to the cuboid-shaped bricks: *of course*, if you are putting roof tiles on top, you *have* to place the foundation walls of a house underneath it. Validity &amp; Research Ethics. Finally, Q and R differ in their conceptions of validity. In R, a measurement of inference is valid, if and to the extent that the stated concepts correspond to some objective reality &quot;out there&quot;. For example, the above-mentioned WVS item on `your daughter marrying someone from a different ethny` rests on the assumption that there is such a thing as an attitude on inter-ethnic relationships (which seems reasonable). &lt;!-- FIXME again, fix the WVS reference, or kill it --&gt; No matter how supposedly *inductive* --- probably just *exploratory* --- an R approach to data is, the terms of that data are always set *before the fact*, *by the researcher*. Notice that the WVS item on inter-ethnic relationships will *not* admit of an attitude unforeseen by the researchers, say, a discriminatory sentiment based on *language*. In Q, conventional definitions of validity do not easily apply, at least because there is, by definition, no external and objective standard to verify human subjectivity. &lt;!-- TODO point to respective section where I discuss that --&gt; Instead, one *posits* that viewpoints become *operant* through the act of sorting Q-cards, and that even a limited sample of diverse items will give people a roughly adequate material to impress their subjectivity on. &lt;!-- Notice Browns language on this in the email on coefficients: it&#39;s a logic -of -science kind of axiom; we model things as if - we also model them on the forced distribution, might add this here --&gt; Items may --- or may not --- have been crafted and sampled with some theoretical preconception in mind (for example, libertarianism for `deontological-katallaxy`), but that preconception should not, in principle, limit the meaning that participants ascribe to it. Dramatically *different* interpretations of the same item, are emphatically *not* a threat to validity in Q. It is easy to see how the above, cursory summary statistics shift the standard of validity, and invite what Brown calls &quot;hypotheticodeductive&quot; inferences [-@Brown1980: 121]. To nod at the high positive correlation of `exchange-value` and `natural-market-order` across participants is to reify a concept of &quot;market radicalism&quot; (or something similar), from which these items probably sprung *in the mind of the researcher*. *&quot;Ah, that makes sense&quot;*, one thinks --- and by sense, meaning the sense of the researcher and (maybe similar) sense of the reader. Looking at any particular item or relationships between items across people, always risks implying that the *meaning* of any given item is the same for everyone, as per the researcher&#39;s specification. It is also easy --- and self-serving for Q --- to overstate the difference to a hypotheticodeductive paradigm. Falling short of accepting a debilitating, radical constructivism, researchers --- as all people --- will always have to rely on some *common* understanding of language, including Q researchers. Researchers cannot interpret shared viewpoints of common sorting patterns (the factor scores) without *some* reference to their own preconceptions in drafting or sampling items. No factor interpretation of holistic patterns of items can be undertaken without *some* reference to their own preconceptions about those items. It would be impossible to interpret even the relative position of `exchange-value`, rejecting any overlap or relationship to our academic understanding of that item: we would be looking at a heap of structured gibberish. Q researchers are, in fact, quite fond of their own judgment: &quot;researchers might, on occasion, &#39;know what to look for&#39;&quot; [Stephenson 1953: 44, as cited in @Watts2012: K2308]. The epistemology of *abduction* suggests to repeatedly go back and forth between hunches, interpretation and what the data will bear out. &lt;!-- TODO that&#39;s weak; fix and refer to abduction section --&gt; The point of validity in Q methodology is not to ban all researcher&#39;s preconceptions, but to both *relax* their grip on meaning, and to *constrain* the plausibility of their explanations. On the one hand, the meaning of items is *relaxed*, because the linear relationship to the researcher&#39;s hypotheticodeductions is no longer presupposed: `pro-socialism`, may, or may not have been interpreted as the researcher intended. On the other hand, the plausibility of any explanation is *constrained*, because any observed shared viewpoint must make *sense*, per *some* set of interpretations of its items. These epistemological &quot;Goldilocks&quot; conditions disappear when R statistics are applied to these data: when presented the isolated finding of the high negative correlation between `wealth-tax` and `corporate-income-tax` (-0.88), we can *only* refer back to some of the preconceptions on &quot;misunderstanding taxation&quot; presented earlier (Flypaper Theory). &lt;!-- TODO add reference to other section, mcccaffery --&gt; This may be a productive approach --- though probably not best undertaken with these items and data gathering --- but thereby &quot;testing&quot; people&#39;s understanding of tax emphatically is not a valid measurement of their subjectivity, and falls short of the deliberative conception of public choice espoused earlier. &lt;!-- TODO that&#39;s not quite right. Also, reference other section --&gt; These epistemological concerns also carry a research ethical imperative: The data should be treated as participants were told it *would* be treated. That adherence to informed consent *includes* the methodology. &lt;!-- TODO link to informed consent --&gt; I told the 16 guests at the first CiviCon Citizen Conference that I was interested in their viewpoints on taxation, that their unique perspectives on the economy mattered, and that there *would be no wrong way* to sort the Q-cards. &lt;!-- TODO link to the condition of instruction --&gt; For up to three hours, these citizens earnestly struggled with these 77 items &quot;on which reasonable people could disagree&quot;, thoroughly weighing thoughts and items against another, graciously pouring their thoughts into the rigid and crude form of a Q-sort. It would betray the trust and degrade the effort of the participants to treat these data as if they were R: I could have just handed them a survey and saved them a lot of time. The ethics get even more damning, when R statistics treat data as tests, and understanding turns to grading, as so easily happens when looking at the extremely negative correlation of `wealth-tax` and `corporate-income-tax`, or similar would-be inconsistencies. In the morning session of the second day, some participants expressed worry over being &quot;tested&quot; in the Q-sorts, as well as the conference. The conundrum of measuring people&#39;s understanding of taxation, without testing them on some narrowly-preconceived notion of consistency is a familiar one, and it will continue to plague this research. &lt;!-- TODO add link --&gt; The least I owe to the CiviCon participants is to make every effort to render their viewpoints understandable *from the inside*. That cannot be done with R methodology, and implore everyone using this data *not* to try. LEGO, again, delivers a coda by analogy. A hypotheticodeductive inference on a persons LEGO building would rely on a preconceived understanding of what any given brick means, for example: &quot;this is a transparent brick&quot;. Any discussion of the (absolute) position of individual items would, absent any other information, have to revert to the preconception of the researcher. For example, one might argue that the extreme placement (on any axis) of the transparent bricks indicates strong feelings for these types of bricks. In the context of a LEGO house, of course, the transparent bricks may be considered *windows* by participant builders, and be placed on the exterior of the building because of that function. More varied meanings of transparent bricks are conceivable. A subset of builders may place *small* transparent bricks *inside* the houses, not readily explicable by their transparency: why would *anyone* greatly place almost all transparent bricks on the perimeter, but only a very small one inside, the R researcher may wonder? Given the context, one may recognize the surroundings of the small transparent bricks as &quot;kitchens&quot; and venture that these builders interpreted them as maybe sinks full of water, or aquariums. Characteristically, the meaning of &quot;transparent brick&quot; is relaxed considerably in the light of context, but it also remains bound to a common understanding of some of its features (transparency), and therefore constrained to the subsets of possible meanings that might make sense of it (aquarium, sink full of water, or other *transparent* objects). One may add that the participant builders may also be quite offended, if they learned that their LEGO constructions did not receive any attention as such, but where instead intended as a test of visual acuity: whether participants could distinguish transparent from non-transparent bricks. The point here is not to criticize R methodology, though the near hegemony over quantitative social sciences that Brown rails against (1980: 321), may indeed have long stretched thin its foundations and overreached its purview. The point is that, for the present research question or given the present data, any excursion to R methodology would at best be a distraction, and at worst strictly unscientific. 10.8 Correlations To extract shared viewpoints, we must first establish what it means or any pair of Q-sorts to be alike. Since we would expect people with similar viewpoints to sort items in similar positions, and therefore have similar values over all items, respectively, we can use a correlation coefficient between any two people’s sorts as a statistical summary of their similarity. A correlation matrix of all such pairs is the simplest summary statistic that can be presented for Q data, maintaining both the holistic and relative nature of the sorts. Though Q studies do not always display or discuss the correlation matrix, all of the downstream analyses are, in fact, based on this table (Brown 1980: 207). 10.8.1 Correlation Coefficients Depending on assumptions made, different correlation coefficients are applicable to Q data. These coefficients are well-known statistics, but, their underlying assumptions can be thorny for Q methodology and consequential for these data. The proceeding discussion also serves to rehearse the uncommon person-correlations at the heart of Q methodology. Pearson’s \\(\\rho\\) Product-Moment-Correlation Coefficient. The conventionally used Pearson product-moment correlation coefficient, or Pearson’s \\(\\rho\\), starts from the covariance of any pair of Q-sorts, in their raw form, \\[cov(X,Y)=\\sum_{i=1}^{N}{\\frac{(x_i-\\overline{x})(y_i-\\overline{y})}{N}}\\].154 Since in a forcibly symmetric Q distribution as ours, the mean will always be zero (see above), we can safely ignore the respective \\(-\\overline{x}\\) terms; by default, Q-sorts are already expressed as deviations from their expected values, \\(E\\). The covariance on all items for, say Ingrid and Petra, is therefore simply the cross-product of all item positions divided by the number of items, \\(N\\). It’s easy to see how this number will be larger when the two scores have similar (absolute) values: Petra’s and Ingrid’s quite different ranks for, say all-people-own-earth (6, 2, respectively) multiply to 12, whereas their more similar positions on deontological-ethics (7, 5, respectively) yields 35. Brown reminds us that this multiplicative weighting of like extreme scores is “phenomenologically” appropriate, because respondents feel more strongly and are more certain about items ranked towards the margins (1980: 271). The resulting covariance is, unfortunately, hard to interpret, because it is in squared units and depends on the spread of the data. It is normalized into Pearson’s product-moment correlation coefficient by dividing it by the product of the two standard deviations, in this case, Petra’s and Ingrid’s — both of which are, by definition, the same. \\[\\rho(Petra,Ingrid) = \\frac{\\operatorname{Cov}(Petra,Ingrid)}{\\sigma(Petra)\\sigma(Ingrid)}\\] Consider the special case of correlating a person’s Q-sort with her own Q-sort, say Petra with Petra, to understand the bounds of the Pearson’s \\(\\rho\\). The denominator in (2) yields the \\(\\operatorname{Var}(X)\\), the square of Petra’s standard deviation over the item scores is, by definition, her variance. The numerator is given by the summed product of Petra’s scores with Petra’s scores, which is also the variance. The correlation coefficient of two identical Q-sorts is therefore \\(1\\). A correlation coefficient of \\(-1\\) would result if the two Q-sorts were diametrically opposed, as if mirrored around the distribution mean of zero: in that case, the numerator would be \\(-\\operatorname{Var}-\\), and everything else would remain the same. A correlation coefficient of \\(0\\) would be expected if two Q-sorts are entirely unrelated. All of these extremes are rarely reached in Q, with a range of -0.12, 0.62 in this case. The Pearson correlation coefficient is a measure of linear association between two variables, or Q-sorts in our case. Spearman’s \\(\\rho\\) Rank Correlation Coefficient. Spearman’s \\(\\rho\\) works much like Pearson’s \\(\\rho\\), but starts from rank orders, instead of raw Q-sort values. Raw Q-sorts are rank ordered, and assigned their rank index. Crucially, in a case of ties — of which there are many in Q —, the average of the would-be occupied ranks is assigned as a rank. For example, if two cards are sorted in the \\(-5\\) column, as per the Q distribution, they will both receive the rank of \\(3.5\\), because they would occupy the 3rd and 4th position, if they were not tied. A Q-sort, beginning from the left (negative) extreme will look like this if transformed into Spearman’s ranks: Raw Value Index Spearman’s Rank -7 1 1 -6 2 2 -5 3 3.5 -5 4 3.5 -4 5 6.5 -4 6 6.5 -4 7 6.5 -4 8 6.5 -3 9 12 -3 10 12 -3 11 12 -3 12 12 -3 13 12 -3 14 12 Spearman’s procedure then computes a Pearson’s correlation coefficient (as in the above) from the rank differences \\(d_i = x_i - y_i\\) (instead of the difference to the mean). This version of Pearson’s coefficient can be summarized thus: \\[\\rho = 1- \\frac{6 \\sum d^2}{n^3}\\] The resulting coefficient is likewise bounded from \\(+1\\) to \\(-1\\), with perfect correlation indicating not a linear association, but a strictly monotone association, where all items are ranked the same, though possibly by different “amounts”. Conversely, two Q-sorts with mutually reversed ranks would yield a \\(-1\\) coefficient, indicating a negative, monotone relationship. Kendall’s \\(\\tau\\) Rank Correlation Coefficient Kendall’s \\(\\tau\\) starts not with differences in scores or ranks, but with a comparison of item pairs between the two Q-sorts. When the ranks of two items (\\(j\\), \\(i\\)) agree between two Q-sorts (\\(x\\), \\(y\\)), the item pair is said to be concordant (\\(x_i &gt; x_j\\) and \\(y_i &gt; y_j\\), or \\(x_i &lt; x_j\\) and \\(y_i &lt; y_j\\)). When the ranks between the two respondents disagree (\\(x_i &gt; x_j\\) and \\(y_i &lt; y_j\\) or \\(x_i &lt; x_j\\) and \\(y_i &gt; y_j\\)), they are said to be discordant. An item pair is said to be tied on a Q-Sort if both items have received the same score (as frequently happens in Q), \\(x_i = x_j\\). In the simplest form, Kendall’s \\(\\tau_A\\) divides the difference between the number of concordant pairs \\(n_c\\) and the number of discordant pairs \\(n_d\\) by half the number of pairs squared: \\(\\tau_A = \\frac{n_c - n_d}{n(n-1)/2}\\). For example, Petra and Ingrid are concordant on poll-tax (-3, -4, respectively) and corporate-income-tax (q_sorts[&quot;corporate-income-tax&quot;,c(&quot;Petra&quot;,&quot;Ingrid&quot;),&quot;before&quot;], respectively): they both prefer corporate-income-tax over poll-tax, if by a different amount. They are discordent on infinite-growth (0, -3, respectively) and use-value (-1, 0, respectively): Between the two, Petra prefers infinite-growth, Ingrid prefers use-value. Because the denominator gives the total number of combinations of two items out of the total number of items (in this case 2926), Kendall’s \\(\\tau\\), too, is bounded from \\(+1\\) to \\(-1\\). If two Q-sorts are identical (opposed), they will (dis)agree on all pairwise item comparisons, divided by all possible such combinations, yielding (\\(-\\))\\(1\\). If two Q-sorts agreed and disagreed on half of the comparisons each, the numerator will be \\(0\\), yielding a \\(0\\) coefficient. Because in Q-sorts items must be stacked in some columns, there are many ties. Kendall’s \\(\\tau_B\\) additional terms in the denominator adjust for such ties, considering the number of ties \\(n_1\\) and \\(n_2\\) on both Q-sorts: \\[\\tau_B = \\frac{n_c - n_d}{\\sqrt{(n_0-n_1)(n_0-n_2)}}\\]. Kendall’s \\(\\tau\\), does not give an association between the two Q-sorts, but expresses the probability, that any given pair of items will be ranked equally between the two Q-sorts. On a technical level, however, all of the code used to calculate correlations reported here us the sample statistic. Because because the de-biased \\(N-1\\) term occurs both in the numerator and denominator of the correlation coefficient, it cancels out. The choice among these correlation coefficients depends first on the kind of data gathered upstream, and the kind of analysis intended downstream. Pearson’s \\(\\rho\\) requires at least interval-scaled data: the difference between values must have cardinal, not just ordinal meaning. ^[Aside from these considerations, Pearson’s \\(\\rho\\) is susceptible to outliers (not robust) and requires normally distributed data, though both are not a great concern in Q methodology, when forced distributions bound extreme values and close to a bell curve (as the current distribution is). For example, cardinal valuation would imply cards sorted under \\(3\\) are preferred to cards sorted \\(2\\) by the same amount as cards between \\(7\\) and \\(6\\) respectively — not just that higher sorted cards are valued more. Both Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) relax this assumption, and can be used on ordinally-scaled data, with item scores merely rank, but not a rating. 10.8.1.1 Ordinal vs Interval Data Q method is, unfortunately, a bit muddled on the issue of cardinal or ordinal valuation in Q-sorts. Most studies simply use the default Pearson coefficient and many articles as well as the leading introductory textbook on the method (Watts and Stenner 2012) do not even mention, let alone justify, the use of one of the three common coefficients. This oversight might be easily forgiven, because for Q data, at least Spearman’s and Pearson’s coefficients appear to be closely related. Given the relatively high number of ranks assigned (usually more than 9, 15 in this case), many researchers might have no trouble treating the data as interval-scaled, much as (shorter-ranged) Likert-scales are often interpreted. Additionally, Q data is tightly bounded, especially when a forced distribution is used: there can be no outliers, that might inordinately affect Pearson’s, but not Spearman’s. One might then conclude, as Block (1961: 78) and Brown (1971: 284) that the choice of correlation coefficient does not much matter, effectively sidestepping the fundamental question of which coefficient is the appropriate one. Brown argues correctly that absent ties, Spearman’s and Pearson’s must be identical, and that illustrate with his data, that even with ties, Spearman’s and Pearson’s “produce virtually identical results […]”, which are “in turn […] essentially the same as those obtained utilizing Kendall’s \\(\\rho\\)” (1980: 279). The problem is that, in fact, Q data has many ties, and using this data, the coefficients do differ by an average of 0.03, ranging up to a sizable 0.19 for Spearman’s and Kendall’s, each compared to Pearson’s. As the sensitivity analysis in the appendix shows, these differences in correlations can also filter down to the factor analysis, and may even bias the interpretation in a systematic way. Aside from Brown’s here falsified empirical claim that “within the factor-analytic framework […] the interval-ordinal distinction is of no importance” (1980: 289), there appears to be no methodological literature to justify a choice of coefficients on Q-epistemological or ontological grounds. The choice between interval and ordinal valuations in Q-sorts seems to turn on the status of the Q distribution. On the one hand, we can seek to ascribe positive status to the Q distribution, that is, assume that there is such a thing as a true distribution on any given set of items for any given respondent, and that this distribution can be measured. The simplest way to go about this may be to let people sort items freely into a fixed number of bins. Given the relatively great number of bins (15 in this case), it seems reasonable to treat distances between adjacent bins as meaningful, because respondents were able to freely project their subjectivity, however formed. Alternatively, one can assume that any respondent’s feelings towards any set of items are normally distributed as a matter of ex-ante empirical finding. Assuming that every person has one \\(-7\\) item, two \\(-6\\) items, and so forth under the bell curve, it is reasonable to assume same and meaningful distances between equal bins slicing up the (continuous!) normal distribution. This latter assumption is, of course, quite heroic, especially considering that P-sets of items cannot be random sampled and any given set of items will likely include idiosyncracies. I am not aware that anyone in the Q literature would make, nor provide evidence for this latter assumption, though few researchers have implemented free distributions. On the other hand, we can ascribe epistemological status to the Q distribution, that is, treat it “as a formal model in the logic-of-science sense and in terms of which persons are instructed to present their points of views”, in Brown’s characteristically cogent words (2015, via email on Qlistserv). Oddly — though not unconvincingly — this is then an operational definition of the distribution to operantly measure the structure of subjective viewpoints (Stephenson 1968: 501), to borrow a popular dichotomony in Q circles. The quasi-normal Q distribution is defined by the researcher as the terms in which respondents may express the concept of subjectivity, much as R researchers define items based on some pre-existing concept. This latter view of the distribution underpins most of Q research, though not always as explicitly as in Brown’s writing. The case for the forced-distribution-as-model is twofold. First, the forced distribution strongly single-centers all the items, a pre-condition for a transposed factor analysis: under a forced curve, all items are always ranked relative to the same reference point of all other items.155 Second, the forced distribution extracts “hidden” preferences by making Q sorters make the same, tightly circumscribed decisions: for example, people cannot place more than one item in the leftmost bin. A complete rank ordering might be the logical — if impractical — solution to reveal all hidden preferences: in a complete rank order, no ambiguities are left. The former positive view on the Q distribution might straightforwardly justify cardinal valuation of Q-sorts. Stephen Brown argues that cardinal valuation can be maintained for the latter, epistemological view on the Q distribution, too, or at least that “the equality of intervals along the Q-sort range […] is no more subject to proof than it is to disproof, which is common to models.” (Brown 2015, email to Qlistserv) It seems to me though, that the model implied here conceives of Q-sorts as ordinal valuations, and that data should be treated respectively. To operationally “reveal the Q sorters’ preference (as opposed to their likes)” by forcing people to decide between items is to negate meaningful distances (ibid.). Stephen Brown may be correct that “preferences” and “likes” cannot be observed at the same time, akin to the quantum principle of complementarity (Brown’s analogy). However, this same complementarity holds between ordinal and cardinal valuations. If preferences are to be revealed in terms of choices constrained by the measuring mechanism, then that is an ordinal measurement, without meaningful distances. Meaningful distances require that respondents be arbitrarily free in their choices, including, for example, all items in one column, with no distinctions enforced. An ordinal interpretation of data also dovetails with the related, relentness instructions to respondents that they worry only about the relative positioning of items. Readers may wonder, at this point, what could possibly warrant this methodological diversion in the context of the Keyneson data analysis. Problematically, the correlation matrix for Keyneson looks not just different, depending on the chosen coefficient, but in a systematic way. This bias stems from the different weighting of similarly (and dissimilarly) sorted items in the extremes of the distribution. Recall that Pearson’s \\(\\rho\\) squares the covariance, giving great weight to whatever similarity people’s viewpoints express in their tails. By comparison, Spearman’s \\(\\rho\\) offsets this quadratic weighting partly by “spreading out” the center of the distribution: for example, in Spearman’s ranks, the difference between the (raw, Pearson’s) \\(0\\) and the \\(1\\) column is \\(10.5\\), whereas the difference between the \\(6\\) and the \\(7\\) columns is only \\(1\\). As an illustration, Spearman’s \\(\\rho\\) assumes that respondents ordered the items side to side, with equal signs pasted where they were indifferent (tied), instead of on top of one another in equal-width bins. Because the number of ties in the center is, by virtue of the forced distribution, greater than the number of ties towards the extremes, differences in valuation towards the center occupy a greater range. For example, two equally ranked items at (\\(+7\\)) and (\\(+1\\)), will contribute 1444 and 110.25, respectively, at a ratio of and two equally ranked items at a ratio of 13.1 to 1 in Spearman’s ranks, but 49 and 1 at a ratio of 49 to 1. In short: similarities and differences between Q-sorts towards their middle get greater thrift with Spearman’s \\(\\rho\\) compared to Pearson’s. Brown, as previously noted, suggests that the quadratic weighting of differences and similarities towards the extremes is warranted because respondents feel strongest about these items. That may be so, but it implies an otherwise rejected positive assumption on the Q distribution, namely, that all respondents do feel much stronger about items placed in the extremes, and by the same amount. Absent such a positive assumption, the inordinate influence of covariance in the extremes can conspire to introduce bias as a function of the given Q-set. Consider that items may — especially when they are not gathered “in the wild”, but crafted, as for Keyneson — differ in the starkness of their wording. For example, pro-socialism might have been formulated without reference to the informative, but tainted term “planned economy”.156 Consider also that some (and only one) item has to fill the extreme position, irrespective of whether a participant really feels that much stronger about the last item. It may then be that starkly worded items tend to occupy these extreme positions, greatly load the correlation matrix and downstream factor analysis. In the extreme case, the inclusion (or exclusion) of a divisive item such as pro-socialism (greatest standard deviation at 3.79) may produce so much covariance as to become something of an “anchor item” for a factor. The issue here is not that some items “anchor” a factor more than others (that is very much the point of distinguishing statements), nor that some items are worded in a different style (that is very much the point of the factor interpretation). The issue is that absent a positive view on the Q distribution — which we have rejected — the divisiveness of some items may crowd out other, “milder” patterns of covariance towards the middle of the distribution, even trivialize factors in an extreme scenario, all for no good reason. Recall that without a positive distributive assumption, we have no reason to assume that the difference between, say \\(6\\) and \\(7\\) is the same as between \\(0\\) and \\(1\\). We cannot consequently know in any absolute amount how much more or less strongly people felt about more or less divisive items: the weighting of the (mildly worded) middle covariances to the (starkly worded) extreme covariances is, becomes arbitrary. Q methodology with a forced distribution has thrust us into a no-mans-land between interval and ordinal data, inhospitable to sound statistics. If we assume interval scaling, and use Pearson’s \\(\\rho\\), covariances in the extremes are weighted more, though the magnitude of that weighting is suspect, for the above-mentioned reasons. If we assume ordinal scaling, and use Spearman’s \\(\\rho\\), covariances in the extremes are weighted less, though the magnitude of that weighting is suspect, too. Recall that the “spreading out” occurs as a function of ties within the higher columns towards the middle of the distribution. These ties, however, are also enforced by the method, and did not arise organically. Ideally, Q method would produce a thoroughly grounded clarification of how covariances across the (forced) distributions should be appropriately weighted. Unfortunately, at least the outspoken members of the Q listserv appear untroubled by this supposed conundrum. Bob Braswell (2015, email on Qlist) suggests that because the downstream analyses require Pearson’s “the burden of proof is on someone who would like to substitute another correlation coefficient”. Braswell and Brown also doubt differences in coefficients will be consequential, and suggest any downstream effects may be counteracted by judgmental flagging and rotation. In a way, these skeptics are right: even a (rejected) uncertainty on the choice of correlation coefficient does not invalidate Q Methodology, and final factors are likely to share a great family resemblance, as also indicated by the robustness analysis in the appendix. Still, this ambiguity on coefficients affects Q methodology at a fundamental level, carrying on through all downstream analyses, and even small differences should matter for an avowedly scientific study of subjectivity. The troublingly common resort to abductive judgment in the face of muddied statistics risks loosening the tight positive bounds placed on researcher’s interpretation ever mover: if, in addition to the rotation method, the flagging, the number of factors and the extraction method, even the correlation coefficient were up for grabs, researchers would enjoy considerable latitude in specifying their models — all before the supposedly bounded qualitative interpretation has begun. These and other methodological challenges are often met by reframing them in terms of (outsider) R and (insider) Q approaches, even where, as in this case, the conundrum is thoroughly grounded in Q, possibly betraying an insular retreat of the method. Alternatively, Q method could take either ordinal or cardinal valuations in Q sorts to their respective logical conclusions: complete rank orders and non-parametric statistics in the former, freely distributed sorts in the latter. Such innovations would require, among other things, thorough validation, and are clearly beyond the purview of this research. Among the imperfect — because arbitrary — choices, Spearman’s \\(\\rho\\) appears to be the conservative choice, and for that reason, will be used in the following. Spearman’s stresses the middle of the distribution, an area where respondents feel less strongly, and less clearly about placed items. Any confusion or indifference about these items should result in random placement around the mean, and therefore, cancel out in the correlation coefficient. Overall correlations may be lower (they are not at 111.23 and 107.84, respectively) , or less patterned, and resulting factors are less likely to be superficially anchored by some agreement in the extreme. 10.8.1.2 (Linear) Association vs. Probability Assuming that the present Q-sort data best be treated as ordinal scaled, the question arises how their correlation coefficients can be appropriately and meaningfully summarized into ideal viewpoints. Q methodology, ordinarily relying on Pearson’s \\(\\rho\\) has it easy: as measure of linear association it lends itself well to the matrix algebra of factor or principal components analysis. The case is more complicated for rank order statistics. Fabrigar and Wegener rule out exploratory factor analysis for ordinal data, suggesting (2012: 96). Instead of shoe-horning rank data into factor analysis , Bartholomew, Stew et al. recommend new “model-based methods” (2011: 45). As indicated in the above, taking an ordinal view on Q-sorts to its statistical conclusion might be worthwhile, but the required re-validation and epistemological foundation is untenable here. Luckily, other statisticians are more optimistic about factor-analyzing ordinal data. Basilevsky recommends “euclidian measures such as Sperman’s (sic!) \\(\\rho\\) […]” over Kendall’s \\(\\tau\\) probability, because the latter obscures local differences and yields no meaningful factor analytic result (1994: 512, emphasis added). Fittingly, Basilevsky explicitly validates a Spearman’s-based principal-components analysis of an “n x n matrix […] among the observers”, though he might not have had Q methodology in mind (1994: 515, emphasis added). He concludes for Spearman’s that “if intrinsic continuity can be assumed either as a working hypothesis or by invoking a priori theoretical reasoning, most factor models carry through just as if the sample had been taken directly from a continuous population” [(1994: 518). That bodes well for this analysis. Intrinsic continuity is customarily assumed for “six or seven categories” (Bartholomew et al. 2011: 245), and should be defensible for 15 categories. This remains very much an “ad hoc method[]” for treating Q-sort data (Bartholomew et al. 2011: 45, emphasis in original), but it is a conservative one. At best, analyses based on Spearman’s \\(\\rho\\) dovetail with Brown’s operational definition of subjectivity as Q-sorts under a quasi-normal distribution, implementing an appropriate statistic for fine-grained, but ordinal data. At worst, Spearman’s-based analyses fall short off the ordinal scaling of Q-sorts, and merely effect a monotone transformation of would-be cardinal data, emphasizing covariances towards the center of the distribution. 10.8.2 Correlation Matrix The simplest summary statistic in lieu of descriptives we are left with is the correlation matrix. Qmethod — as other Q software, does not report the correlation matrix, but the below correlation matrix is produced calling the same function also used in later analyses, a Spearman’s \\(\\rho\\) correlation coefficient. The correlation matrix is characteristically complex, and hard to interpret — which is probably why most Q studies, especially with more participants, do not report it. Recall, again, that the correlations in the above matrix are between people-variables across item-cases. The below plot illustrates this for the strongest positive, the strongest negative, the weakest and a self-correlation. To protect the anonymity of participants, as discussed elsewhere, I cannot provide additional details to contextualize individually high or low correlations between Q sorters. Suffice it to say that the correlations broadly track views expressed at the conference, and, to a lesser extent, age and education. Still, some general observations are in order. Correlations do not go especially high, with a maximum of 0.6, though merely rough similarities may be expected with a diverse and complicated Q-set as keyneson. Recall that some participants reported problems sorting several items: the resulting (random) error might prevent greater correlations (greater overall, and more bifurcated correlations after the conference appear to bear this out.) More remarkable is the fact that while there are several correlations close to 0 — quite rare in Q —, there are no substantial negative correlations. Recall that the Q-set was designed for “reasonable people to disagree on”, so why did respondents not disagree more? One the hand, the absence of negative correlations might indicate that people do not think in strictly opposite ways about taxation — or the keyneson items, at any rate. Instead, people might think differently about it, expressing uncorrelated sorts. The greatest difference in expressed subjectivity among the participants lies not between diametrically opposed patterns, but between different patterns. On the other hand, the predominantly positive correlations might betray a lack of diversity, or presence of skew among the P set of participants already suspected from the conference report. For example, in a broadly social-democratic crowd, with some socialists and conservatives sprinkled in, but no libertarian, disagreement is unlikely to be diametrical. Neither a socialist, nor a conservative take the precisely the opposite viewpoint of a social democrat, though they may differ considerably. While this lack of diversity may spell trouble for the quality of the sample, it is good news for the factor analysis. Without negative correlations, it is unlikely that people will load on opposite ends of “polar factors” — a great hassle for computing composite scores. 10.9 Factor Extraction Based on the above correlation matrix showing the pairwise similarities of all participants, Q methodology proceeds by summarizing these patterns into fewer, shared viewpoints. By way of LEGO analogy, the correlation matrix superimposes any two buildings on top of another, displaying the degree of overlap between them. To extract the kinds of objects that our participant builders have constructed, we must somehow further summarize this matrix to yield a shorter list of family resemblances among the LEGO objects. Q methodology, in other words, rests on an exploratory data reduction technique. 10.9.1 Which Data Reduction Technique Just which of a number of such data reduction techniques would be a appropriate for Q methodology is, alas, again equally disputed and consequential. The Q literature, unfortunately, offers no definitive resolution, but instead loosely refers to epistemological (metaphysical?) axioms (“indeterminacy”) or invokes tradition (Stephenson, and increasingly, Brown) and past computational limitations (e.g. Stephenson (1935): 18]). Criticism of such Q orthodoxy is rising on the Qlistserv and elsewhere, most notably (and convincingly) by decade-long contributor (and PQMethod maintainer) Peter Schmolck as well as recently Akhtar-Danesh (2007), but no work has been published yet. Butler’s critique, originally directed at factor analysis in general, oddly, still seems to apply to Q: “far too many […] students complain that the only basis they have for choosing among the many types of factor analysis is the prestige of a given theorist or the fact that this or that”maximin&quot; solution has often been used&quot; (Butler 1969: 252f), also see updated critique in (Yates 1987: 6) Absent well-justified choice in the literature, we then have to delve into the alternative methods and decide for this study on their merits. Readers will be relieved that the purview of this investigation can be pragmatically limited to only three different extraction methods for exploratory factor analysis (EFA): Principal Components Analysis (PCA), sometimes also known as the Hoteling method Centroid Factor Analysis (CFA), also known as the Simple Summation Method Principal Axis Factoring (PAF), also known as Common Factor Analysis There are many other multivariate techniques that may — or may not — hold promise for Q methodology, including cluster analysis (CA), multi-dimensional scaling (MDS), latent profile analysis (LPA) or structural equation modeling (SEM), as well as yet different non-parametric methods (latent trait and latent class analysis) (Yates 1987: 2), as may be required by a stringently ordinal reading of Q sorts (see above). Such a systematic reconstruction of Q methodology in light of currently available techniques may be worthwhile — or not —, but it is clearly beyond the scope of this study. Of course, just because alternative methods may be difficult and untested does not mean that the above conventional techniques are adequate. Limiting the discussion to the above techniques is still prudent, not only because they have been tested in Q studies, but also because they (especially PCA) are relatively simple mathematical operations with limited assumptions that can be readily defended. Readers will also be relieved that only the chosen procedure will be presented in some mathematical detail; the choice among PCA, CFA and PAF can be well-justified on a verbal level. The choice boils down to two features of the three methods: a supposed indeterminacy and a possibly implied latent variable model. Indeterminate Determinate Latent Variable Model Centroid Factor Analysis (CFA) Principal Axis Factoring (PAF) Linear Composition Principal Components Analysis (PCA) 10.9.1.1 Latent Variable Model or Linear Composition Centroid Factor Analysis (CFA) and Principal Axis Factoring (PAF) are both latent variable models. They estimate a smaller set of latent variables (here: Q factors) from a larger set of manifest variables (here: individual Q sorts). By the same token, these techniques allow for error: any particular manifest respondent’s Q sort would be explained by her loadings on a set of latent factors, plus some residual error term, accounting for the particularities of that person’s Q-sort. As such, CFA and PAF summarize only the common variance among the sorts, but not the specific variance. By contrast, Principal Components Analysis (PCA) considers all of the available variance, common and unique. There is no concept of measurable manifest and underlying latent variables in PCA: as will be described later, the principal components are merely a compressed “vector recipe” to re-expand to the original correlation matrix. This difference between CFA and PFA on the hand and PCA on the other also reflects in the correlation matrix passed on to the data reduction technique. Recall that the diagonal in a correlation matrix are all ones; by definition, a Q sort is perfectly correlated with itself. PCA takes this original correlation matrix as the starting point, with all ones in the diagonal. If expanded again, the summary produced by (a complete) PCA would yield the original data, including the idiosyncrasies of individual sorts. CFA and PAF start from the same correlation matrix, but replace the diagonal with some measure of that sort’s degree of common-ness, or shared variance with the other Q sorts. PAF places the communality \\(h^2\\) of a variable (here: Q-sort) in the diagonal, the squared factor loadings for any given Q-sort across all the factors, indicating the percentage of total variance of a given Q-sort explained by all the factors. For CFA, Brown also suggests other diagonal substitutes, including estimated test-retest reliabilities which will be discussed later (1980: 211). Brown — as well as many orthodox Q methodologists — recommend CFA for its latent variable model because as a “closed model” PCA “assumes no error” (1980: 211). Brown is of course correct that a person is unlikely to reproduce the same Q-sort after a day or two exactly, though it appears implausible that her subjectivity on a general topic such as taxation and the economy would have perceptively changed in that time frame. It is less clear however, that such test-retest reliabilities — or any other substitution on the diagonal — would effectively identify anything that might be called error. As Brown is quick to point elsewhere, there is no external standard of one’s subjectivity, denying conventional notions of validity. If a Q-sort at any one point in time is operant subjectivity, how would we know that any difference to a later Q-sort would be in error? It may well be that such test-retest differences reflect spurious fluctuations in subjectivity, but any one snapshot of such variable subjectivity would still be accurate for that moment in time. We might want to consider such reliabilities as resolutions, when interpreting the factor scores, or when comparing Q-sorts after some “treatment” (as is the case here), but for now, we want a summary model that also explains this supposed “noise” in a person’s Q-sort. As such, we would want a summary model that also explains this supposedly “unreliable” component of a person’s Q-sort.157 A latent-variable model, more generally, would appear to fit awkwardly into Q methodology. Consider, by contrast, an exploratory factor analysis of burnout, a supposedly externally valid phenomenon that cannot be measured directly (example taken from Field and Miles (2012): 750). In such a study, we may legitimately be interested only in those variables that are highly correlated among burnout patients, such as motivation, sleeping patterns, self-worth and so on. If and to the extent that the sampling frame (burnout) is externally valid — a big if — we can draw a meaningful distinction between common and specific variance. If, for example, only some burnout patients report back pain, we may reasonably consider this to be specific variance related to another (skeletal-muscular) condition, and disregard it for the present model. In the correlation matrix of variables, we may want to replace the diagonal entry for the back pain variable with something less than one, because we do not require a model that also accounts for levels of back pain. The sampling frame “burnout patients” implies — however justifiably — a latent variable (burnout), that we expect behind all shared manifestations. Conversely, we would only want to model patterns of variable correlations that are widely shared by our burnout patients, and would therefore greatly “discount” idiosyncratic variables such as back pain. Recall, again, that in Q methodology, items are cases, and participants are variables. Q methodology has a case sampling frame, too, but these cases are items in Q: it is assumed that all items relate to taxation and the economy.158 It is not clear, however, what the equivalent to the back-pain scenario of a “stray” variable in Q might imply. It is possible that some participants (as the variables) will have less in common with everyone else than others. In this study the degree of common variance, provisionally measured as the average correlation per participant row (or columns) in the correlation matrix, ranges from 0.22 to 0.42. Applying the latent-variable logic of an ordinary, items-as-variables factor analysis to Q, we might then conclude that the relatively uncommon Q-sorters introduce idiosyncratic specific variance, and ought to be discounted in the model. At least in this study, such discounting does not make sense, because we are principally interested in all Q-sorters, qua being participating citizens. Recall that when studying burnout — but not back pain — there are many variables that you are not interested in, even though it may not be clear at the outset what these are. In contrast to a burnout study, there is no conceivable variable-participant that we would not be interested in. Health symptoms may — or may not — have a bearing on burnout, but all citizens do have a bearing on common understandings of taxation and the economy.159 A latent-variable generally seems at odds with the scientific study of human subjectivity. In contrast to many R-type exploratory factor analyses, per the abductive logic of Q, there is no preconception of what might delineate specific from common variance in substantive terms. To the extent that all Q sorters participate in the same realm of communicability, and share in the same concourse — as is assumed by the respective theories for any modern society — we are equally interested in maverick and mainstream Q sorts. In fact, to discount the poorly correlated Q-sorts would be to (slightly) stack the deck in favor of the one straightforwardly falsifiable hypothesis that q methodology admits of: namely, that viewpoints are, in fact shared (see later discussion on the number of factors).160 10.9.1.2 In(determinacy) Proponents of Centroid Factor Analysis (CFA) often cite the technique’s supposed indeterminacy in favor of the method. It might seem odd that a statistical procedure may be favored for not producing the same result, with the same data. This unfamiliar criteria might be best understood in the historical context of factor analysis, as Peter Schmolck recently suggested. Different methods of factor analysis and rotation were originally developed to summarize differences in human personality and intelligence. Some of these early factor analysts were convinced that alternative theories of personality and intelligence could be adjudicated purely based on the mathematical elegance of the extraction and rotation method that produced results in line with these theories. Against this backdrop, Stephenson’s insistence on a supposedly more flexible method such as Centroid Factor Analysis seems plausible. Convincingly, Stephenson (1953) points out that “it is difficult to accept any one kind of geometrical substructure, as, in principle, the only basis for inference” (1953: 41). It is, however, less clear what the suggested indeterminacy of CFA means, and how it might meaningfully enhance an analysis in practical terms. CFA is often associated with judgmental rotation and PCA with automatic procedures, such as varimax (Brown 1980: 33). While these extraction and rotation procedures may have evolved in historical tandems, there is no inherent logic tying them together. A CFA can be automatically rotated just as a PCA can be judgmentally rotated. In fact, as Peter Schmolck has pointed out, already back in Stephenson’s day, factor analysts pointed out that PCA, too, admits of an infinite number of rotations, from which one must be chosen for mathematical (varimax?) and/or theoretical (judgmental) reasons: “the factors found by the Hotelling method of principal components present the same necessity for rotation as those found by the Thurstone method of multiple factors” (McCloy, Metheny &amp; Knott 1938). The question of which rotation procedure to use then arises for every possible extraction method and will be further discussed later on. (Brown 1980: 211) seems suggests two additional reasons for CFA, though a clear and accessible statement of advantages other than the historically associated rotation procedures is hard to come by. Because CFA operates directly on the correlation matrix, diagonal entries can be easily replaced, as the researcher sees fit. Brown (1980) suggests various replacements on the diagonal of the correlation matrix, including communalities and test-retest reliabilities (1980: 211), though any choice of such replacements would appear to imply one or another latent variable model. At least if a latent variable model is deemed inapplicable, as is the case here, so is this rationale for using CFA. Fundamentally, CFA is indeterminant, because it involves reflecting of variables in the correlation matrix, a process that can be done in varying orders, and with varying levels of skill. Because CFA extracts factors by dividing row (or column) sums by the square root of the sum total of all correlations, it requires maximally positive correlation coefficients. To achieve that, negative-sum rows (and columns) are multiplied by \\(-1\\) (reflected), effectively mirroring that Q-sort around the center. The factor loadings of the respective rows (or columns) are later multiplied by \\(-1\\) again to restore the original orientation. The operation, done manually, is quite demanding, because an earlier reflection of a row, will affect other column sums and may require additional multiplications. Depending on the order in which variables are reflected, different factor loadings will result from the downstream extraction, with an infinite number of equally “correct” models. By contrast, a procedure like PCA will yield only one correct (unrotated) model: that which explains the greatest amount of variance. Oddly — for the proponents of CFA — such a model of maximum explained variance also exists theoretically for the centroid method, but is not identified by the procedure. It is this formal equality of different resulting models in CFA that supposedly invites judgmental rotation and strengthens the researcher’s interpretation. From Brown’s exposition, it remains unclear, however, how the reflection procedure — a very early step in the analysis — may conceivably be informed by a researcher’s substantive abductions, because these hunches would need to be in the form of more or less plausible ideal viewpoints, a summary that is not available until much later in the procedure. It is hard to imagine how even knowledgeable and skillful researchers might be substantively guided in this technical and preparatory step. It is entirely unclear what reflecting one column first over another might mean in terms which only become visible through that very procedure. In addition — though this would be remediable — most, if not all, recently published Q studies do rely on computer algorithms for extraction, including the reflection stage. While these algorithms may proceed differently, and, accordingly, produce different results, these differences are caused by the heuristics of programmers, not the abductions of researchers. In conclusion, the vicissitudes of Centroid appear less as Q epistemological necessity, than a product of its genesis. As (Brown 1980: 209) admits, CFA was developed in the era before electronic computing “largely because of its computational ease”, and “as only an approximation to the more refined methods such as principal axes” (emphases added). Keeping in mind the necessity for rotation, Centroid Factor Analysis can, at least for this study, be considered technically “obsolote” (Yates 1987, 1) and historical “dead freight” of Q methodology. Principal Components Analysis (PCA) then appears the most appropriate data reduction technique for this study, if not Q methodology in general. 10.9.2 Principal Components Analysis in Q Methodology Principal Components Analysis (PCA), originally developed by Pearson (1901) and (independently) by Hotelling (1936) Hotelling (1933), is a well-rehearsed technique with several good introductions (Child 2006, @Fabrigar-Wegener-2012, @Basilevsky-1994,Uslaner-1978, @Bartholomew-Steele-etal-2011, @Kim-Mueller-1978, @Cureton-DAgostino-1983, @Yates-1987,Mulaik-2009). There is, however, no published discussion of an application of PCA to Q methodology and its unfamiliar correlation matrix of people-variables. Here again, no reiteration of elementary multivariate statistics will be necessary (nor possible), but central concepts must be briefly rehashed in their application to Q data. Recall that exploratory factor analysis in Q serves as a data reduction technique, in our case from 18 individual Q-sorts by our participants (as variables) to a smaller number of shared, ideal-typical Q-sorts. More precisely, this process attempts to reduce the dimensionality of the original data, from 18 dimensions (one for each participant-variable) to a lower-dimensional space, while accounting for as much of the original Q sorts as possible. PCA accomplishes this by decomposing the dataset into orthogonal (that is, uncorrelated), eigenvectors. Because 18-dimensional spaces as are hard to wrap your head around, I illustrate how PCA operates on Q data using only selected pairs of Q-sorts: the most positively, most negatively and lowest correlated pair. The results of such an attempted data reduction from two to one dimension are emphatically not interesting in substantive terms (because there are more participants), but these subsets serve well to dissect the mathematical operations. PCA can be (equivalently) described in at least two formalism: linear and matrix algebra. Throughout the following examples, I rely mostly on the graphical, linear algebra but also briefly present the same operations in matrix form. 10.9.2.1 Eigenvalue Decomposition The central operation in PCA, eigenvalue decomposition, can be readily illustrated for 2-dimensional data using familiar plots. The below panels display the scatterplot of the original people-variables with overlayed eigenvectors (what might be called eigenplots) on the left, and the same data plotted against their respective eigenvectors as axes, with the original people-variables on top (known as biplots).161 The points in both plots are the item-cases, that is, the Q cards positioned according to their ranks by the two Q sorters.162 To remind readers that the data points are item-cases, not people-cases as in R-type correlations and PCA, some points are labeled. Beginning with the most highly correlated pair of Q sorts, eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]], the left-hand eigenplot in the below figure displays a cloud of points, scattered around an upward-sloping axis, as would be expected for a positive correlation. The first principal component (PC) of any data is the axis through this 2-dimensional cloud of points along which there is the most variance. This axis can also be imagined as the longest diameter of an ellipsoid fitted around the data points. Because the two sorters eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]], are positively correlated, this ellipsoid is oblong along a diameter with a positive slope.163 Each principal component consists of a pair of eigenvector and eigenvalue. The eigenvector describes the direction of the principal component, and the eigenvalue constitutes the length of that vector, illustrated with the small arrows plotted over the PC axes in the above figure. The same variance-summarizing can be illustrated using matrix algebra, as in formula (5). The relationship between the two sorters eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]] is here presented as a familiar correlation matrix, with a diagonal of 1. The eigenvector is simply that vector, which, multiplied with a scalar (the eigenvalue, actually), yields the original matrix, multiplied by the eigenvector. Somewhat trivially for a two-dimensional correlation matrix, this first eigenvector in matrix form merely extracts the direction (notice the slope 1 of the vector (.71, .71)!) and length (1.6) of the main diagonal, the first PC. $$ $$ Once the first PC is extracted, PCA continues to the next, orthogonal axis along which data points (here: Q cards as item-variables) have the second-largest variance. This process continues until there are no possible orthogonal axes left, which is the case when there are as many PCs as there were people-variables. In our exemplary pairs, the second PC is both easy to find and trivial: because there are only two dimensions to begin with (one for each Q-sorter in the pair), there is only one remaining orthogonal axis. Crucially, a complete PCA down to the last component does not, in fact, reduce dimensions. That is precisely the point of an orthogonal transformation: it merely re-organizes the data on a new set of axes, with axis sorted by the amount of variance they account for. The result can be seen on the right-hand side of the above figure: the same data is here plotted on the two PCs. The first PC, by definition, explains more of the variance in the data, and the points are consequently more spread out on this horizontal dimension than on the vertical dimension. Because the principal components are just linear combinations of the original axes, these people-variables, eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]], can also be added to the left-side biplot, by placing them according to their respective (unrotated) loadings on the two components (the eigenspace). These (unrotated) loadings indicate how much of the respective original people-variable is explained by the two PCs. In our example of complete extraction, by construction, the two PCs together explain all of the original people-variables.164 Consequently, multiplying the respective PC scores of the data points (statement-cases) with the loadings of the two original people-variables yields the original data. This equivalence of the two plots in the above figure is borne out on close inspection: the left-hand side is, in fact, merely a 135° rotation of the original data. 10.9.2.2 Operant Subjectivity as Vector Bundles Maybe more so than in R-methodological research, the language of linear algebra is well-suited to describe Q, and captures its epistemology with some surprising mathematical precision. Recall first, that an individual sorting act, of, say growth-empty under column 5 is conceived as operant subjectivity, that is, subjectivity as and through behavior. Rather than passively measuring some (scalar-type) magnitude of agreement with, or distance from a set of items, a Q sorter will impress her subjectivity onto the limited Q set of statements she is given by displacing it from the neutral origin of 0, from which the Q distribution “distends” to both extremes. An individual data point in Q (say, growth-empty under column 5) is therefore, quite literally, best understood as a one-dimensional vector associated with a statement: it has a magnitude (5) and a direction (right of 0), and physical work was done for this displacement. Recall further, that respondents express their subjectivity using a sample of Q items. Any one Q-sort is therefore a family of one-dimensional vector spaces, one for each sampled items. Topologically, operant subjectivity in general is then — loosely — defined as a vector bundle over some continuous (high-dimensional?) space of human communicability, that sum of things that could be said about any subject. A particular subject and associated condition of instruction are then a plane through that space, in this case, the concourse of things that could be said about taxation and the economy. The present Q study further reduces the vector bundle by sampling 77 statements from their respective bases. The eigenvectors and -values of PCA, illustrated in the above example, now extract — to the extent possible — common sets of movements of item-cases across the people-variables. Perplexingly, the Principal Components (precisely: their scores) are, themselves, vector bundles, that is, sets of displacements shared by several respondents. Loosely, but helpfully, the first PC in the above example can be imagined as a set of common displacement instructions shared between people-variables: move growth-empty from 0 to 5, move ability-2-pay from 0 to 2, and so on. Each individual Q sort, such as eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]] could be completely recreated using several sets (here, two PCs) of such instructions, each weighted by that respondents loading. Throughout these operations, a Principal Components Analysis (PCA) maintains the epistemological and ontological stance of operant subjectivity. Appropriately, it produces results that, as Stephenson demanded, can be quite literally read as “pure behavior”: PCA scores are shared bundles of vectors, telling researchers how to displace Q items from 0, according to particular viewpoint. 10.9.2.3 Data Reduction Readers unfamiliar with PCA may now wonder how the procedure can reduce, in addition to transform data, which is the key objective in the current analysis. In short, PCA allows us to reduce dimensions, because it re-organizes the data in a new set of orthogonal dimensions (PCs), ranked from most to least “descriptive” of the original data. Depending on the relative amounts of variance accounted for by subsequent PCs, “later” dimensions may be dropped, with relatively little information lost. This familiar process again takes on specific meaning in Q methodology. A comparison of the above eigenvalue extraction of the most positively correlated pair, eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]] with the least correlated pair, eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]], illustrates the process. The original, uncorrelated sorts by eigen.examples[[&quot;lowest&quot;, &quot;pairs&quot;]] can still be decomposed into eigenvectors, and transformed accordingly, but there is very little reason to drop one of the components, because both explain equal amounts, and about as much as the original case-variables. This relative “weakness” of the first eigenvector is also obvious in matrix form in formula (6): The first eigenvector is essentially the same, but the associated multiplying eigenvalue is only marginally above 1, meaning that it “explains” little more than the original case-variables.165 $$ $$ The ellipsoid of the eigenvector plot on the left-hand side appears quite close to circular; only marginally more variance can be projected on the first PC axis than any other axis. To be sure, the original vector bundles can still be reformulated in terms of eigenvalues, as in the biplot on the right-hand panel, but little is gained by such an operation. Instead of two separate sets of displacement instructions, one for each participant-variable, we now have two compound sets of such instructions, which both apply to the Q sorts, but with different weights (or loadings). For example, instead of a simple move growth-empty from 0 to 5 eigen.examples[[&quot;lowest&quot;,&quot;pairs&quot;]]’s sort might now be recreated by, say, multiplying the first PC instruction (move growth-empty from 0 to 6) by its loading (.5) and adding the second PC instruction (move growth-empty from 0 to -4), multiplied by its loading (-.5). No information is thus lost, but there is also no reason to drop any of the PCs as dimensions. Because they are uncorrelated, the sorts by eigen.examples[[&quot;lowest&quot;,&quot;pairs&quot;]] cannot be meaningfully compressed without loosing a lot of information. This futility of reducing the dimensionality of eigen.examples[[&quot;lowest&quot;,&quot;pairs&quot;]] is borne out in Q methodological terms. Their respective operant subjectivities are expressed in quite unrelated vector bundles; these could be eigenvector-decomposed into two vector bundles, but both of these synthetic instruction tests would be alien to the real world, and likely unintelligible. To further illustrate, consider the familiar LEGO analogy. Suppose that LEGO cars and houses share no common assembly instructions. Still, imagine these two building manuals would be bunched up in two new, synthetic manuals, where, say, the respective first lines read “take a ramp-shaped object and move it up 4” and “take a ramp-shaped object and move it down 4”. The meta-manual for the house would then advise you to do what the first manual said, plus the opposite of what the second manual said. If you follow the instructions precisely, you will still place the roofing tile at the top of the house, where it belongs, so strictly speaking, no information would be lost. Obviously, though, the separate synthetic building manuals do not, in themselves, describe any real existing objects, and do not invite a design critique (or otherwise). The same holds for a Q-methodological interpretation: a synthetic vector bundle that is notat least roughly expressive of actual Q sorts is emphatically not subjectivity turned behavior, and allows no abductive interpretation thereof, because there is no such underlying behavior to begin with, just like there is no “car-house” (granted, there are motorhomes). By contrast, consider the below, spurious eigenvalue decomposition of eigen.examples[[&quot;identity&quot;,&quot;pairs&quot;]] with herself in linear and matrix algebra notation. $$ $$ Such a perfect (1.00) correlation is unlikely to exist in real Q studies, but serves as an instructive boundary case. In the eigenplot on the left, all (overplotted) items form, by definition, a perfect straight line. An ellipsoid shaped around this data would be infinitesimally narrow, extending along all points on that line. There is only one eigenvector (the second one has a length, or eigenvalue, of 0). Once transposed into the biplot on the right-hand side, it becomes obvious that the second PC can be dropped as a dimension: the data only vary horizontally, and no information is lost. Circularly, because these two Q sorts are identical, they can be created from one synthetic set of displacement instructions, which we might call eigen.examples[[&quot;identity&quot;,&quot;pairs&quot;]]. In terms of the LEGO analogy consider a scenario, in which two supposedly distinct building manuals yielded the exact same LEGO car –– you might conclude that the two separate instructions were created in error, and that you might as well keep only one of them. Notice further that the “synthetic” vector bundle describing the construction of both Q sorts, is, in fact, correlated (perfectly!) with real-existing operant subjectivity. In this boundary case, the scores for the first Principal Component, are, in fact, not synthetic at all: they are simply eigen.examples[[&quot;identity&quot;,&quot;pairs&quot;]]’s sorting vectors. In real world PC analysis of Q data, as is the objective here, matters are a great deal more nuanced, because correlations are often neither perfect, nor completely absent. Recall the above figure on the most strongly correlated pair, eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]] for a more realistic example. In this scenario, it might be acceptable to drop the second PC as a dimension, and call it the eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]]-factor. This would be a lossy data reduction; the variance spread on the second PC (vertical axis in the right-hand side biplot) is lost. This second PC accounts for the difference between the two Q sorts, as illustrated by their case-variable arrows pointing in opposite dimensions on this vertical dimension. By dropping the second PC, we thereby preserve and interpret only eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]]’s shared subjectivities, or viewpoints –– exactly as intended here. Consider a LEGO-built car and train as analogous illustrations; these objects share a decent amount of features (both have wheels, headlamps, windshields, etc.) but they also differ tremendously (length, weight, undercarriage). Dropping the second PC, only a first PC of shared characteristics would remain, a set of vector bundles that might be interpreted as a rolling-thing factor, or something similar. This may well be a gross oversimplification, but notice that both a rolling-thing and a eigen.examples[[&quot;positive&quot;,&quot;pairs&quot;]]-factor, while synthetic, roughly describe observed, operant subjectivity, and may be amenable to some interpretation (“it’s designed to move”, “it’s a pretty leftist outlook”, respectively.) Of course, there is not much to reduce two-dimensional data, of pairs of LEGO objects or Q-sorts, as in the examples above. The need to reduce dimensions becomes pressing only when the original dimensionality is overwhelming for interpretation, as is the case for the 18-dimensional space of Q-sorts to be analyzed here. That is the decision we turn to next: how many dimensions to drop, and why. 10.9.3 Number of Factors In choosing a number of factors (or PCs, to be precise) for a Q methodological analysis, you can err both ways. Retaining too many factors leaves you with spurious viewpoints which, at best, are only marginally more expressive than an individual Q sort, and, at worst, spurious amalgams of quite different Q sorts, all mixed together, as in the analogous “car-house” building manual. Retaining too few factors, at best, you needlessly loose nuance in people’s viewpoints, and, at worst, you may disregard people’s subjectivity by (through rotation) subsuming them under overly broad factors. Much of the Q literature appears to treat this choice as methodological issue, to be decided based on their downstream merit for factor interpretation, and generally gives the alternative statistical criteria short shrift. At least in the context of this study on changing operant subjectivity on taxation and the economy, the number of factors is primarily the first empirical concern in the chain of analyses. Varying shared viewpoints on taxation and the economy are primarily a positive, even falsifiable question, notwithstanding the need for downstream abductive interpretation. Consider, for example, a hypothetical Q sort of items written in a language that the participants did not understand. In that scenario, we would assume items to be sorted randomly, and would clearly expect no shared viewpoints. In fact, some respondents noted during the before Q-sort that they did not understand some, or even many of the items. Arguably, a Q-set on taxation and the economy is thus somewhere along the continuum between intelligible and “foreign language”. The absence of any shared viewpoints are thus a null-hypothesis of sorts, applicable even though the analysis is ultimately geared towards an interpretation. We therefore need to look at competing criteria for the number of factors to be extracted in some detail. 10.9.3.1 Criteria Based on Eigenvalues Eigenvalues and derivative statistics, summarized in the below figure, give a readily interpretable standard for deciding the number of factors: they summarize the descriptive power of each successively extracted eigenvector. Because there are as many eigenvalues as there are people-variables in a Q study, and because eigenvalues sum to that number of people-variables, they can be interpreted in terms of constituent Q-sorts: the highest eigenvalue accounts for as much as max(howmany$before$eigenvalues$Unadjusted) Q sorts. Recall that, conversely, a Q study with all zero-correlated sorts would yield eigenvalues of 1: every extracted principal component would explain exactly as much variance as one original Q sort — no data reduction would be possible. A threshold eigenvalue of 1, otherwise known as the Kaiser-Gutmann rule, (Kaiser 1960, @Gutmann-1954) thus makes sense for a Q study, too. Shared subjectivity, at least, should be more expressive than any one Q sort. Eigenvalues can be easily translated into their successive contribution to the overall \\(R^2\\) by dividing them by the number of people-variables in the study. The first and highest principal component accounts for a dominant max(howmany$before$eigenvalues$R2) share of the variance, followed by distant second and third principal component. By the Kaiser-Gutmann rule, also plotted as a red line in the below figure, we may retain up to howmany$before$summary[&quot;KaiserGuttmann&quot;, &quot;NFactors&quot;] factors. In addition to the level, we may consider the change in successive eigenvalues, as additional components are retained. Catell (1966) has suggested a visual criterion based on the above scree plot to guide the decision on how many factors to extract. According to Catell, factors should be retained up to an “elbow” point, beyond which successive components drop gently in (unadjusted) eigenvalue, akin to a scree field at the foot of a steep mountain. A scree test often involves some subjective judgement, especially when the eigenvalue curve has several elbow points, or falls gently over all components. In this case, components beyond 3 can probably be considered “scree” to be safely dropped from further analysis, though the sharpest turn occurs already at 2. Under the scree test, either 2 or 3 factors should be retained for further analysis. The Kaiser-Guttman and Catell criteria both rely on “raw”, unadjusted eigenvalues. While this makes intuitive sense, it risks overretention, especially when sample sizes of item-cases are relatively low, as is always the case in Q studies. In small samples, some correlations may persist due to “sampling error and least-squares ‘bias’” (Horn 1965: 180), where in larger samples, these spurious similarities would likely cancel out. We can readily imagine how this might play out in a Q study: even if the item-cases were in a foreign language, because they are so relatively few, given a sufficiently large number of people-variables, some Q sorts will likely share some variance by random chance. From these fluke correlations, Q methodologists might then extract entirely spurious factors, and arrive at interpretations however grounded in their abductive faculties, but unsupported by facts. To avoid such overretention, John Horn has suggested a procedure called “parallel analysis”. The procedure, here based on an implementation in R by Dinno (2012), repeatedly extracts factors from random (thereby uncorrelated) data, with the same range and rank as the observed Q study..166 As suggested by Glorfeld (1995), the Dinno’s parallel analysis calculates not only the mean of randomly extracted eigenvalues, but, using Monte Carlo estimations, calculates biases at arbitrary centiles, here set to siglevel = 0.05. The random eigenvalues reported in the above table and plot are the limit below which 95% of all eigenvalues from random data fall. The adjusted eigenvalues are the observed eigenvalues, corrected for the random component. Using the familiar LEGO analogy, unadjusted eigenvalues as “magnitudes of overlap” between different brick constellations, may express similarities between structures which arose out of random chance. Imagine, for example, a set of LEGO bricks placed into a washing machine for a spin cycle.167 The bricks may well assemble into partly similar structures merely by random chance, such as a stack of colored bricks forming a seemingly meaningful national flag. Parallel analysis estimates the overlap between different brick structures that may, in a 95% percentile “worst case scenario” result from random chance only and then subtracts such a possible random component from any supposed overlap between, say, car-like and house-like objects assembled by real builders. Applying Horn’s parallel analysis and Glorfeld’s extension to Q methodology, we can be 95% certain only that the first two factors are not a product of random chance and explain more than any one constituent Q-sort. Parallel analysis posits a fairly conservative standard characteristic for positive research: at the 95th percentile — as opposed to the mean — it sharply favors false negatives over false positives. There may well be real factors beyond the 2nd principal component, but unless we can be at least 95% sure of their existence, we assume them to be spurious.168 This strict standard is in line with mainstream empirical research, and serves well to falsify shared subjectivity on taxation and the economy: under a positivist standard, it may be preferable not to pollute scientific debate with a result unless proven robust by a conventional probability (95%). 10.9.3.2 Eigenvalues and Q Methodology Some leading Q methodologists are, perhaps unsurprisingly, skeptical about statistical criteria for factor retention in general, and eigenvalue-based thresholds in particular. Convincingly, Brown reminds readers that “eigenvalues and total variance are relatively meaningless in Q-technique studies as they depend to too great extent on the arbitrary number of persons included in the study who happen to be of one factor type, rather than another” (Brown 1980: 233). Notice that in Q studies, both the item-cases and the people-variables are not randomly sampled. The Q-set of item-cases, as discussed before, cannot be randomly sampled because there is no theoretically enumerable universe of the concourse. The P-set of people-variables may theoretically be randomly sampled, but is usually not, and will arguably always remain self-selected for deliberation, as is the case here. In any event, because P-sets are variables, they carry no presumption of representation: much like in an R-methodological study, you include variables because they may be interesting, not because they are a random subset of all possible variables. Conversely, a set of only very few correlated variables does not make the resulting factor any less interesting, if we can be sure that the relationship is not a product of random chance. This is emphatically so for Q methodology, which, despite the appearance of clustering people is not designed to measure the relative prevalence of viewpoints, but their substance.169 Variable selection and its effects on factor and other multivariate analyses are not peculiar to Q methodology; they affect R-type research in similar ways. Contrary to R-technique, however, Q methodology — at least in its current incarnations — does not have the luxury of increasing the sample size of item-cases to filter out random associations: people can be expected to sort only a limited number of items, with the current nrow(q_sorts) already at the upper bound. This leaves eigenvalue-based criteria, and especially the stringent parallel analysis in a seemingly awkward spot within Q methodology: under their veneer of scientific objectivity lies the arbitrariness of any particular P-set, in this case, the participants at the first CiviCon Citizen Conference on Taxation. With different, especially with more politically diverse participants, more, and different factors may have arisen. Entirely compatible with this caveat, eigenvalue-based criteria and parallel analysis do, however, constitute sufficient — not merely necessary — conditions to identify shared subjectivities. A factor retained under Horn’s stringent criteria from any one, however arbitrary P set, will at the specified 95% likelihood persist through future Q studies with different participants. Even if the factor could not be found on a future P set, using the same Q set, we could be certain that, if factor analyzed together, the original factors would again yield similar eigenvalues. In fact, because Q methodology is not concerned with the prevalence of factors, the original factors would, by construction, persist even in an arbitrarily large and diverse P set. On the one hand, the covariance explained by the original factors may, in an expanded Q study, be subsumed by other, more refined factors. But on the other hand, in the unlikely scenario that even among thousands of other Q sorts, they were unique, the original factors would still be retained with adjusted eigenvalues above one. One way or another, the patterns of covariance robustly identified on the basis of eigenvalues and parallel analysis will remain recognizable in superseding P sets. Conversely, would-be factors dropped from the analysis under eigenvalue-based criteria may well fulfill some necessary conditions for shared subjectivity, such as making abductive “sense”, but we cannot be certain that these patterns will persist in a larger, or more diverse, P-set. To err on the side of caution, and to drop factors unless proven robust over alternate P-sets might be appropriate, especially considering the substantial leeway accorded to Q methodologists during downstream judgmental rotation and abductive interpretation. Barring any innovations allowing a (far) greater number of item-cases in Q, reliable sensitivity can only be increased by adding diverse people-variables to the P-set. Recall that eigenvalues and parallel analysis — though not scree tests — posit absolute, not relative thresholds: in line with Q methodology, it does not matter how many people-variables contribute to a factor, only that the corresponding (discounted) eigenvalue is greater than one. In a sufficiently large and diverse P-set, even very small groups of non-randomly While superficially alien to Q methodology, where participants constitute people-variables, greatly increasing and diversifying the While the variance explained by the original factors and shared by those relatively few, and arbitrary participants, may be subsumed under broader, or more precise factors, the same relatio Eigenvalue-based criteria and parallel analysis, do, however, remain not only is the Q-set of item-cases not randomly sampled (because there is no theoretically enumerable universe of for the concourse Turner 1998 as cited in Hayton 202 noted that &gt; due to the interdependent nature of eigenvalues, the presence of a large first factor in a PA will reduce the size of noise eigenvalues. The consequence is that in certain situations, PA can underfactor, which is potentially more serious than overfactoring. The impact of this limitation is most serious for smaller sample sizes, where there is a high correlation between factors, or where a second factor is based on a relatively small number of items. (!) 10.9.3.3 Criteria Specific to Q Methodology 10.9.3.4 Criteria Based on Communalities and Residual Correlations Braswell in email points out that communalities are also affected by non-random sampling and in the same way, and he is correct. Horn is the method of CONSENSUS says Dinno scree plot is from Catell 1966b Wilson &amp; Copper 2008: 867 (as cited in Watts/Stenner) define Paran: &gt; it “shows how big the first, second, third etc. eigenvalues typically are (…) when in reality there are no factors present in the data” Notice the problem with Brown’s loadings-based criteria: these depend on the rotation, and the rotation, in turn, can be decided only after the number of factors to be rotated on. Kaiser-Guttman criterion is EV &gt; 1, Guttman 1954, Kaiser 1960, 1970) Below that accounts for less variance than any one Q sort “Magic number 7” is Brown 1980: 223 n/6-8 is Watts Simon Loc 2546 Humphrey’s Rule: “factor siginificant if the cross-product of its two highest loadings exceeds twice the standard error” (Brown 1980: 223) unknown / Brown 1980: 222-223 &gt; 2 sig loading people The basic logic behind parallel analysis is to improve upon the eigenvalue &gt; 1 (principal component analysis) or eigenvalue &gt; 0 (common factor analysis), by (1) recognizing that in finite data, some eigenvalues will be greater than 1 or less than 1 simply due to chance because the correlation matrix will not be a perfect identity matrix, and (2) correcting for this “sampling and least squares bias” by averaging the each of the p eigenvalues from “many” uncorrelated data sets of the same n and p as your observed data, and retaining only those observed eigenvalues greater than the corresponding mean of random data eigenvalues. In plain english: it is the likelihood that it is less than … etc. Just what bRown means with this is characteristically unclear: But as will be shown, insignificant factors frequently contain small amounts of systematic variance that can help in improving the loadings on a major factor, in the same way that otherwise useless autos may contain parts that can be pillaged in order to make a good car even better. After rotation, insigificant residual factors are merely discarded. – BRown 1980Ö: 223 comment on communality later on; it stays the same no matter how the stuff is rotated Kaiser 1960 is just all eigenvalues greater than .1 Kaiser’s generally overestimates (Field Miles discovering stats 762) Kaiser’s rule: more than EV1 scree plot: where is the elbow Chi-Square: non-significant result Horn’s parallel analysis Techniques most commonly used include 1) Extracting factors until the chi square of the residual matrix is not significant. 2) Extracting factors until the change in chi square from factor n to factor n+1 is not significant. 3) Extracting factors until the eigen values of the real data are less than the corresponding 27 eigen values of a random data set of the same size (parallel analysis) fa.parallel (Horn, 1965). 4) Plotting the magnitude of the successive eigen values and applying the scree test (a sudden drop in eigen values analogous to the change in slope seen when scrambling up the talus slope of a mountain and approaching the rock face (Cattell, 1966). 5) Extracting factors as long as they are interpretable. 6) Using the Very Structure Criterion (vss) (Revelle and Rocklin, 1979). 7) Using Wayne Velicer’s Minimum Average Partial (MAP) criterion (Velicer, 1976). 8) Extracting principal components until the eigen value &lt; 1. Each of the procedures has its advantages and disadvantages. Using either the chi square test or the change in square test is, of course, sensitive to the number of subjects and leads to the nonsensical condition that if one wants to find many factors, one simply runs more subjects. Parallel analysis is partially sensitive to sample size in that for large samples the eigen values of random factors will be very small. The scree test is quite appealing but can lead to differences of interpretation as to when the scree“breaks”. Extracting interpretable factors means that the number of factors reflects the investigators creativity more than the data. vss, while very simple to understand, will not work very well if the data are very factorially complex. (Simulations suggests it will work fine if the complexities of some of the items are no more than 2). The eigen value of 1 rule, although the default for many programs, seems to be a rough way of dividing the number of variables by 3 and is probably the worst of all criteria. 10.9.4 Other tests Some of these criteria are general others are specific to Q What about bartlett test? test of sphericity? test of multicolinnearity? Bartlett’s test of sphericity tests whether it is, in fact, worthwhile at all to extract factors. (though this may not be necessary for PCA?!) 10.9.4.1 Conclusion: The Degree of Shared Viewpoints Before CiviCon 10.9.5 Principal Components Extraction 10.9.6 What Rotation? Brown calls varimax etc “atheoretical rotation” (Brown 1980: 227 ) - nice! great example / illustration with the stuff on the beach comes from Brown 1980: 222 Judgmental / Manual Automatic (varimax, equamax etc.) Varimax - minimizes number of variables with high (or low) loadings on a factor, makes it possible to identifiy a variable with a factor Quartimax - minimizes the number of factors needed to explain each variable. Tend to generate a general factor on which most variables load with med to high values - not helpful for resurch Equimax - combination of above. What is a good factor structure? Should have more than .6 total variance explained (barely so!) 10.10 Loadings 10.11 Factor scores 10.11.1 Flagging Peter SChmolck: &gt; The automatic flagging method in PQMethod often misunderstood as a well-founded, decisive statistical-q-methodological solution for the problem. However, it originates neither from William Stephenson nor Steven Brown. The reason for the frequent misunderstanding might lie in bad English. With the ‘pre-’ prefix I wanted to say, ‘preliminary’, before the ‘real’ work has to be done. I later noticed that the prefix never appeared when people referred to this method of selecting the defining sorts. This appears to be a good opportunity for disclosing how the algorithm came into PQMethod. In John Atkinson’s original version of the software, the only possible method of manual flagging was rather cumbersome, one had to enter the series of sort numbers to be flagged for every factor (this method is still available in PQMethod as the ‘old’ version). In the course of checking and further developing the software I had to permanently repeat running the program from start to end for testing if the last changes worked as intended. Going through the flagging stage each time was quite annoying. I therefore needed a solution for automatic processing. From my times as a research assistant in scale construction projects I had a vague recollection how so-called ‘marker items’ were found for factor interpretation and item selection for the scales. It was the so-called Fuerntratt Criterion: To be eligible as a marker item for a certain factor, the variance explained by that factor must exceed the total amount of variance explained by the other factors. This seemed to me a sound method for securing that a sort cannot be definer for more than one factor. Since I was aware that Q people have a strong affection for significance formulas, I added the (lenient) p&lt;.05 ‘significance’ requirement. The latter, BTW, has nearly never an additional effect when the first criterion is met. 10.11.2 Scoring 10.11.2.1 QDC 10.11.3 What visualization 10.12 Interpretation 10.13 Discussion Using data gathered before and after the 2014 CiviCon Citizen Conference on taxation, hosted by the author, I suggest several approaches to measure deliberative quality using Q methodology. At the week-long conference, 16 diverse, self-selected citizens were tasked to design a tax system “from scratch”, choosing among possible combinations of base schedule. They participated in learning phases, deliberated in moderated small group and plenary sessions, met with experts and held a concluding press conference. Before and after the conference, citizens sorted 79 statements on taxation and economics according to their subjective viewpoint. Following traditional Q methodology, sorts were factor analyzed to extract ideal-typical viewpoints shared by participants. Before the conference, citizens expressed resentful, radical and moderate viewpoints, including some apparent inconsistencies between beliefs, values and preferences on taxation. After the conference, citizens shared decommodifying, pragmatic and critical viewpoints and displayed a simpler, lower-dimensional structuration of viewpoints. Such results are meaningful and encouraging for the use of Q in deliberation, but fall short of testing a treatment effect, because before- and after-factor models are extracted and rotated separately. To tease out the effects of deliberation, I therefore also apply a newly developed 2-dimensional Q analysis (Q-2D), based on three-mode Principal Components Analysis (nPCA) by Tucker and Kroonenberg. On the one hand, results indicate that, after considerable computation and graphical transformation, the effects of deliberation can be meaningfully interpreted using Q2D. Participation in the Citizen Conference appears to increase substantive consistency of political viewpoints, seemingly meeting a demanding standard for deliberative democracy. On the other hand, a unified longitudinal factor model may also unreasonably restrict the change of subjectivity during deliberation. Analyzed separately, there is some evidence that before and after factor models are, in fact, incongruent. Alternative analytical approaches to Q data in empirical deliberative democracy research are discussed. Political subjectivity has been conceptualized by Dryzek and Niemeyer to involve beliefs and values, as well as preferences. These are conceptually independent dimensions and the very status of some assertion as either value or belief is often contested, and thereby part of political subjectivity proper. Here, too, Q2D can significantly improve upon conventionally analyzed, repeated Q studies by yielding jointly extracted and rotated, two-dimensional ideal viewpoints vis-a-vis the same items, readily interpretable in terms of meta-consensus and intersubjective rationality. Algorithms and visualizations in open-source R-code are presented, conceptual limitations of Q2D are discussed and compared to alternative analytical approaches to Q data in empirical deliberative democracy research. 10.14 Factor changes References "],
["structuration.html", "Chapter 11 Preference Structuration", " Chapter 11 Preference Structuration "],
["conclusion.html", "Chapter 12 Conclusion 12.1 Limitations 12.2 Better Deliberation 12.3 Further Research", " Chapter 12 Conclusion Democracy is not being, it is becoming. It is easily lost, but never finally won. — William Hastie (1904–1976) 12.1 Limitations 12.2 Better Deliberation 12.3 Further Research "],
["epilogue.html", "Chapter 13 Epilogue 13.1 The Great Unravelling: Why we Must Now Embrace Complexity to Avoid a Return to Fascism 13.2 Second Best", " Chapter 13 Epilogue Hope is definitely not the same as optimism. It is not the conviction that something will turn out well, but the certainty that something makes sense, regardless of how it turns out. — Václav Havel (1985) 13.1 The Great Unravelling: Why we Must Now Embrace Complexity to Avoid a Return to Fascism With Donald Trump elected to the most powerful office on the planet, Brexit passed, rightwing populists banging on the gates of parliaments and government in France, Germany and elsewhere, we find ourselves in the midst of the greatest assault on liberal democracy since World War II. Instead of the heralded “End of History” (Fukuyama 1992), we must now consider a broad civilisational regression, and even the return of large-scale violence as real and frightening possibilities. Calmer minds may find this alarmist; I hope they are right. But when the downside risks are so enormous, the precautionary principle almost dictates alarmism. When the stakes are this high, we’d better be safe than sorry. Marginal polling errors notwithstanding, the return of demogoguery and populism isn’t exactly a sudden shock, but it calls us all to urgent action. But first, we need a precise diagnosis of what is going on. There are already many thoroughly argued explanations out there, including of a global white identity backlash (Ivarsflaten 2008, @cederman-wimmer-etal-2010), widening income inequality and postindustrial decay, cultural alienation, education and age. Perhaps crucially, Trump and other right-wing populist voters may be driven by loss aversion concerning their material or (however illegitimate) cultural status, compared to some real or imagined past, as James Surowiecki has recently pointed out in the New Yorker (compare Levy 2003, @KahnemanTversky1979). As always, the preliminary empirical evidence (at least from US exit polls) seems more complicated, and many of these correlations are also multicollinear. To these, I add a new (if not especially original) hypothesis: Voters turn to right-wing populists, because they seek to avoid the moral and ethnical cognitive load that the abstractions, complexity and interdependencies of the modern world place on them. And no, this is not just another way of calling Trump-supporters “dumb”. We may all be the 99% ignorant. Consider, for a first illustration, anthropogenic climate change. If you believe climate change is a fact, and if you value – as is increasingly demanded, and rightfully so – all human beings equally, but also fly across the atlantic to attend a conference of questionable value, as I recently did, then you face a pesky moral-cognitive dissonance (compare Festinger 1957). More than a mere dissonance, this behavior challenges my comfortable identity as a biking, vegetarian, and generally blameless treehugger: turns out, I did value the cosmopolitan ego-boost of an international conference more than the people of Bangladesh, my future childrens future and the rest of the planet. Straightforwardly, I can either own up to this inconsistency (which sucks), or I can change my behavior (which also sucks). There are, alas, two other ways to resolve the dissonance: I can discredit the fact of global warming (“the Jury’s still out!”), question the authority of climate scientists (“the climate lobby is corrupted!”), or even reject the entire abstraction of a global climate system out of hand (“but this winter, there was a lot of snow!”). I can discount the value of other human beings, by straightforward jingoism (“America first!”), subtle privilege (“Flying is part of our transatlantic culture!”) or othering racism (“these people only live in huts anyway!”) By forming either, or any combination of these preferences over these beliefs (Caplan 2007), I’ve solved my problem. Notice that this is quite a taxing problem, it’s not just liberal handwringing: to really consider the lives of people I have never met, or lives in a distant future, and to relate that to an exciting trip is a mighty ask for a cognitive and moral miser like myself. To be fair, as someone who has spent all of his adult life arguing things, I can come up with somewhat cleverer-sounding delusions. I could argue that I only buy marginal seats at cheap prices, and that therefore, the plane would have flown anyway, and only the marginal CO2e cost of adding my body and bags to the wide-body jet were incurred. Or I can point out that transatlantic flights have no carbon-free substitute (for now), and that if only all other parts of the economy were carbon neutral, then flying would not be a big problem. In a way, Elon Musk is my Donald Trump: A self-made entrepreneur whose every outlandish utopia I want to believe in, because it helps me resolve these pesky inconsistencies. I have developed this hypothesis in my exploratory dissertation work on public understandings of taxation, as well as personal conversations with AfD-voters and Trump-supporters. Over the past 10 years, I have studied tax policy and made it my (hopefully) 1% non-ignorant domain. As it turns out, taxation is marred in abstractions and complex interdependencies, which force you to re-evaluate your conceptions of justice and your own (financial) biography at every juncture. For example, if you want a progressive schedule (rich people paying more as a percentage), and you do not want to “punish” people for marrying, then you will have to accept that the tax burden of a couple depends on the distribution of income between the spouses (stay-home moms with high-earning husbands will do well). This is not an opinion, it is a logical fact: of the above three oft-desired outcomes, you can only have two (Moffitt 2003: 124, @Dalsgaard2005: 29). As a voter and as a politician, it is a whole lot easier to disregard this unavoidable abstraction, and to simply demand, or promise a tax system which would support all “working-class families equally”, even though that is a fairly meaningless statement in this context. Most abstractions of tax do not even take the form of a trilemma, and are still cognitively and morally vexing. For example, most OECD tax regimes do not tax imputed rents from owner-occupied housing (and the US slaps on the infamous home mortgage interest deduction). To treat renters and owners equally, as horizontal tax equity would dictate, we would have to (partly) tax house owners on the rents they would have to pay for their own houses, because this is a capital income, and the otherwise equal renter will have to pay the same tax on her capital income, from, say, a mutual fund. I would wager that, in the abstract, most people would agree that renters and owners should be treated equally. But faced with the consequence of imputed-rent taxation, many of the (self-selected, small N) citizens I worked with balked at the idea. They came up with with (very benign) forms of the above-mentioned forms of dissonance resolution: they either rejected the abstraction (“but it’s my house!”), or they othered-away the equal rights of renters (“they could have bought a house, it’s a lot of work!”). I chose these illustrations, because they are the easiest to explain, and the least controversial. There are other, far more complicated and more consequential abstractions of taxation, including the adequate base of taxation, the incidence of (corporate) taxes and welfare effects. What does any of this have to do with President Trump and his supporters? It has everything to do with rampant inequality, low growth, high debt, post-industrial decay, poor jobs and a government structurally unresponsive to resulting voter demands: the whole post-democratic malaise (Crouch 2004). This is not some inconsequential, wonky distinction. This fine print makes or breaks the social contract. Over the past decades, this foundation of the social contract has fractured. In fact, if you were to purposely design a tax system to protect rich people and heirs while squeezing future generations, poor and middle earners, you could hardly design a more effective regime then the current income tax and value-added tax system (compare McCaffery 2005). The tax system really is rigged, though not in the ways that Donald Trump or Bernie Sanders might think: it simply presents the legislatures with prohibitively and needlessly unattractive tradeoffs, such as between equity and efficiency. Predictably, faced with the choice of a rock or a hard place, democratic institutions become unresponsive, and voters feel increasingly disenfranchised. This might look like a grand conspiracy, but I don’t think it is one. Instead, I fear, it is simply a slowly unfolding trainwreck of fiscal history. Some people may – in the medium term – benefit from a fractured social contract, though any potential sabotage may yet backfire (violence), and I am skeptical whether these beneficiaries ever successfully colluded. The bigger blame lies with us: the 99% ignorant. The data from my exploratory (small-N) dissertation study show that ordinary citizens hold staggeringly inconsistent beliefs and values on taxation, which can, at least partly, be explained by their cognitive ease and comfort and as an outlet for resentment against the economic system. (I am aware that this is not a deductive proof; it is an abductive hunch, but also see McCaffery and Baron (2006), Sausgruber and Tyran (2011)). Absent a deeper understanding, or at least appreciation of complexity, some of the most promising suggestions for reform will never be discussed in such an impoverished political process: a Negative Income Tax (Friedman 1962), a Progressive Consumption Tax (Seidman 1997), a Land Value Tax (George and Busey 1879), or even a proper Inheritance and/or Property Tax are all unfamiliar, complex and demand a recognition of our mutual interdependence. In 2008, a leading party operative for the Social Democratic Party of Germany (SPD) told me that frankly, a Progressive Consumption Tax was a nice idea, but that it would be too vulnerable to populist attacks to promote. This is the vicious cycle we find ourselves in: a dysfunctional policy makes democratic institutions unresponsive, which frustrates and impoverishes voters, who turn to increasingly populist candidates, who degrade the political discourse, and make it impossible to pass better policy. This is a democracy, eating itself alive. Complex policy, of course, is not a recent phenomenon, and voters decades ago were probably not any better equipped to deal with the complexity of their day. However, background conditions have shifted dramatically over the last decades, exacerbating the mismatch. On the one hand, the factual complexity has grown, mutual interdependencies have deepened and the moral horizon has widened. This process of social integration is unequivocally good, perhaps such reaping of positive-sum cooperation is even the telos of human cultural evolution. On the one hand, voters have also learned to demand more of government, and traditional political elites (smoke-filled backrooms) and institutions (party affiliation) have eroded. This process of emancipation and individualisation is also, by and large, a good thing. These two processes together do not in itself “overload government”, as conservative critics once feared. They do, however, demand a lot from voters. They also open up a tremendeous opportunity on the supply side of politics to engage in “discursive dumping”, to dumb down and discredit complexity and interdependence, to offer voters (false) relief from their cognitive and moral loads. After an initial successful fix, and the following hangover, as voter demands continued to be frustrated, politicians slowly increased the dosage – from “welfare queen” to “Joe the Plumber” to “death tax”, and, ultimately, “Make America Great Again”. This was neither a fast, nor continuous process, but rather, a slow and self-reinforcing chipping away at communicative rationality, one sound bite at a time, one delusional simplification after another. This is not yet a rigorously conceptualised hypothesis, let alone a tested one. As sociological theory goes, it is also pretty banal. 13.2 Second Best 13.2.0.1 Second Best. Today, maybe one of the clearest Panglossian pronouncements comes under the imposing heading of a Theory of Second Best, originally formulated by (Lancaster 1956). As so many great economic ideas, this one has strayed far from its original form, and interbred with ideology to father many illegitimate — and sometimes deformed — offspring. In its initial, rigorous formulation, the Theory of Second Best showed formally that if — as seems likely — some conditions for the pareto optimality of markets cannot be meet, it might enhance efficiency to allow additional, possibly offsetting deviations from perfect competition elsewhere in the economy. Rather than to fight all market failures everywhere in isolation (“piecemeal welfare economics”, (1956, 11), Lancaster (1956) suggested that from a general equilibrium view, there may be less demanding necessary conditions that could pareto improve the economy, in addition to the often implausible, sufficient conditions of perfect competition (1956, 17). (The Economist 2007) explains it beautifully thus: if your optimal cookie recipe requires chocolate chips and coconut flakes, but you cannot find the chocolate chips, your (second) best bet may be to bake gingersnap, rather than chocolate chip cookies without chocolate chips. This is the kind of devilish complexity that I allow myself to ignore here, but that policy makers have to consider: if, for example, research and development are so prohibitively expensive that we absolutely cannot profitably have more than one manufacturer of wide-body aircraft, our (second) best policy may indeed be to stray further from neoclassical doctrine, and to keep subsidizing The Boeing Company and Airbus SES, so that we can have at least have ourselves a decent, somewhat competitive, duopoly. There is nothing Panglossian about such hard choices because, it is, in fact, materially impossible to develop dozens of competing wide-body designs. Because for the social sciences — as for moral philosophy — ought implies can, the second-best of the subsidized Boeing/Airbus duopoly also needs no social scientific explanation. This really is collateral damage to a worthy cause. After (1956), however, the Theory of Second Best as slowly morphed into a general skepticism of state intervention, it is now name-dropping “proof” of its ipso-facto futility and serves as welcome absolution for the demise of the mixed economy. Wolf (1987, 1979), for example, argued that because governments fail, just as markets do, the second-best response to such market failures may be to just let them be. You can take this argument to merely logical extremes, and throw out government and democracy altogether. (Leeson and Williamson 2009), for instance, wonders whether in some (developing economies) settings, anarchy may not be preferably to predatory statehood, whether, in other words, no state would not be second-better than an inevitably failing government. (Caplan 2007), in an otherwise thoughtful book, seems to suggest that because voters are so invariably rationally irrational, markets may be second-better than democracy altogether. This has very little to do with the rigorous argument presented by Lancaster (1956): he did, at least in (1956), never consider a constrained government, let alone an incapacitated democracy to be grounds for “second-besting”, but, instead even seemed to hope for a government and people powerful and wise enough to heed his call. This is Pangloss at his finest: if you assume, as modern-day second-besters do, that the very means to deliberately get to a better world — government and democracy — are inevitably flawed, you can show, with almost hermetic logic, that whatever world we find ourselves in, must be the best of all possible worlds. In that word — possible — lies the catch. Modern-day second-besters assume that, akin to markets and evolution, government and democracy are aimless processes that merely aggregate pre-social, more- or less rational self-interest. If government and democracy are aimless, it follows — as it does, in fact, follow for markets and evolution — that any positive results of government and democracy are beyond reproach, and beyond improvement. Government failures, as monopoly pricing or an appendicitis, are just facts. Enlightenment, the mother of modern science, did not consider democracy, a merely positive affair, but a normative prescription. (Kant 1785) asked us not to “follow your incentives”, but to “act only on that maxim through which you can at the same time will that it should become a universal law” (1785 Chapter 11)(ibid.. The US Constitutional Convention in 1787, did not just decide to try out this new fptp way to aggregate preferences, but “We the People” were to do so “in Order to form a more perfect Union”. In a free society, social scientists need not believe in these, or any other ideas, but if they reject them all, they deserve not the air of scientific sophistication in which they cloak their utter cynicism. As mere accountants of the status quo, their work is anathema to Enlightenment, and they ought to be disowned of the emancipatory heritage that the social sciences were endowed with. But ignoring, for now, the enlightened humanism that comes part and parcel with the social sciences, the logic of modern-day second-besters is also simply fallacious. Even if government and democracy turn out to inevitably disappoint, such flaws are not, to the social scientist, quasi-material constraints. Because government and democracy are the subject matter for the social scientist, she must not presume, but has to explain them. If we do, as the second-besters, exclude from the first-order alternatives to be explained by second-order theory, those alternatives that the political process may corrupt, we have thereby conflated first and second-order theory. Whatever second-order theory might have to tell us about a corrupted political process, we would never learn, if we did not test for the absence of first-best solutions. This Frankenstein variant of the Theory of Second Best confuses, as (Brubaker 2002) succinctly criticized the literature about ethnic conflict, the “empirical data” with our “analytical toolkit”: government failure is “what we want to explain, not what we want to explain things with” (2002, 165, emphasis in original). References "],
["statement-authorship.html", "Statement of Authorship", " Statement of Authorship I hereby certify that this document has been composed by myself and describes my own work, unless otherwise acknowledged in the text. All references and verbatim extracts have been properly quoted and all sources of information have been specifically and clearly acknowledged. "],
["references.html", "References", " References "],
["appendix.html", "", ""]
]
